{"index":{"title":"Sanjay V","links":["tags/database","tags/book","tags/genai","tags/AI","tags/system_design","tags/operating_system","tags/"],"tags":[],"content":"Welcome to My World! üåç\nHi! I‚Äôm Sanjay, a full‚Äëstack developer who loves solving hard problems and building resilient, delightful systems. I write about Web Development, DevOps, Generative AI, and more.\nThis site is my digital ‚Äúsecond brain‚Äù ‚Äî a living collection of notes, debugging journeys, design explorations, and distilled lessons from real projects and experiments.\nüß† My Tech Notes\nDive into my curated collection of resources, notes, and lessons. I capture what I learn so I can revisit it quickly ‚Äî and hopefully it helps you too.\n\nDatabase\nBooks\nGenerative AI\nArtificial Intelligence\nSystem Design\nOperating System\n\nBrowse all tags ‚Üí tags\nüöÄ Let‚Äôs Connect\nFeel free to reach out if you want to talk tech, share ideas, or just say hello. I‚Äôm always up for a good discussion on the latest in development and AI!"},"notes/2024/AWS-ECR":{"title":"AWS ECR","links":[],"tags":["aws","devops","ecr"],"content":"üßä What is AWS ECR?\nECR (Elastic Container Registry) is a fully managed container image registry provided by Amazon Web Services (AW.\nüîë Key Features:\n\nStores, manages, and deploys Docker container images.\nIntegrated with other AWS services like ECS, EKS, Lambda.\nSupports both private and public registries.\nHighly available and secure (integrates with IAM for access control).\n\n\nüê≥ AWS ECR vs Docker Hub\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureAWS ECRDocker HubOwnershipAmazonDocker Inc.IntegrationDeep integration with AWS (ECS, EKS, Lambda)Works with all environmentsAccess ControlIAM roles/policiesUsername/password or tokensPricingPay per GB (storage and transfer)Free tier, then subscription plansPrivate ReposUnlimited (charged)Limited on free tierPerformanceOptimized for AWS networks (faster pulls within AWS)Public cloud, might be slower for AWS workloadsSecurityIAM + KMS + scan on push (with image scanning)Basic auth + scanning (Pro &amp; Team plans)"},"notes/2024/CORS-and-Preflight":{"title":"CORS and Preflight","links":[],"tags":["cors","web_security","backend"],"content":"Technical Documentation: Understanding Same-Origin Policy (SOP) and CORS in Web Development\nOverview\nThis document explains the Same-Origin Policy (SOP), Cross-Origin Resource Sharing (CORS), and related CORS headers, tailored for Node.js developers. It also includes a security deep dive into why Access-Control-Allow-Origin: * cannot be used with credentials.\n\n1. What is the Same-Origin Policy (SOP)?\nDefinition:\nSOP is a security measure implemented by browsers to restrict how documents or scripts loaded from one origin can interact with resources from another origin.\nOrigin:\nAn origin is defined by:\n\nProtocol (http/https)\nHost (domain or IP)\nPort\n\nExample:\nIf your frontend runs on:\nhttp://localhost:3000\n\n\nIt can only make requests to:\nhttp://localhost:3000/some-api\n\n\nAny attempt to access http://localhost:8080 will be blocked by the browser unless the server explicitly allows it using CORS headers.\n\n2. What is CORS?\nDefinition:\nCORS is a mechanism that allows a web server to indicate that it permits a web page from another origin to access its resources.\nExample:\nFrontend (React): http://localhost:3000\nBackend (Node.js/Express): http://localhost:8080\nWithout CORS, the browser blocks the request. With proper CORS headers, the backend allows the request.\n\n3. What is the Preflight Request?\nDefinition:\nBefore making certain types of requests (e.g., POST, custom headers, credentials), the browser sends a OPTIONS request to the server to check if the actual request is safe.\nPurpose:\nTo prevent unexpected side effects from cross-origin requests.\n\n4. Key Response Headers in Preflight (OPTIONS) Response\nImportant Headers:\n\nAccess-Control-Allow-Origin: Specifies allowed origin (cannot be  if credentials are used)\nAccess-Control-Allow-Credentials: Allows cookies/auth headers if set to true\nAccess-Control-Allow-Methods: Specifies allowed HTTP methods (e.g., GET, POST)\nAccess-Control-Allow-Headers: Specifies allowed request headers (e.g., Content-Type)\n\n\n5. Why Access-Control-Allow-Origin: * Cannot Be Used with Credentials\nScenario:\nYou are logged into yourbank.com and a malicious site evil.com runs this code:\n \nfetch(&#039;yourbank.com/account&#039;, {\n  credentials: &#039;include&#039;\n});\n \nIf server responds with:\nAccess-Control-Allow-Origin: *\nAccess-Control-Allow-Credentials: true\n\n\nThen:\n\nCookies from your session at yourbank.com are sent ‚úÖ\nResponse is visible to evil.com ‚ùå\n\nThis causes a major security vulnerability:\nevil.com can now read your private banking info ‚Äî this is why CORS forbids * when credentials are included.\nCorrect Usage:\nServer must explicitly echo the origin:\nAccess-Control-Allow-Origin: yourbank.com\nAccess-Control-Allow-Credentials: true\n\n\n\n6. Secure CORS Configuration in Express\napp.use((req, res, next) =&gt; {\n  const allowedOrigins = [&#039;http://localhost:3000&#039;, &#039;yourfrontend.com&#039;];\n  const origin = req.headers.origin;\n\n  if (allowedOrigins.includes(origin)) {\n    res.setHeader(&#039;Access-Control-Allow-Origin&#039;, origin);\n    res.setHeader(&#039;Access-Control-Allow-Credentials&#039;, &#039;true&#039;);\n  }\n\n  res.setHeader(&#039;Access-Control-Allow-Methods&#039;, &#039;GET,POST,PUT,DELETE&#039;);\n  res.setHeader(&#039;Access-Control-Allow-Headers&#039;, &#039;Content-Type,Authorization&#039;);\n  next();\n});\n\n\n\n7. Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeaderCan be *?With credentials: true?Access-Control-Allow-Origin‚úÖ Yes‚ùå NoAccess-Control-Allow-Credentials‚úÖ Yes‚úÖ Yes (with exact origin)\n\nTo securely support cross-origin requests with credentials (cookies or tokens), always specify the exact origin.\n\n"},"notes/2024/CPU-Scheduling-and-process-and-thread":{"title":"CPU Scheduling and process and thread","links":[],"tags":["operating_system"],"content":"The rest of your content lives here. You can use Markdown here :)\nProcess scheduling is an important activity done by the process manager to remove the process from the CPU and schedule the next process, the process removal and dispatch are based on multiple factors like process completion, priority, I/O requirement, etc. Process scheduling plays an important role in Multiprogramming operating systems.\nThere are mainly three types of schedulers in operating systems, which are: Short-term schedulers, medium-term schedulers, and long-term schedulers. In this article, we are going to discuss about difference between these schedulers.\nSet of instruction in cpu\n\nLOAD\nADD\nCOMPARE\nSTORE\nJUMP IF\nJUMP\nIN\nOUT\n\nThere are two modes of program execution one is kernel mode and another one is user mode\nif the program is running in the user mode then the program  does not have the direct access to the memory , if the program runs in the kernal mode then it has the direct access to the memory and the resources.\nif the program runs in the kernel mode  if that program happens to crash during the execution then the whole system leads to crash or comes to the halt\nSystem Calls\nwhen the program is running on the user mode , if it needs access to the memory or resources to complete that program for that instance it will shift to user mode from kernel mode this switching is called context switching"},"notes/2024/Clock-Synchronization-1":{"title":"Clock Synchronization 1","links":[],"tags":["distrubuted_systems","backend"],"content":"In distrubuted system , there will be no assurance that the timestamp of each server is same ,and it is not syncronized perfectly\nClock synchronization is the process of ensuring that multiple clocks in a system or network show the same time. This is essential for:\n‚úÖ Distributed systems (e.g., databases, cloud computing)\n‚úÖ Logging and debugging\n‚úÖ Secure communications (e.g., authentication protocols)\nClock drift\nClock drift  is referred to where some amount of time is drifted  from the original time, for example suppose there is a server A which  has the time stamp of March 12 , 12: 45 PM and the server B has the timestamp of March 12 , 12:50 PM here the server B is drifted by 5 mins of actual time\nactual def:\nClock drift refers to the gradual deviation of a system clock from the actual time due to hardware imperfections, temperature changes, or power fluctuations.\nClock Skew\nClock skew is the time difference between two or more system at a give moment of time\nthe more clock skew then the system will not synchronized properly, we cannot make the clock skew difference to zero but we can reduce the clock skew\nTo overcome this issue we have NTP - Network Time Protocal\nWorking Of NTP:\n\nhere\nt1 - timestamp when the client send the request to the ntp server , timestamp is attached to the request header\nt2 - timestamp when the server the receive the request from the client\nt3 - timestamp when the server send the response after some processing\nt4 - timestamp when the client receive the response\nRound trip network delay:    $ ‚Üí delta\n$ = (t4- t1) - (t3 - t2)  here delta is the total time taken by the network to take the request and response\nwe cannot calculate the time taken by response and request alone , why because the clock is not synchronized in client and server\nbut we can assume that both response and request takes the same time so\nrequest = response = $/2;\nso the timestamp at the client side when it receive the response is\nt4 = t3 + ($/2);"},"notes/2024/DNS":{"title":"DNS","links":[],"tags":["backend","networking","dns"],"content":"How DNS Works Behind the Scenes?\nWhen you type a website URL (like www.example.com) into your browser, DNS translates that domain name into an IP address so your browser can load the website. Here‚Äôs the process step-by-step:\na. The User‚Äôs Request\n\nUser Enters URL: A user types www.example.com into their browser.\nBrowser Queries DNS: The browser asks the DNS resolver (usually provided by your ISP) for the IP address of www.example.com.\n\nb. DNS Lookup Process\n\nCheck Local Cache: The DNS resolver first checks its local cache to see if it has the IP address stored. If it does, it returns the IP address right away.\nContact Root DNS Server: If the IP address is not in the cache, the resolver contacts one of the root DNS servers, which manages top-level domains (TLDs) like .com, .org, etc. There are 13 root DNS servers globally, maintained by organizations like ICANN (Internet Corporation for Assigned Names and Numbers).\nTLD DNS Server: The root server directs the resolver to the DNS server responsible for the .com TLD, called the TLD nameserver.\nAuthoritative DNS Server: The TLD nameserver tells the resolver which authoritative DNS server knows the IP address for example.com. This server is typically managed by the domain registrar or hosting provider (e.g., Heroku if it manages DNS for you).\nIP Address Response: The authoritative DNS server returns the IP address of the domain (e.g., 192.168.1.1), and the DNS resolver caches this IP to speed up future queries.\n\nc. Connecting to the Website\n\nBrowser Makes a Request: The browser now knows the IP address (e.g., 192.168.1.1) and makes a request to that IP address to load the website.\nWebsite is Loaded: The server at that IP address (e.g., Heroku) sends back the website‚Äôs content, and the browser renders the page.\n\n4. Where are IP Addresses Stored?\n\nDNS Records: IP addresses are stored in DNS records on authoritative DNS servers. When you configure DNS for your website, you manage these records through a DNS service (like Cloudflare, GoDaddy, or a hosting provider like Heroku).\nTypes of DNS Records:\n\nA Record: Maps a domain name to an IP address (e.g., example.com ‚Üí 192.168.1.1).\nCNAME Record: Maps a domain name to another domain name (e.g., www.example.com ‚Üí example.com).\nMX Record: Maps a domain name to an email server.\nNS Record: Points to the DNS servers that are authoritative for a domain.\n\n\n\n5. Does Heroku Register My Website in DNS?\nWhen you deploy your application on Heroku, Heroku provides a default subdomain (e.g., yourapp.herokuapp.com). However, Heroku does not automatically register your custom domain in DNS. If you want to use a custom domain (e.g., www.yourdomain.com), you must do the following:\na. Custom Domain Setup with Heroku\n\nBuy a Domain: First, you buy a domain from a domain registrar (like GoDaddy, Namecheap, or Google Domains).\nPoint DNS to Heroku: You log in to your domain registrar‚Äôs DNS settings and create DNS records that point your domain to Heroku. Usually:\n\nA Record (or ALIAS/CNAME if using www) that points to Heroku‚Äôs IP address or specific Heroku endpoint.\nCNAME Record if you‚Äôre using a subdomain like www.yourdomain.com, which points to yourapp.herokuapp.com.\n\n\nConfigure in Heroku Dashboard: In Heroku‚Äôs dashboard, you configure your application to recognize your custom domain.\n\nOnce these steps are completed, Heroku doesn‚Äôt ‚Äúregister‚Äù the domain name itself. That‚Äôs the responsibility of the DNS server (usually your registrar), but Heroku acts as the server that serves your app when the DNS points to it.\n6. How DNS and Heroku Work Together (Example)\nLet‚Äôs look at an example to make this clearer.\nExample Scenario:\nYou have an app deployed at Heroku (myapp.herokuapp.com), and you want to access it via your custom domain www.mywebsite.com.\na. Step 1: Buy a Domain\n\nYou buy the domain mywebsite.com from a domain registrar like Namecheap.\n\nb. Step 2: DNS Configuration\n\nYou log into your Namecheap account (or whichever registrar you used).\nYou configure a CNAME Record for www.mywebsite.com pointing to myapp.herokuapp.com.\nOptionally, you also create an A Record pointing the root domain (mywebsite.com) to Heroku‚Äôs IP address, or use an ALIAS.\n\nc. Step 3: Configure Heroku\n\nYou go to the Heroku Dashboard, find your app, and add the custom domain www.mywebsite.com to it.\nHeroku verifies the domain and links it to your app.\n\nd. Step 4: User Access\n\nNow, when a user types www.mywebsite.com:\n\nThe browser queries the DNS server for the IP address.\nThe DNS resolver finds the CNAME Record for www.mywebsite.com pointing to myapp.herokuapp.com.\nIt retrieves Heroku‚Äôs IP address and loads the website.\n\n\n\n7. Summary of How It Works Behind the Scenes\n\nUser Typing Domain Name:\n\nUser types www.mywebsite.com.\n\n\nDNS Resolution:\n\nThe DNS resolver queries the DNS system to find the IP address corresponding to www.mywebsite.com.\nDNS servers respond with either the IP address (for an A Record) or direct the browser to myapp.herokuapp.com (for a CNAME Record).\n\n\nHeroku‚Äôs Role:\n\nOnce the IP address is found, the browser connects to Heroku‚Äôs servers.\nHeroku hosts your app, processes the request, and returns the website content to the user.\n\n\nDomain Registration:\n\nYour domain is managed by a domain registrar, not Heroku. You must configure DNS records with the registrar to point to Heroku.\n\n\n\n\nKey Takeaways\n\nDNS Servers are maintained by different entities like ISPs, registrars, and cloud service providers. They map domain names to IP addresses using DNS records.\nIP Addresses are stored in DNS records like A and CNAME records on authoritative DNS servers.\nHeroku provides hosting for your application but doesn‚Äôt automatically register your custom domain in DNS. You must configure DNS settings to point your domain to Heroku.\nDNS Resolution involves querying root DNS servers, TLD DNS servers, and authoritative DNS servers to resolve the domain name to an IP address, allowing the browser to connect to the correct server.\n\nBy understanding DNS and its relationship with hosting platforms like Heroku, you can configure and manage your custom domains effectively.\nA CNAME (Canonical Name) record is a type of DNS record that maps one domain name (an alias) to another domain name (the canonical name). It‚Äôs used when you want multiple domain names to point to the same content or server without needing to update each individual DNS record.\nHow CNAME Works:\n\nImagine you have two domain names:\n\nwww.example.com (primary domain)\nblog.example.com (subdomain)\n\n\nYou want both www.example.com and blog.example.com to point to the same content or server, so you don‚Äôt have to set up separate IP addresses for each domain.\n\nInstead of creating separate A records (which map domains to IP addresses), you can create a CNAME record for blog.example.com that points to www.example.com. So, whenever someone visits blog.example.com, the DNS system will check the CNAME record and forward them to www.example.com.\nWhy Use a CNAME?\n\nSimplification: You only need to manage one set of DNS records (the original or ‚Äúcanonical‚Äù domain). If the IP address of www.example.com changes, you don‚Äôt need to update blog.example.com; the CNAME will automatically reflect the change.\nMultiple Aliases: You can use CNAME records to create multiple aliases for one canonical domain (e.g., mail.example.com, ftp.example.com, all pointing to example.com).\n\nExample:\n\n\nCNAME Record:\nblog.example.com CNAME www.example.com\nThis means that blog.example.com will resolve to www.example.com and inherit its DNS settings (like the IP address).\n\n\nImportant Notes:\n\nCNAMEs can‚Äôt point to IP addresses directly. They only point to another domain name.\nYou can‚Äôt have a CNAME for the root domain (like example.com). It‚Äôs only used for subdomains (like www.example.com or mail.example.com).\n\nWhen to Use CNAME:\n\nWhen you have subdomains that need to point to the same server as the main domain.\nWhen you want to redirect traffic from different domain names to a single domain name without managing individual IP addresses.\n\nExample in Real Life:\n\nHeroku Setup:\n\nSuppose you deploy an app on Heroku, and Heroku gives you a domain like yourapp.herokuapp.com.\nYou want to use a custom domain www.mywebsite.com to point to your Heroku app.\nYou can create a CNAME record that maps www.mywebsite.com to yourapp.herokuapp.com. This way, when people type www.mywebsite.com, they get redirected to your Heroku app.\n\n\n\nSummary of CNAME:\n\nCNAME = Alias for another domain.\nIt simplifies DNS management and allows multiple domain names to point to the same content.\nCommonly used for subdomains or to point custom domains to third-party services (like Heroku or GitHub Pages).\n\nLet me know if this clears up the concept for you!\nwww.cloudflare.com/en-gb/learning/dns/dns-server-types/"},"notes/2024/Database":{"title":"Database","links":[],"tags":["database","graph_database","vector_database"],"content":"What is graph database ?\nDatabases that store data in a graph structure are called graph databases. These databases represent data in the form of nodes, edges, and properties, reflecting the relationships between entities naturally and intuitively. Some of the most well-known graph databases include Neo4j, Amazon Neptune, ArangoDB, OrientDB, and Microsoft Azure Cosmos DB (with a Gremlin API).\n1. How Graph Databases Store Data\nGraph databases store data in two main components:\n\nNodes (Vertices): Represent entities or objects, such as people, products, or concepts. Each node can have properties (key-value pairs) that describe the attributes of that entity.\nEdges (Relationships): Represent the connections between nodes. Edges also can have properties that describe the relationship, such as the type of relationship, its strength, or other relevant data.\n\nFor example, in a social network graph:\n\nNodes could represent people.\nEdges could represent friendships between people.\nProperties of nodes could include names, ages, and locations.\nProperties of edges could include the date when the friendship was established.\n\n2. How Data is Stored Internally\n\nNodes and Relationships: Both nodes and edges are stored as distinct records in a graph database. Each node has a unique identifier, and each edge connects two node identifiers. The relationships are stored alongside the nodes they connect, allowing quick traversal from one node to another.\nIndexes: While graph databases emphasize relationships, they often still use traditional indexing mechanisms to quickly find starting points for graph traversals. For example, Neo4j uses B-trees to index node properties or relationships for efficient querying.\nStorage Formats: Some graph databases store data in an adjacency list format (like Neo4j), where each node stores references (pointers) to its neighboring nodes. Others might use adjacency matrices or more specialized storage engines optimized for graph traversal.\n\n3. How Graph Databases Work\nGraph databases excel at queries that involve traversing relationships, such as finding the shortest path between two nodes, exploring friends of friends in a social network, or recommending products based on user behavior. Here‚Äôs how they work:\n\nQuery Language: Graph databases typically use graph-specific query languages. For example:\n\nCypher (used by Neo4j): A declarative query language that makes it easy to express complex graph queries.\nGremlin (used by Apache TinkerPop and Amazon Neptune): A graph traversal language that can be used across different graph databases.\nSPARQL (used in RDF stores like Virtuoso): A query language for querying RDF (Resource Description Framework) data.\n\n\nTraversal: The core operation in graph databases is traversal. Given a starting node, the database follows edges to other connected nodes. Traversal can be breadth-first or depth-first, depending on the query. The traversal process is highly optimized in graph databases, enabling complex queries to be executed efficiently.\nQuery Execution: When a graph query is executed, the database follows the relationships between nodes as specified in the query. Since relationships are first-class citizens in a graph database, these traversals are typically very fast, even with deep and complex relationships.\n\n4. Example: Neo4j\nIn Neo4j, one of the most popular graph databases:\n\n\nData Model: Data is stored in a labeled property graph model, where nodes have labels (e.g., Person, Product), and edges have types (e.g., FRIEND, PURCHASED).\n\n\nStorage: Neo4j uses a native graph storage engine that stores nodes and edges in a way optimized for rapid traversal. Each node stores pointers to its adjacent nodes, and relationships are stored with direct references to the nodes they connect.\n\n\nQuerying: You use the Cypher query language to interact with the graph. For example, to find all friends of a person named ‚ÄúAlice,‚Äù you might write:\ncypherCopy code\nMATCH (alice:Person {name: &#039;Alice&#039;})-[:FRIEND]-&gt;(friend)\nRETURN friend.name;\n\n\n\n\nThis query tells Neo4j to start at the node labeled Person with the name ‚ÄúAlice‚Äù, follow the FRIEND relationships, and return the names of the connected nodes.\n5. Use Cases for Graph Databases\n\nSocial Networks: Modeling and querying relationships between people, such as friendships, followers, or influencers.\nRecommendation Systems: Finding products or content similar to what a user has already interacted with.\nFraud Detection: Analyzing connections between entities to identify suspicious patterns, like unusual links between accounts.\nKnowledge Graphs: Structuring and querying interconnected data, such as in Google‚Äôs Knowledge Graph.\n\nSummary\nGraph databases are designed to store and query data that is rich in relationships, like social networks, recommendation systems, and more. They represent data using nodes (entities) and edges (relationships) and use specialized storage formats and query languages to efficiently handle complex queries involving multiple relationships. By treating relationships as first-class citizens, graph databases provide a powerful tool for any application where relationships are central to the data model.\nWhat is the vector database ?\nCharacteristics of vector database\n\nvector\nembedding\n\nVector is a array of numerical values that represents the different forms of data such as images, text , videos etc. where embedding is the multidimensional space to represent the large data sets such as LLM\nUse Case\n\nNLP (Natural Language Processing) ‚Üí ChatBot\nVideo And Audio Recognition\nSearch ‚Üí recommendation engine\n"},"notes/2024/Debouncing-And-Throttling":{"title":"Debouncing And Throttling","links":[],"tags":["javascript","typescript"],"content":"Debouncing is a programming practice used to ensure that time-consuming tasks do not fire so often, which can cause performance issues. In essence, it limits the execution of a function until a certain amount of idle time has passed without it being triggered again.\nthrottling is a programming concept where it triggers a function for the particular time limit\nmedium.com/@mujaffarhssn/debouncing-vs-throttling-optimizing-your-javascript-for-performance-a99d38f5eb3b"},"notes/2024/Disk-storage-Basics":{"title":"Disk storage Basics","links":[],"tags":["operating_system","storage","filesystem"],"content":"Got it üëç ‚Äî let‚Äôs slow this down and walk through how Unix file systems (like ext4) store data on disk step by step with a real-world example.\n\nüß© 1. Disk Basics\n\nThink of your hard disk (or SSD) as a big grid of tiny storage boxes.\nEach small box is called a block (commonly 4 KB).\nBlocks are the smallest unit of storage the file system works with.\n\nüìå Example:\nImagine your disk is a notebook with 100 pages.\n\nEach page = block (can store 4 KB of data).\nThe file system decides which page gets written to.\n\n\nüß© 2. What happens when you create a file?\nSuppose you create a file called hello.txt that contains:\nHello World!\n\n\nThat‚Äôs 12 bytes of data.\nStep 1: Inode is created\n\nEvery file has an inode (like an ID card).\nThe inode does not store file data itself ‚Äî instead, it stores:\n\nFile size (12 bytes)\nOwner (user ID)\nPermissions (read/write/execute flags)\nTimestamps (created/modified time)\nPointers (addresses of blocks where actual file content is stored)\n\n\n\nThink of the inode as a table of contents entry for the file.\nStep 2: Blocks are allocated\n\nThe file system finds a free block using the block bitmap (a big checklist where 0 = free, 1 = used).\nIt writes &quot;Hello World!&quot; into that block.\n\nüìå Example:\n\nBlock 37 is free ‚Üí allocate it.\nInode of hello.txt now points to block 37.\n\n\nüß© 3. Reading the file\nWhen you type cat hello.txt:\n\nOS looks up the inode of hello.txt.\nInode says: ‚ÄúData is in block 37.‚Äù\nOS fetches block 37 from disk.\nReturns &quot;Hello World!&quot; to you.\n\n\nüß© 4. Bigger Files (multiple blocks)\nSuppose you create a 20 KB file big.txt.\n\nBlock size = 4 KB.\nFile needs 5 blocks (20 √∑ 4).\n\nFile system allocates blocks: 11, 12, 45, 46, 47.\n\nInode stores pointers ‚Üí [11, 12, 45, 46, 47].\n\nWhen reading big.txt, OS uses inode to fetch those blocks in order.\n\nüß© 5. Bitmap (tracking free/used blocks)\n\nThe file system has a bitmap for the whole disk.\nExample:\n\nBlocks:   0 1 2 3 4 5 6 7 8 9\nBitmap:   1 1 0 0 1 0 1 0 0 0\n\n\nHere:\n\n1 = used (allocated to some file)\n0 = free (available for new files)\n\nSo, block 2, 3, 5, 7, 8, 9 are free.\n\n‚úÖ Summary with analogy:\n\nDisk blocks = notebook pages.\nFile data = text written on those pages.\nInode = table of contents entry for each file (says where the text is stored).\nBitmap = checklist of which notebook pages are free/used.\n\n"},"notes/2024/Docker-Basics":{"title":"Docker Basics","links":[],"tags":["devops","docker","containers"],"content":"Docker Basics\nwhat is virtual machine and its advantage?\nvirtual machines needs a physical resources to function. it has the CPU ,\nmemory , file storage , mouse keyboard etc and the ability if required to\nconnect to the internet\nwhat is virtualization ?\nvirtualization is the process of simulating the the hardware and software\nin the virtual(software) environment\n\nIn the Above diagram hypervisor is the software that is used to perform the virtualization\nby managing the system resources the VM‚Äôs\nSuppose consider a example we have servers one is email server, another one webserver another one internal server each server is running on different system\nand different operation system like linux,windows and unix etc in traditional architecture\nbut in virtualization we can run all three server of different os in same system using virtualizationusing hypervisor technology\ntypes of hypervisor\n\nbare metal hypervisor and it is the type one hypervisor where the\nthe hypervisor is built on top of baremetal system like the system\nwith no os\nnext one is type 2 hypervisor where the hypervisor is built on top\nof operating system\n\nadvantages of the virtual machines\n\ncost efficiency\nportability - the vm‚Äôs in the machine can easily transferred to\nanother machines\nfault tolerance - vm are just stored in the file which can be\nrecovered easily\n\nDisadvantage of Virtual machines\n\noccupies of lot of disc space\nconsumes lot of RAM and consumes more power by server from\ncpu\n\nWhat is Container?\nThe container is the contains the applications with all necessary files packages and environment variables like container is bundled with all the necessary files to run the application , it consist of application configuration libraries and all dependencies to run the application\ncontainer is the instance of the image , isolated from each other , with their own environment\nWhat is Docker ?\ndocker is a software platform that uses the os level virtualization to create a self contained containers, you may have created several virtual machines using oracle VM or any other VM&#039;s  , docker is like but with better performance .\nIn Docker  we select a image and download it and create a one or more instance or\ncontainers , pretty similar to creating virtual machines\nwhat is Image ?\nA package or template used to create one or more containers"},"notes/2024/Docker-Zero-to-Hero":{"title":"Docker Zero to Hero","links":[],"tags":["devops","backend","docker"],"content":"What is Docker ?\nDocker is an open source platform that enables developers to build, deploy, run, update and\nmanage containers‚Äîstandardized, executable components that combine application source\ncode with the operating system (OS) libraries and dependencies required to run that code in\nany environment.\nDocker terminologies :\nImages: The blueprints of our application which form the basis of containers. These\ncontain all of the configuration settings that define the isolated environment.\nDocker images contain executable application source code as well as all the tools, libraries\nand dependencies that the application code needs to run as a container. When you run the\nDocker image, it becomes one instance (or multiple instances) of the container.\nMultiple Docker images can be created from a single base image\nDocker images are made up of layers and each layer corresponds to a version of the image.\nWhenever a developer makes changes to the image, a new top layer is created and this top\nlayer replaces the previous top layer as the current version of the image. Previous layers are\nsaved for rollbacks or to be re-used in other projects.\nEach time a container is created from a Docker image, yet another new layer called the\ncontainer layer is created. Changes made to the container‚Äîsuch as the addition or deletion\nof files‚Äîare saved to the container layer and exist only while the container is running.\nThis iterative image-creation process enables increased overall efficiency since multiple live\ncontainer instances can run from just a single base image and when they do so, they\nleverage a common stack.\nContainers: Are instances of a Docker image and are what run the actual application.A\nDocker container image is a lightweight, standalone, executable package of software that\nincludes everything needed to run an application: code, runtime, system tools, system\nlibraries and settings.\nContainer images become containers at runtime and in the case of Docker containers ‚Äì\nimages become containers when they run on Docker Engine.\nDocker Daemon: That background service running on the host that listens to API calls (via\nthe Docker client), manages images and building, running and distributing containers. The\nDeamon is the process that runs in the operating system which the client talks to ‚Äì playing\nthe role of the broker.\nDocker daemon is a service that creates and manages Docker images, using the commands from the client. Essentially the Docker daemon serves as the control center of your Docker implementation. The server on which Docker daemon runs is called the Docker host\nDocker Client:\nDocker CLI is the most commonly used tool. It is the command-line interface used to control\nthe Docker service. The Docker CLI enables you to interact with the system. To do this, we\ncan use the Docker command, which will allow us to send and receive messages from the\nDocker daemon. The Docker command executes commands, creates images and networks,\nstarts and stops containers, and more.\nThe Docker CLI receives commands, checks to ensure they are correctly formatted, and\nthen turns them into a REST API call. The Docker daemon listens for requests from Docker\nCLI and processes them according to its configuration. It is responsible for executing\ncommands, processing requests, monitoring how containers run, mounting volumes, starting\nand stopping containers, and more.\nThe Docker client and daemon can both run on the same device. Alternatively, you can\nconnect a Docker client to a remote Docker daemon. It allows you to manage a remote\nmachine‚Äôs Docker system. After all, the Docker client and daemon communicate with each\nother using a REST API over UNIX sockets or a network interface.\nDocker Hub: A registry of Docker images containing all available Docker images. A user\ncan have their own registry, from which they can pull images.\nVirtual Machines VS Containers :\nA virtual machine (VM) is another way of creating an isolated environment. A VM is\neffectively an individual computer that lives inside a host machine; multiple VMs can live\ninside a single host machine. VMs are created by virtualising the host machine‚Äôs underlying\nhardware (processing, memory and disk). The hardware is virtualised and split up, with a\npiece representing a portion of the underlying physical hardware, which a VM can be run on.\n\nUse Case: Running a Node.js Application in Docker\n\n\nInstall Docker:\nEnsure Docker is installed and the Docker Daemon is running on your machine.\n\n\nCreate a Simple Node.js App:\n\n\nFile: app.js\n \nconst http = require(&quot;http&quot;);\nconst port = 3000;\n \nconst server = http.createServer((req, res) =&gt; {\n    res.writeHead(200, { &quot;Content-Type&quot;: &quot;text/plain&quot; });\n    res.end(&quot;Hello from Docker!\\n&quot;);\n});\n \nserver.listen(port, () =&gt; {\n    console.log(`Server running at http://localhost:${port}/`);\n});\n \n\n\nFile: package.json\n \n{\n  &quot;name&quot;: &quot;docker-node-example&quot;,\n  &quot;version&quot;: &quot;1.0.0&quot;,\n  &quot;main&quot;: &quot;app.js&quot;,\n  &quot;scripts&quot;: {\n    &quot;start&quot;: &quot;node app.js&quot;\n  },\n  &quot;dependencies&quot;: {}\n}\n \n\n\nCreate a Dockerfile:\nFROM node:16\n\n# Set the working directory\nWORKDIR /app\n\n# Copy app files to the container\nCOPY . .\n\n# Install dependencies\nRUN npm install\n\n# Expose the application port\nEXPOSE 3000\n\n# Command to run the application\nCMD [&quot;npm&quot;, &quot;start&quot;]\n\n\n\n\nBuild and Run the Application:\n\n\nBuild the image:\ndocker build -t node-docker-example\n\n\nRun the container:\ndocker run -d -p 3000:3000 node-docker-example\n\n\n\n\n\nDocker Daemon‚Äôs Role in This Example\n\nImage Build (docker build):\n\nThe CLI sends the build request to the Docker Daemon.\nThe daemon reads the Dockerfile, pulls the required base image (node:16), and creates the new image node-docker-example.\n\n\nContainer Start (docker run):\n\nThe CLI sends the run request to the Docker Daemon.\nThe daemon uses the node-docker-example image to create and start a container.\nIt maps port 3000 of the container to port 3000 on the host machine.\n\n\nNetworking:\n\nThe daemon sets up a bridge network for the container to communicate with the host.\n\n\nMonitoring:\n\nThe daemon continuously monitors the container‚Äôs state and manages its lifecycle (e.g., restarting it if needed).\n\n\n\n\nUseful Commands to Observe Docker Daemon in Action\n\n\nList Running Containers:\n \ndocker ps\nDisplays all active containers managed by the Docker Daemon.\n\n\nInspect Container Logs:\n \ndocker logs &lt;container-id&gt;\n \n\n\nView Daemon Logs (Linux):\n \nsudo journalctl -u docker\n\n\nCheck Daemon Status:\n \nsystemctl status docker\n\n\nUNIX socket\nunix socket is used to communicate with process  running on the same machine\nref:\nWhat is the difference between Unix sockets and TCP/IP sockets?\ndocker deamon create the bridge network between the host and the container\nIn the context of Docker, the host refers to the physical or virtual machine on which the Docker Daemon is running. This could be:\n\nYour Local Machine:\n\nIf you are running Docker on your laptop or desktop, the host is your personal computer.\n\n\nA Cloud Instance:\n\nIf you are using a cloud service like AWS, Google Cloud, or Azure to run Docker, the host is the virtual machine instance in the cloud.\n\n\nA Server:\n\nIn a production environment, the host might be a dedicated server or a virtual server within a data center.\n\n\n\n\nBridge Network in Docker\nWhen Docker sets up a bridge network, it allows containers to communicate with each other and the host machine. Here‚Äôs how the relationship works:\n\nThe Docker Daemon creates a virtual network interface (usually named docker0) on the host machine. This acts as a bridge between the host and containers.\nContainers attached to the bridge network can communicate:\n\nWith each other using private IP addresses assigned by Docker.\nWith the host via the docker0 interface.\n\n\n\n\nExamples\nScenario 1: Local Machine as the Host\n\n\nYou run a Node.js app in a container with:\nbash\nCopy code\ndocker run -d -p 3000:3000 node-docker-example\n \n\n\nThe host is your local machine.\n\n\nWhen you access http://localhost:3000 in your browser, you are connecting to the host (your computer), which forwards the request to the container through the bridge network.\n\n\nScenario 2: Cloud Instance as the Host\n\n\nYou deploy a Dockerized Node.js app to an AWS EC2 instance:\nbash\nCopy code\ndocker run -d -p 3000:3000 node-docker-example\n \n\n\nThe host is the EC2 instance.\n\n\nWhen you access http://&lt;EC2-public-IP&gt;:3000, you are connecting to the host (the EC2 instance), which forwards the request to the container.\n\n\nDocker Registry\nThere is two type of registry one public registry and another one is private registry , public registry means docker hub itself where the organization register their  images publicly like redis , ngnix and rabbitmq etc\nIn private registry we cannot run the image directly , to run the image we want to authenticate and run the image , some of the image registry platforms are AWS ECR (Elastic Container Registry) , Google Container Register , Azure Container Registry and docker hub etc\nDocker File :\nDocker builds images automatically by reading the instructions from a Dockerfile.\nIt is a text file without any .txt extensions that contains all commands in order,\nneeded to build a given image.\nIt is always named *Dockerfile*.\n\nDocker image consists of read-only layers each of which represents a Dockerfile instruction. The layers are stacked and each one is created by the change from the previous layer. For example, if I create a base layer of ubuntu and then in second instruction I install Python it will create a second layer. Likewise, if I do any changes by the instructions(RUN , COPY , ADD) it will create a new layer in that image. Containers are read-write layers that are created by docker images.\nIn simple words, a Dockerfile is a set of instructions that creates a stacked-layer\nfor each instruction that collectively makes an image(which is a prototype or\ntemplate for containers)\nSteps to create docker file to create image\n\n\nchoose the base image , eg: if your application is a node application then choose the any version of the node image and make it as the base image . FROM command defines the base image and the container is built on top of the base image\nFROM [image_name]:[version]\nFROM node: alpine // this defines the base image of the container\n\n\nNext to run the code we want to copy the code and move to our container folder for that we want to use the COPY  command  COPY source dest\nCOPY package.json /app/\nCOPY src /app/\n\n\nTo run the application first we want to download the package  for that we want to run npm i\nto execute the command we RUN command\n\n\nTo go to the correct directory to run the application , want to use the WORKDIR command to execute\n\n\nTo Run the application or to start the application we want to use the CMD command for start the docker container\n\n\nEach RUN command in a Dockerfile creates a new image layer to enable a modular, efficient, and flexible way of building and maintaining Docker images. Here‚Äôs a detailed explanation:\n\n1. What are Image Layers?\n\nDefinition: Docker images are built in layers. Each layer represents a set of filesystem changes (like adding files, running commands, or modifying configurations).\nStacking Layers: When you build an image, Docker stacks these layers on top of a base image.\n\n\n2. Why Does Each RUN Command Create a New Layer?\na. Modularity\n\nEach RUN command isolates changes made by that step.\nThis modularity allows Docker to manage and reuse layers efficiently.\n\nExample: If a later layer changes, Docker can rebuild only that layer and subsequent layers, leaving unchanged layers intact.\n\n\n\nb. Layer Caching\n\nLayers are cached to improve build performance. If the content of a layer hasn‚Äôt changed, Docker reuses it rather than recreating it.\n\nExample: If you install dependencies with RUN apt-get install, Docker will cache this layer. If no changes are made to this step, it won‚Äôt rerun on subsequent builds.\n\n\n\nc. Fault Isolation\n\nIf a step in the Dockerfile fails, only that specific layer and subsequent ones need to be rebuilt, not the entire image.\nThis makes debugging faster and more manageable.\n\nd. Layer Sharing\n\nDocker uses layers as a way to optimize disk usage.\nIf multiple images share the same base layers, Docker stores these layers once and reuses them across images.\n\n\n3. Purpose of New Layers for Each RUN\nCreating a new layer for each RUN command provides:\n\nEfficiency:\n\nSpeeds up builds by leveraging cached layers.\nReduces storage requirements since layers are reused across images.\n\n\nFlexibility:\n\nModular layers make it easier to modify and debug specific parts of the Dockerfile without rebuilding the entire image.\n\n\nVersion Control:\n\nLayers act as checkpoints during the build process, enabling better control over the build process.\n\n\n\n\n4. How It Works in Practice\nExample 1: Modular Build Process\nDockerfile\n\nFROM ubuntu:latest\nRUN apt-get update\nRUN apt-get install -y curl\nRUN echo &quot;Hello, Docker!&quot;\n\n\n\nEach RUN command creates a new layer:\n\napt-get update (Layer 1)\napt-get install curl (Layer 2)\necho &quot;Hello, Docker!&quot; (Layer 3)\n\n\n\nIf you modify RUN echo &quot;Hello, Docker!&quot;, Docker rebuilds only Layer 3 and any dependent layers.\nExample 2: Efficient Caching\nDockerfile\nCopy code\nRUN apt-get update &amp;&amp; apt-get install -y curl\n\n\n\nCombining commands into a single RUN reduces the number of layers, improving efficiency.\nUse it when the commands are logically related.\n\n\n5. Best Practices for Docker Layers\n\n\nMinimize Layers: Combine commands to reduce the number of layers.\n\n\nOrder Commands Wisely: Place frequently changing steps later in the Dockerfile to maximize caching.\n\n\nClean Up: Remove temporary files within the same RUN command to avoid bloating the image.\nDockerfile\nCopy code\nRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n\n\n\nBy understanding why each RUN command creates a new layer, you can build efficient, reusable, and maintainable Docker images.\nNote:\n\nevery time when you run the docker container , will create a new instance of the image , it doesn‚Äôt reuse the old container , but when you execute the command docker ps it only display the active container, to view the container which we created before want to use the -a\n\nDocker Commands\n\nList the docker images - docker image\nList the active docker container - docker ps\ndownload the image - docker pull [image _Name]:[VERSION]  ‚Üí will download the image locally , this will pull the image directly from the docker hub , docker hub is like npm registry\nhow to run the container ‚Üí docker run [image name]: version\nthe above command will run the image or run the container which will show the logs of the container which will block the terminal you won‚Äôt be able to run any other command , if you want to run the command you only be able to do it in another tab, for non blocking container execution add a option -d in the above command\n\ndocker run -d [image_name][version] - where d for detach  but in this command you wont see any logs , if we want to see the logs the of that container execute this command\ndocker log [docker_id]\n5. stop the container - docker stop [container_id]\n6. port binding -  docker run -d -p [host_port]:[container_port] [image_name]: [version_name]  it always good to bind the same port as the container port\n7. start the container - docker start [docker_id]\ndifference btw docker start and docker run is docker run create a new instance of the image and execute where as the docker start will restart the already existed container\n8. we can also give the custom name to the container for that we want to use the ‚Äîname flag while running the container docker run ‚Äîname [name] -d [image_name]:[version] so we can start , stop and get the log we if runs the command -d flag with name which we have given while running the docker instead of docker_id\n9. CMD - cmd command is used execute when the container runs we can also override the default command by specifying the params in CMD command\n10. ENTRYPOINT:\n\nPurpose: Defines a command that always runs when the container starts.\nUsage: Used for containers with a specific task or application (e.g., running scripts, services).\nWhen It Runs: At runtime and cannot be fully overridden (but arguments can be added).\nEffect: Always executes the specified command as the primary process of the container.\n\n\nbuild command for building image - docker build -t [imageName]:[version] [locatlion of the dockerfile]\nUsually docker doesn‚Äôt remove the unused container , unless we manually specify the prune command\n\nreference:\nDocker Best Practices: Choosing Between RUN, CMD, and ENTRYPOINT | Docker\nprune:\nref: docs.docker.com/engine/manage-resources/pruning/#:~:text=Prune containers,-When you stop&amp;text=A stopped container‚Äôs writable layers,the docker container prune command.&amp;text=By default%2C you‚Äôre prompted,f or ‚Äîforce flag."},"notes/2024/Event-Loop-In-Javascript-1ed82f6618b380d08461d82a3c704d81":{"title":"Event Loop In Javascript","links":[],"tags":["javascript","event_loop","nodejs"],"content":"Event Loop In Javascript\nEvent Loop has four types of queue\n\nTimer Queue\nI/O Queue\nCheck Queue\nClose Queue\nMicroTask Queue\n\n\nExecution Order Of Event Loop\n\n\nCheck Queue\nTo queue a callback function inside the check queue we can use the setImmediate function"},"notes/2024/Helmet-with-node-js":{"title":"Helmet with node js","links":[],"tags":["security","web_security","nodejs"],"content":"Helmet in node js  is a set of middleware functions that sets the various http headers to increase the security to prevent from xss , clickjacking etc..\n\n\nwhat is xss\nCross-Site Scripting (XSS) is a type of security vulnerability typically found in web applications. It allows attackers to inject malicious scripts (usually JavaScript) into web pages viewed by other users. When an unsuspecting user views the web page, the malicious script is executed in their browser, potentially leading to data theft, session hijacking, or other malicious activities.\nTypes of XSS\n\nStored XSS (Persistent XSS):\n\nThe malicious script is permanently stored on the target server, such as in a database, a comment field, or a user profile.\nWhenever a user accesses the affected page, the script is delivered to their browser and executed.\n\n\nReflected XSS (Non-Persistent XSS):\n\nThe malicious script is reflected off a web server, typically via a URL parameter, and executed immediately without being stored.\nThe script is part of the request sent to the server, and the server includes it in the response without proper sanitization.\n\n\nDOM-Based XSS:\n\nThe vulnerability exists in the client-side code (JavaScript) itself.\nThe malicious script is executed as a result of modifying the DOM (Document Object Model) environment on the client side, without involving the server.\n\n\n\nExample of XSS\nLet‚Äôs look at a simple example of a Reflected XSS attack.\nVulnerable Scenario:\nImagine you have a search functionality on your website that takes user input via a query parameter and displays it back on the page.\nHere‚Äôs a vulnerable search form:\nhtmlCopy code\n&lt;form action=&quot;/search&quot; method=&quot;GET&quot;&gt;\n    &lt;input type=&quot;text&quot; name=&quot;q&quot; placeholder=&quot;Search...&quot;&gt;\n    &lt;button type=&quot;submit&quot;&gt;Search&lt;/button&gt;\n&lt;/form&gt;\n \nThe server-side code might render the search term like this (in a simplified way):\n \n&lt;h1&gt;Search Results for &quot;&lt;?php echo $_GET[&#039;q&#039;]; ?&gt;&quot;&lt;/h1&gt;\n \nIf a user enters a search term like test, the resulting page might display:\nhtmlCopy code\n&lt;h1&gt;Search Results for &quot;test&quot;&lt;/h1&gt;\n \nXSS Attack:\nAn attacker can exploit this by crafting a URL like this:\nphpCopy code\nexample.com/search(&#039;XSS&#039;);&lt;/script&gt;\n \nWhen a user clicks on this link, the browser will render the following HTML:\nhtmlCopy code\n&lt;h1&gt;Search Results for &quot;&lt;script&gt;alert(&#039;XSS&#039;);&lt;/script&gt;&quot;&lt;/h1&gt;\n \nThe browser will execute the script, and an alert box with the message ‚ÄúXSS‚Äù will appear.\nWhy is This Dangerous?\nWhile a simple alert() box might seem harmless, this vulnerability could be exploited for more malicious purposes, such as:\n\nStealing cookies: The attacker could access the user‚Äôs session cookies.\nRedirecting users: Users could be redirected to a malicious site.\nKeylogging: The script could log the user‚Äôs keystrokes to capture sensitive information.\n\nPreventing XSS\n\nInput Validation and Sanitization:\n\nAlways validate and sanitize user input on both the server and client sides. Ensure that any data input by users does not include executable scripts.\n\n\nOutput Encoding:\n\nEncode output based on the context (HTML, JavaScript, CSS, URL). For instance, when outputting data in an HTML context, use HTML entity encoding to prevent it from being interpreted as code.\n\n\nContent Security Policy (CSP):\n\nImplement a Content Security Policy to restrict the types of scripts that can run on your page. CSP can mitigate the impact of XSS by disallowing inline scripts or loading scripts from untrusted sources.\n\n\nUse Security Libraries and Frameworks:\n\nUse security libraries and frameworks that automatically handle escaping and sanitization, reducing the chances of XSS.\n\n\nHTTP-Only Cookies:\n\nSet cookies with the HttpOnly flag to prevent client-side scripts from accessing them, mitigating the risk of session theft via XSS.\n\n\n\nConclusion\nXSS is a significant security risk that can compromise the security of users interacting with a web application. Proper input validation, output encoding, and security practices are essential to protect against these vulnerabilities.\n\n\nwhat is cors\ndeveloper.mozilla.org/en-US/docs/Web/HTTP/CORS\n\n\nsecurityheaders.com/ to check wether the link is safe or not\n\n"},"notes/2024/Hosting-the-website":{"title":"Hosting the website","links":[],"tags":["deployment","devops","backend"],"content":"The rest of your content lives here. You can use Markdown here :)\nThe rest of your content lives here. You can use Markdown here :)\nHosting a MERN stack application involves deploying both the frontend (React) and backend (Node.js/Express with MongoDB) components, ensuring they communicate properly in a production environment. Below is a step-by-step guide to help you host your MERN stack sales engagement platform:\n\n1. Prepare Your MERN Stack Application for Deployment\nBefore hosting, ensure your app is production-ready:\nFrontend (React):\n\nBuild the React App:\n\nNavigate to your React frontend folder (e.g., client or frontend) in your terminal.\nRun: npm run build\nThis creates a build folder with optimized static files.\n\n\nServe the Frontend:\n\nYou can serve the React build folder using your Node.js backend or a static hosting service (more on this later).\n\n\n\nBackend (Node.js/Express):\n\n\nSet Up Environment Variables:\n\n\nCreate a .env file in your backend folder (e.g., server or api) to store sensitive data like your MongoDB connection string, API keys, and port number.\n\n\nExample .env:\nPORT=5000\nMONGO_URI=mongodb+srv://&lt;username&gt;:&lt;password&gt;@cluster0.mongodb.net/&lt;dbname&gt;?retryWrites=true&amp;w=majority\nNODE_ENV=production\n\n\nUse a package like dotenv in your backend code to load these variables:\n\n\n \nrequire(&#039;dotenv&#039;).config();\nconst PORT = process.env.PORT || 5000;\n\n\nUpdate API Endpoints:\n\nYour frontend likely uses Axios or fetch to call the backend (e.g., http://localhost:5000/api). In production, this needs to point to your deployed backend URL (e.g., your-backend-domain.com/api).\nYou can set this dynamically using environment variables or configure it during deployment.\n\n\n\nCombine Frontend and Backend (Optional):\n\nIf you want a single server, configure your Express app to serve the React build folder. Add this to your server.js:\n\nconst path = require(&#039;path&#039;);\napp.use(express.static(path.join(__dirname, &#039;../client/build&#039;)));\napp.get(&#039;*&#039;, (req, res) =&gt; {\n  res.sendFile(path.join(__dirname, &#039;../client/build/index.html&#039;));\n});\n\n\nDatabase (MongoDB):\n\nUse MongoDB Atlas (Recommended):\n\nSign up for MongoDB Atlas (a cloud-hosted MongoDB service).\nCreate a cluster, get the connection string (SRV address), and whitelist your server‚Äôs IP (or set it to 0.0.0.0/0 for broad access during testing‚Äîjust tighten security later).\nUpdate your backend to use this connection string via the .env file.\n\n\n\n\n2. Choose a Hosting Platform\nMERN stack apps require a server for the backend (Node.js) and a way to serve the frontend (static files). Here are some popular options:\nOption 1: All-in-One Hosting (e.g., Render, Vercel, Heroku)\nThese platforms simplify deployment by handling both frontend and backend.\n\nRender:\n\nSteps:\n\nPush your project to a GitHub repository.\nSign up for Render (free tier available).\nCreate a new ‚ÄúWeb Service‚Äù for your backend:\n\nLink your GitHub repo.\nSet the runtime to Node.\nSpecify the build command (e.g., npm install) and start command (e.g., node server.js).\nAdd environment variables (e.g., MONGO_URI, PORT).\n\n\nCreate a ‚ÄúStatic Site‚Äù for your frontend:\n\nLink the same repo, point to the client folder.\nSet the build command (e.g., npm run build) and publish directory (e.g., build).\n\n\nUpdate your frontend API calls to use the backend URL provided by Render (e.g., your-backend.onrender.com).\n\n\nPros: Free tier, easy setup, auto-scaling.\nCons: Free tier sleeps after inactivity, slower cold starts.\n\n\nVercel (Frontend) + Render/Heroku (Backend):\n\nSteps:\n\nDeploy the frontend on Vercel:\n\nPush your React app to GitHub.\nImport it into Vercel, select the client folder, and deploy (Vercel auto-detects React).\n\n\nDeploy the backend on Render (as above) or Heroku:\n\nOn Heroku, push your backend code, add a Procfile (e.g., web: node server.js), and set environment variables.\n\n\nUpdate frontend API calls to the backend URL.\n\n\nPros: Vercel is great for React, free tier available.\nCons: Requires splitting frontend and backend.\n\n\n\nOption 2: VPS (e.g., DigitalOcean, AWS EC2)\nFor more control, use a Virtual Private Server (VPS):\n\nSteps:\n\nSign up for DigitalOcean or AWS.\nLaunch a droplet/instance (e.g., Ubuntu server).\nSSH into the server and install Node.js, MongoDB (optional if using Atlas), and Git.\nClone your repo, install dependencies (npm install), and start your app (use pm2 for process management: npm install -g pm2, then pm2 start server.js).\nConfigure a reverse proxy with Nginx to serve your app and handle domains.\nSet up a domain (via a registrar like Namecheap) and point DNS to your server‚Äôs IP.\n\n\nPros: Full control, scalable, no sleep time.\nCons: Requires server management, paid (starting ~$5/month on DigitalOcean).\n\nOption 3: Shared Hosting (Not Recommended)\nTraditional shared hosting (e.g., Hostinger) is designed for static sites or PHP apps, not Node.js. You‚Äôd need a VPS plan to run a MERN stack app, making it similar to the VPS option above.\n\n3. Deploy Your Sales Engagement Platform\nLet‚Äôs assume you‚Äôre using Render for simplicity:\n\nPush to GitHub:\n\nEnsure your repo has both client (React) and server (Node.js) folders.\n\n\nDeploy Backend:\n\nOn Render, create a Web Service, point to the server folder, and configure as above.\n\n\nDeploy Frontend:\n\nCreate a Static Site, point to the client folder, and configure.\n\n\nTest:\n\nVisit the frontend URL (e.g., your-frontend.onrender.com) and ensure it connects to the backend (e.g., your-backend.onrender.com).\n\n\nDomain (Optional):\n\nAdd a custom domain via Render‚Äôs dashboard and update DNS records.\n\n\n\n\n4. Specific Considerations for a Sales Engagement Platform\nSince your app is a sales engagement platform, consider:\n\nScalability: Platforms like Render or AWS auto-scale, but monitor usage as your user base grows.\nSecurity: Use HTTPS (most platforms provide free SSL), secure your MongoDB Atlas cluster, and sanitize user inputs.\nPerformance: Optimize API calls and use a CDN (e.g., Cloudflare) for faster static file delivery.\nAnalytics: Integrate tools like Google Analytics or Mixpanel via your React frontend.\n\n\nResources for More Info\n\nRender Docs: render.com/docs/deploy-node-express-app\nVercel Docs: vercel.com/docs\nDigitalOcean Tutorials: Search for ‚ÄúDeploy MERN stack on DigitalOcean‚Äù\nMongoDB Atlas: www.mongodb.com/cloud/atlas\n\nIf you specify your hosting provider or preferred setup (e.g., free vs. paid), I can tailor this further! What‚Äôs your next step?\nreference:"},"notes/2024/How-Browser-works":{"title":"How Browser works","links":[],"tags":["browser","front-end","react","css"],"content":"\nStep 1 : First it will get file from remote server in and load  it and get the raw data (binary data)\nstep2 : convert that into the characters and tokenize the characters\nstep3 : convert the characters and build the objects for example\n{\n   tagName: &quot;h1&quot;,\n   text:&quot;asirfnioar&quot;,\n   children:\n}\nstep4: relationship among the models\nstep3: convert them as nodes (NodeList) ‚Üí DOM\nsame things for css will happen (CSSOM)\nHow a Web Browser Works: Architecture and Example\nA web browser is a software application that allows users to retrieve, view, and interact with content from the web, primarily in the form of web pages. The browser‚Äôs architecture consists of several components that work together to fetch web content, render it, and display it to the user.\nLet‚Äôs break down the architecture of a browser and how it works step-by-step with an example.\nBrowser Architecture Components\n\n\nUser Interface (UI):\n\nThis is what the user interacts with: the address bar, back/forward buttons, bookmarks, etc.\nExample: When you open a browser like Chrome or Firefox, the visible part is the user interface where you can type a URL, navigate pages, or view open tabs.\n\n\n\nBrowser Engine:\n\nThe browser engine manages actions between the UI and the rendering engine. It takes commands from the UI (like navigating to a new page) and forwards them to the rendering engine.\nExample: When you type a URL and hit Enter, the browser engine coordinates the fetching and rendering of that webpage.\n\n\n\nRendering Engine:\n\nThis component is responsible for parsing HTML, CSS, and JavaScript, and rendering the content onto the screen.\nExample: Once the page‚Äôs HTML and CSS are received, the rendering engine processes this information and displays the styled content on your screen.\n\nPopular rendering engines:\n\nBlink (used by Chrome, Edge)\nWebKit (used by Safari)\nGecko (used by Firefox)\n\n\n\nNetworking Layer:\n\nThis layer handles all network calls, like fetching resources (HTML files, images, CSS, etc.) from a web server via HTTP/HTTPS.\nExample: When you enter a URL, this layer sends an HTTP request to the server hosting the website, retrieves the response (HTML, CSS, JavaScript, etc.), and forwards it to the rendering engine.\n\n\n\nJavaScript Engine:\n\nBrowsers have a JavaScript engine to parse and execute JavaScript code. The engine runs scripts embedded in web pages.\nExample: If a webpage has interactive elements (like forms, buttons, etc.), the JavaScript engine executes the required logic to handle user input or animations.\nJavaScript Engines:\n\nV8 (used by Chrome, Edge)\nSpiderMonkey (used by Firefox)\nJavaScriptCore (used by Safari)\n\n\n\n\n\nUI Backend:\n\nThis component is used to draw the basic widgets (checkboxes, buttons, etc.) on the web page. It works closely with the rendering engine.\nExample: When a form on a webpage has checkboxes or radio buttons, the UI backend helps render them.\n\n\n\nData Storage (Persistence Layer):\n\nBrowsers have local storage mechanisms (like cookies, local storage, indexedDB) to store user data locally on the client side.\nExample: When you log into a website and choose to ‚Äústay logged in‚Äù, the browser stores some data (like session tokens) in local storage.\n\n\n\nNetworking Layer:\n\nManages network calls like DNS lookup, HTTP/HTTPS requests, and so on.\nExample: This layer sends out the request when you visit example.com and brings back the HTML response from the server.\n\n\n\n\nHow a Browser Works: Example Walkthrough\nLet‚Äôs say you open a web browser (e.g., Google Chrome) and type example.com in the address bar and press Enter. Here‚Äôs a simplified step-by-step breakdown of what happens:\n\nInput (User Interface):\n\nYou enter example.com in the address bar and press Enter.\n\n\nNetworking (Browser Engine and Networking Layer):\n\nThe browser checks if example.com is cached locally. If not, it starts the process of retrieving the page from the server.\nThe browser sends a DNS request to find the IP address of example.com and then sends an HTTP request to that server for the page.\n\n\nServer Response:\n\nThe server responds with an HTML file (and possibly CSS, JavaScript files, etc.) to the browser.\n\n\nRendering Process (Rendering Engine):\n\nThe HTML is parsed into the DOM tree (Document Object Model), which represents the structure of the web page.\nThe CSS is parsed into a CSSOM tree (CSS Object Model), defining how elements should be styled (e.g., colors, fonts, layouts).\nThe rendering engine combines the DOM tree and CSSOM tree to form the Render Tree, which will be used to display content visually.\n\n\nLayout &amp; Painting:\n\nLayout: The browser calculates the exact position and size of each element on the screen based on the render tree.\nPainting: The visual representation (pixels) is drawn to the screen.\n\n\nJavaScript Execution (JavaScript Engine):\n\nIf the HTML contains JavaScript, the JavaScript engine (e.g., V8 for Chrome) parses and executes it. JavaScript can manipulate the DOM and CSSOM trees dynamically (e.g., add new elements, update styles).\n\n\nDisplay (User Interface Backend):\n\nThe fully rendered page is displayed in the browser window.\nExample: You see the content of example.com now rendered on your screen.\n\n\n\nExample Interaction\nIf the page has a button (e.g., ‚ÄúClick Me‚Äù), and that button has JavaScript associated with it, here‚Äôs how the browser will handle it:\n\nYou click the button.\nThe JavaScript engine processes the button‚Äôs event handler (code that should run when the button is clicked).\nBased on the code, the JavaScript engine might manipulate the DOM (e.g., show a message or change the button color).\nThe rendering engine then repaints the part of the screen where the button is to reflect the changes.\n\n\nKey Performance Aspects\n\nRendering Performance: Browsers try to optimize rendering to avoid unnecessary recalculations and repaints (e.g., reflow or layout thrashing).\nJavaScript Optimization: JavaScript engines use Just-In-Time (JIT) compilation to optimize code execution speed.\nNetwork Optimization: Browsers employ techniques like HTTP caching, DNS prefetching, and resource prioritization to improve page load times.\n\nConclusion\nA web browser is a complex system with many interworking components. From sending network requests to rendering web pages, the architecture is designed to handle a wide range of tasks to provide a seamless browsing experience. By splitting up these components, the browser can efficiently manage everything from HTML parsing to JavaScript execution and CSS styling."},"notes/2024/Indexes-in-mongodb":{"title":"Indexes in mongodb","links":[],"tags":["mongodb","database","indexing"],"content":"In mongodb indexes are used to speed up the query , suppose if your application depends some particular field and you making frequent querying in that document you can create index for that field\nTypes of Index\n\nsingle field index\ncompound index\n\nSingle field index\nIn single field index we can create a index for top level document and embedded document also.\nsyntax for creating the single field index\ndb.&lt;collection&gt;.createIndex( { &lt;field&gt;: &lt;sortOrder&gt; } )\ndb.student.createIndex(‚Äùname‚Äù:1);\nIn the above query we are creating the index for the name field in student collection in ascending order\nsuppose you want to create a index for the embedded field ,\ndb.student.createIndex(‚Äùlocation.city‚Äù:1)\nhow to check if the query uses index\ndb.&lt;collection&gt;.explain(‚ÄùexecutionStats‚Äù)\nOptimize the query performance\nwww.mongodb.com/docs/manual/tutorial/optimize-query-performance-with-indexes-and-projections/#std-label-optimize-query-performance\n\ncreate index for the field which is frequently searched\ncreate compound index if your getting the document with multiple key\nlimit the number document\nuse projection\nuse $hint to use the particular document\nuser  $inc to increment the value in the server side\nQuery Selectivity\n\nQuery Selectivity\nwww.mongodb.com/docs/manual/core/query-optimization/#std-label-indexes-covered-queries\nFor instance, the inequality operators¬†$nin¬†and¬†$ne¬†are¬†not¬†very selective since they often match a large portion of the index. As a result, in many cases, a¬†$nin¬†or¬†$ne¬†query with an index may perform no better than a¬†$nin¬†or¬†$ne¬†query that must scan all documents in a collection.\ncovered query\nCovered query is the query that can be entierly  using the index and doesn&#039;t  have to scan the other documents\nWhen we say the query is covered ?\n\nall the fields in the query are part of the index\nall the fields returned in the results are in the same index\nno fields in the query are  equal to null\n\nexample :\ndb.Collection.createIndex({name:1,email:1});\n \ndb.Collection.find({name:&quot;sanjay&quot;},{name:1,_id:0})\n \n// this is the example for the covered query\nCompound Index\nCompound index is used to create the index for the two or more fields , it is more helpful to make the query covered\nSyntax :\ndb.&lt;collection&gt;.createIndex( {\n&lt;field1&gt;: &lt;sortOrder&gt;,\n&lt;field2&gt;: &lt;sortOrder&gt;,\n...\n&lt;fieldN&gt;: &lt;sortOrder&gt;\n} )\n\nexample :\ndb.employes.creaetIndex({name:1,email:1})\nusecase\nIf your application repeatedly runs a query that contains multiple fields, you can create a compound index to improve performance for that query. For example, a grocery store manager often needs to look up inventory items by name and quantity to determine which items are low stock. You can create a compound index on both the¬†item¬†and¬†quantity¬†fields to improve query performance.\nA compound index on commonly queried fields increases the chances of¬†covering¬†those queries. Covered queries are queries that can be satisfied entirely using an index, without examining any documents. This optimizes query performance.\nLimitation using Compound Index\n\nField Limit ‚Üí 32 field per index\nField Order ‚Üí The order of the indexed fields impacts the effectiveness of a compound index. Compound indexes contain references to documents according to the order of the fields in the index. To create efficient compound indexes, follow the¬†ESR (Equality, Sort, Range) rule.\n\nUsing Indexes to Sort Query Results\nIndexes and Sorting:\n\nMongoDB uses indexes to efficiently sort query results. If your query involves sorting on fields that are part of an index, MongoDB can quickly retrieve the results in the specified order.\nAn index is a data structure that holds ordered records, so if your query‚Äôs sort criteria match an index, MongoDB leverages this to avoid scanning and sorting all documents in the collection.\n\nMultiple Indexes:\n\nIf a query predicate (the condition specified in the query) uses the same fields as the sort criteria, MongoDB might use multiple indexes to support the sort operation.\nThis means if your query both filters and sorts on indexed fields, MongoDB can optimize the operation using these indexes.\n\nWhen Indexes Can‚Äôt Be Used\nBlocking Sort:\n\nIf MongoDB cannot use an index to obtain the sort order (for instance, if the sort field is not indexed), it must perform a blocking sort.\nA blocking sort involves consuming and processing all input documents to produce the sorted output. This operation needs to load all relevant documents into memory, sort them, and then return the results.\nAlthough the term ‚Äúblocking‚Äù might suggest a stop to all operations, it does not block concurrent operations on the collection or database. It only means that the sort operation itself must complete before the results are returned.\n\nMemory Usage and Disk Spilling\nMemory Limits:\n\nStarting with MongoDB 6.0, if a pipeline execution stage (like sorting) requires more than 100 megabytes (MB) of memory, MongoDB automatically writes temporary files to disk to handle the excess, unless you specify allowDiskUse: false.\nThis is to prevent the system from running out of memory during complex or large sorting operations.\n\nHandling Large Memory Requirements:\n\nIf the server needs more than 100 MB of system memory for a blocking sort and you haven‚Äôt enabled disk use (via cursor.allowDiskUse()), MongoDB will return an error.\nThe option allowDiskUse() allows the query to use disk storage to handle large data sets, preventing memory-related errors.\n\nExample for Clarity\nSuppose you have a collection of users with fields name and age, and you want to sort the results by age.\n\nUsing Index:\n\nIf there‚Äôs an index on age (e.g., db.users.createIndex({ age: 1 })), MongoDB uses this index to quickly sort the documents by age.\nThe query db.users.find().sort({ age: 1 }) will be efficient as MongoDB leverages the age index.\n\n\nWithout Index:\n\nIf there‚Äôs no index on age, MongoDB must perform a blocking sort.\nFor large collections, this could be memory-intensive. If the operation requires more than 100 MB of memory, MongoDB will spill to disk if allowDiskUse is enabled: db.users.find().sort({ age: 1 }).allowDiskUse().\nWithout enabling disk use, and if the sort exceeds 100 MB of memory, the query will fail.\n\n\n\nBy understanding these concepts, you can better optimize your queries and indexing strategy to ensure efficient sorting and avoid memory-related issues in MongoDB.\nSort on Multiple Fields\n\nCreate a¬†compound index¬†to support sorting on multiple fields.\nYou can specify a sort on all the keys of the index or on a subset; however, the sort keys must be listed in the¬†same order¬†as they appear in the index. For example, an index key pattern¬†{ a: 1, b: 1 }¬†can support a sort on¬†{ a: 1, b: 1 }¬†but¬†not¬†on¬†{ b: 1, a: 1 }.\nFor a query to use a compound index for a sort, the specified sort direction for all keys in the¬†cursor.sort()¬†document must match the index key pattern¬†or¬†match the inverse of the index key pattern. For example, an index key pattern¬†{ a: 1, b: -1 }¬†can support a sort on¬†{ a: 1, b: -1 }¬†and¬†{ a: -1, b: 1 }¬†but¬†not¬†on¬†{ a: -1, b: -1 }¬†or¬†{a: 1, b: 1}.\nSort and Index Prefix\n\nIf the sort keys correspond to the index keys or an index¬†prefix, MongoDB can use the index to sort the query results. A¬†prefix¬†of a compound index is a subset that consists of one or more keys at the start of the index key pattern.\nFor example, create a compound index on the¬†data¬†collection:\ndb.data.createIndex( {a:1,b:1,c:1,d:1 } )\nThen, the following are prefixes for that index:\n{a:1 }{a:1,b:1 }{a:1,b:1,c:1 }\nThe following query and sort operations use the index prefixes to sort the results. These operations do not need to sort the result set in memory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExampleIndex Prefixdb.data.find().sort( { a: 1 } ){ a: 1 }db.data.find().sort( { a: -1 } ){ a: 1 }db.data.find().sort( { a: 1, b: 1 } ){ a: 1, b: 1 }db.data.find().sort( { a: -1, b: -1 } ){ a: 1, b: 1 }db.data.find().sort( { a: 1, b: 1, c: 1 } ){ a: 1, b: 1, c: 1 }db.data.find( { a: { $gt: 4 } } ).sort( { a: 1, b: 1 } ){ a: 1, b: 1 }\nEnsure Indexes Fit in the RAM\nFor the fastest processing, ensure that your indexes fit entirely in RAM so that the system can avoid reading the index from disk.\nTo check the size of your indexes, use the¬†db.collection.totalIndexSize()¬†helper, which returns data in bytes:\n&gt; db.collection.totalIndexSize()\n4617080000\nThe above example shows an index size of almost 4.3 gigabytes. To ensure this index fits in RAM, you must not only have more than that much RAM available but also must have RAM available for the rest of the¬†working set. Also remember:\nIf you have and use multiple collections, you must consider the size of all indexes on all collections. The indexes and the working set must be able to fit in memory at the same time."},"notes/2024/Mastering-Database-design":{"title":"Mastering Database Design","links":[],"tags":["database"],"content":"The rest of your content lives here. You can use Markdown here :)\nMastering database design and schema design is a critical skill for developers and data engineers. It involves understanding business requirements, data modeling techniques, and performance optimization strategies. Here‚Äôs a structured approach:\n\n1. Build a Strong Foundation\n\nUnderstand Database Types:\n\nLearn the differences between relational databases (e.g., MySQL, PostgreSQL) and non-relational databases (e.g., MongoDB, Cassandra).\nUnderstand when to use SQL vs NoSQL databases.\n\n\nStudy Core Concepts:\n\nData normalization and denormalization.\nPrimary keys, foreign keys, and indexes.\nRelationships (one-to-one, one-to-many, many-to-many).\nTransactions and ACID properties.\n\n\n\n\n2. Learn Data Modeling\n\nER Diagrams:\n\nLearn to design Entity-Relationship (ER) diagrams to visually represent data and relationships.\n\n\nTools for Modeling:\n\nUse tools like MySQL Workbench, dbdiagram.io, or Lucidchart for schema design.\n\n\nPractice Logical and Physical Design:\n\nLogical design: Focus on relationships, attributes, and data requirements.\nPhysical design: Focus on storage, indexing, and performance.\n\n\n\n\n3. Apply Normalization\n\nUnderstand and practice the normalization process:\n\n1NF (First Normal Form): Eliminate duplicate columns.\n2NF (Second Normal Form): Eliminate partial dependency.\n3NF (Third Normal Form): Eliminate transitive dependency.\n\n\nLearn when to denormalize for performance gains (e.g., for read-heavy applications).\n\n\n4. Understand Business Requirements\n\nCollaborate with stakeholders to gather requirements.\nTranslate business processes into database entities and relationships.\n\n\n5. Practice with Real-World Scenarios\n\nDesign schemas for different use cases, such as:\n\nE-commerce applications (products, orders, users).\nSocial media platforms (posts, likes, comments).\nAnalytics systems (event tracking, logs).\n\n\n\n\n6. Optimize for Performance\n\nIndexing:\n\nUnderstand when and how to use indexes.\nLearn about clustered vs non-clustered indexes.\n\n\nQuery Optimization:\n\nWrite efficient queries and analyze them using tools like EXPLAIN or EXPLAIN ANALYZE.\n\n\nPartitioning and Sharding:\n\nUnderstand database partitioning and sharding for scaling large datasets.\n\n\nCaching:\n\nUse caching mechanisms (e.g., Redis, Memcached) to reduce database load.\n\n\n\n\n7. Understand Security Best Practices\n\nEncrypt sensitive data.\nUse roles and permissions to control access.\nProtect against SQL injection attacks.\n\n\n8. Learn Advanced Concepts\n\nDatabase migrations and versioning.\nEvent sourcing and CQRS patterns.\nData warehousing and ETL pipelines.\nDatabase replication and high availability.\n\n\n9. Use Resources\n\nBooks:\n\nDesigning Data-Intensive Applications by Martin Kleppmann.\nDatabase Design for Mere Mortals by Michael J. Hernandez.\n\n\nCourses:\n\nDatacamp, Udemy, or Coursera database design courses.\n\n\nPractice Platforms:\n\nLeetCode and HackerRank for database-related problems.\nKaggle datasets for schema design practice.\n\n\n\n\n10. Get Feedback and Iterate\n\nShare your designs with peers or mentors for feedback.\nContinuously refactor your schemas based on usage patterns and scaling needs.\n\n\n11. Stay Updated\n\nFollow blogs like High Scalability or the Database Performance Blog.\nExplore the documentation and new features of popular database systems (e.g., PostgreSQL, MongoDB).\n\nBy combining these steps with consistent practice and exposure to diverse projects, you‚Äôll develop a mastery of database design and schema creation."},"notes/2024/Message-Broker-ChatGPT-Learnings":{"title":"Message Broker ChatGPT Learnings","links":[],"tags":["message_broker","message_queues","backend"],"content":"In RabbitMQ, workers and consumers are related but not necessarily the same thing. Here‚Äôs the distinction:\n\nConsumer:\n\nA consumer in RabbitMQ refers to any application or service that subscribes to a queue and processes the messages from that queue. When a consumer connects to RabbitMQ and subscribes to a queue, RabbitMQ delivers messages to the consumer. Each message is acknowledged (either automatically or manually) after being processed.\nConsumers can be a part of any service that processes messages from queues, such as a web service, background job processor, or any application that needs to handle asynchronous tasks.\n\n\nWorker:\n\nA worker is often used in the context of worker queues or background job processing. It refers to a process (or thread) that does the actual work of processing a message. A worker pulls messages off a queue and handles them, typically performing some computation or task before acknowledging the message.\nIn many setups, a worker acts as a consumer. However, the term worker generally emphasizes the processing task being done by the service, while consumer is the RabbitMQ term for the entity receiving messages.\n\n\n\nSummary:\n\nA worker often plays the role of a consumer, but consumer is the more accurate term in the RabbitMQ context, specifically referring to the service that consumes messages from the queue.\nThe term worker is more common in job processing systems and emphasizes the work being done after consuming the message.\n\n1. Producer\n\nA producer is a user application that sends messages to the RabbitMQ broker. The producer doesn‚Äôt directly interact with queues but rather with exchanges.\nProducers publish messages to an exchange, and it is the exchange‚Äôs responsibility to route the message to the appropriate queue(s).\n\n2. Exchange\n\nAn exchange is responsible for routing messages to one or more queues. When a message is sent to RabbitMQ, it is sent to an exchange first, not directly to a queue.\nThere are different types of exchanges, which determine how messages are routed:\n\nDirect Exchange: Routes messages to queues based on an exact match between the message‚Äôs routing key and the queue‚Äôs binding key.\nTopic Exchange: Routes messages to queues based on pattern matching of the routing key (wildcards like ‚Äú or # can be used).\nFanout Exchange: Broadcasts messages to all queues bound to the exchange, regardless of the routing key.\nHeaders Exchange: Routes messages based on matching message headers rather than routing keys.\n\n\n\n3. Queue\n\nA queue is where messages are stored until they are consumed by a consumer. Queues in RabbitMQ are FIFO (first in, first out).\nQueues are bound to exchanges, and the exchange determines which messages are routed to which queue based on routing rules.\nQueues can have various attributes, such as being durable (persisted to disk), exclusive (restricted to one connection), or auto-delete (deleted when no consumers are connected).\n\n4. Binding\n\nA binding defines the relationship between an exchange and a queue. It specifies how messages sent to the exchange should be routed to the queue.\nA binding key is often used in direct and topic exchanges to determine which messages should be delivered to which queues.\n\n5. Consumer\n\nA consumer is an application that retrieves and processes messages from a queue.\nConsumers subscribe to queues, and RabbitMQ delivers messages from the queue to the consumer. Once the message is processed, the consumer typically sends an acknowledgment (ack) back to RabbitMQ.\n\n6. Message\n\nA message in RabbitMQ consists of two parts:\n\nMessage Body: The content or payload being transmitted (e.g., JSON, XML, plain text).\nMessage Properties/Headers: Metadata associated with the message, such as delivery mode, expiration time, or custom headers.\n\n\n\n7. Connection and Channel\n\nA connection is a TCP connection between the producer/consumer and the RabbitMQ broker.\nA channel is a virtual connection that runs over a single TCP connection. Channels are lightweight and can be used to multiplex multiple logical conversations (publishing, consuming) over a single connection, which is resource-efficient.\n\n8. Virtual Host (vhost)\n\nA virtual host is a way to logically separate different applications using the same RabbitMQ instance. Each virtual host can have its own set of queues, exchanges, and bindings.\nIt allows for multi-tenancy within a RabbitMQ broker by providing isolated environments for different applications.\n\n9. Broker\n\nThe broker is the RabbitMQ server itself. It receives messages from producers, routes them through exchanges to queues, and delivers them to consumers. It also handles message persistence, delivery acknowledgment, and clustering for fault tolerance and scalability.\n\n10. Routing Key\n\nA routing key is used by the producer when sending a message to an exchange. The routing key, in combination with the exchange type, determines how the message should be routed to one or more queues.\n\n11. Acknowledgment (Ack)\n\nAcknowledgment is the confirmation sent by a consumer to RabbitMQ indicating that the message was received and processed successfully.\nIf a message is not acknowledged (Nack or rejected), RabbitMQ can either requeue it for another consumer to process or discard it based on the queue settings.\n\n12. Dead Letter Exchange (DLX)\n\nA Dead Letter Exchange is used to capture messages that are rejected or not processed (e.g., due to expiration or consumer failure).\nQueues can be configured to route unprocessed messages to a DLX for further inspection or reprocessing.\n\n13. Prefetch Count\n\nPrefetch count controls how many messages RabbitMQ sends to a consumer at a time without receiving an acknowledgment. It is part of the quality-of-service (QoS) mechanism to avoid overloading consumers.\n\n\nSummary Diagram:\n\nProducer ‚Üí Exchange ‚Üí Queue ‚Üí Consumer\nBinding connects Exchange to Queue, and Routing Key helps with the routing decision.\n\nEach of these abstractions allows developers to work with RabbitMQ‚Äôs messaging system in a flexible and decoupled manner, building complex and scalable messaging patterns.\nNode.js Example Using Direct Exchange in RabbitMQ\nHere‚Äôs a step-by-step example using Node.js with the amqplib package, demonstrating how to set up a Direct Exchange.\nInstallation\nFirst, install the amqplib package in your Node.js project:\nnpm install amqplib\n \nScenario\n\nExchange Name: direct_logs\nQueues: errorQueue, infoQueue, warningQueue\nRouting Keys:\n\nMessages with the routing key error will go to the errorQueue.\nMessages with the routing key info will go to the infoQueue.\nMessages with the routing key warning will go to the warningQueue.\n\n\n\nProducer Code (Sending Messages)\nThe producer will send messages to the direct_logs exchange with different routing keys (error, info, warning).\n \nconst amqp = require(&#039;amqplib&#039;);\n \nasync function produceMessage() {\n  try {\n    const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n    const channel = await connection.createChannel();\n \n    const exchange = &#039;direct_logs&#039;;\n    const msg = process.argv.slice(2).join(&#039; &#039;) || &#039;Hello World!&#039;;\n    const severity = process.argv[2] || &#039;info&#039;;  // severity represents routing key (e.g., &#039;info&#039;, &#039;error&#039;)\n \n    // Declare an exchange\n    await channel.assertExchange(exchange, &#039;direct&#039;, {\n      durable: false\n    });\n \n    // Publish message to the exchange with a specific routing key\n    channel.publish(exchange, severity, Buffer.from(msg));\n    console.log(`[x] Sent ${severity}: &#039;${msg}&#039;`);\n \n    // Close the connection and channel\n    setTimeout(() =&gt; {\n      channel.close();\n      connection.close();\n    }, 500);\n \n  } catch (error) {\n    console.error(&#039;Error in producer:&#039;, error);\n  }\n}\n \nproduceMessage();\n \nConsumer Code (Receiving Messages)\nThe consumer will listen to the direct_logs exchange and bind different queues with specific routing keys (error, info, warning).\n \nconst amqp = require(&#039;amqplib&#039;);\n \nasync function consumeMessage(routingKey) {\n  try {\n    const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n    const channel = await connection.createChannel();\n \n    const exchange = &#039;direct_logs&#039;;\n \n    // Declare an exchange\n    await channel.assertExchange(exchange, &#039;direct&#039;, { durable: false });\n \n    // Create a queue with a unique name for each consumer\n    const q = await channel.assertQueue(&#039;&#039;, { exclusive: true });\n    console.log(`[x] Waiting for messages in ${q.queue}. To exit press CTRL+C`);\n \n    // Bind the queue to the exchange with the specified routing key\n    await channel.bindQueue(q.queue, exchange, routingKey);\n \n    // Consume messages\n    channel.consume(q.queue, (msg) =&gt; {\n      if (msg.content) {\n        console.log(`[x] Received ${routingKey}: ${msg.content.toString()}`);\n      }\n    }, { noAck: true });\n \n  } catch (error) {\n    console.error(&#039;Error in consumer:&#039;, error);\n  }\n}\n \n// Pass &#039;error&#039;, &#039;info&#039;, or &#039;warning&#039; as the routing key you want to listen to\nconsumeMessage(process.argv[2] || &#039;info&#039;);\n \nRunning the Code\n1. Start Consumers\nEach consumer will listen to a different routing key. Open multiple terminals and run the consumer for different keys:\n\nTerminal 1 (Listening for error messages):\n\n  node consumer.js error\n\nTerminal 2 (Listening for info messages):\n\n\tnode consumer.js info\n\nTerminal 3 (Listening for warning messages):\n\nnode consumer.js warning\n2. Send Messages (Producer)\nSend different messages with various routing keys. Open another terminal and run the producer:\n\nSend an info message:\n\n   node producer.js info &quot;This is an info message&quot;\n\nSend an error message:\n\n node producer.js error &quot;This is an error message&quot;\n\nSend a warning message:\n\n\tnode producer.js warning &quot;This is a warning message&quot;\nExplanation\n\nThe Producer sends messages to the direct_logs exchange with a specific routing key (info, error, warning).\nThe Consumers are bound to the same exchange (direct_logs) but are listening for different routing keys.\n\nOne consumer listens for info messages.\nAnother listens for error messages.\nAnother listens for warning messages.\n\n\n\nOnly the consumer with a matching routing key will receive the corresponding message.\nExample Output\nProducer:\n[x] Sent error: &#039;This is an error message\nConsumer (Error Queue):\n[x] Received error: This is an error message\n \nSimilarly, the other consumers will only receive messages matching their routing key.\nThis demonstrates a Direct Exchange where messages are routed based on specific routing keys.\n1. Durable Queues\nA durable queue ensures that the queue itself survives broker restarts. This means that even if RabbitMQ crashes or is restarted, the queue will still exist and retain any messages that were not yet delivered.\nExample\n\nYou declare a durable queue so that it will persist even if RabbitMQ restarts:\n\nchannel.assertQueue(&#039;my_durable_queue&#039;, { durable: true });\n\nIf RabbitMQ is restarted, the queue will still be available, though any non-persistent messages in the queue may be lost (depending on how they were published).\nPersistent messages will be stored to disk if you set the message to be persistent:\n\n \nchannel.sendToQueue(&#039;my_durable_queue&#039;, Buffer.from(&#039;This is a message&#039;), { persistent: true });\n \n\nIn this case, both the queue and messages will survive a restart.\n\nNon-Durable Queue:\nIf you set { durable: false }, the queue will be deleted when RabbitMQ restarts, and all the messages in it will be lost.\n2. Exclusive Queues\nAn exclusive queue is restricted to the connection that declared it. Only this connection can use the queue, and the queue will be deleted when the connection is closed.\nExample\n\nDeclaring an exclusive queue:\n\nchannel.assertQueue(&#039;my_exclusive_queue&#039;, { exclusive: true });\n \n\nThe queue will only be accessible by the connection that declared it. If you open another connection, it won‚Äôt be able to access this queue.\nWhen the connection that created the queue is closed, the queue is automatically deleted.\n\nUse Case:\nExclusive queues are useful in cases where you want to ensure that only one instance of a consumer (or application) uses the queue. This is typically for temporary queues where you don‚Äôt need persistence or multiple consumers.\n3. Auto-Delete Queues\nAn auto-delete queue is automatically deleted when its last consumer unsubscribes or disconnects from it. The queue exists as long as there is at least one consumer consuming messages, but once the last consumer disconnects, the queue is deleted.\nExample\n\nDeclaring an auto-delete queue:\n\nchannel.assertQueue(&#039;my_auto_delete_queue&#039;, { autoDelete: true });\n \n\nWhen all consumers are disconnected, RabbitMQ will delete the queue.\nIf a consumer connects again later, they must declare the queue again because it will no longer exist.\n\nUse Case:\nAuto-delete queues are useful for temporary or ephemeral messaging setups, such as real-time chat systems or job queues, where you don‚Äôt need the queue to persist if there are no active consumers.\n\nPutting It Together with a Full Example\nLet‚Äôs say you want to create a durable, exclusive, and auto-delete queue:\nProducer Code:\nconst amqp = require(&#039;amqplib&#039;);\n \nasync function produceMessage() {\n  const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n  const channel = await connection.createChannel();\n \n  // Declare a queue with all three attributes: durable, exclusive, and auto-delete\n  const queue = &#039;my_special_queue&#039;;\n  await channel.assertQueue(queue, {\n    durable: true,      // Queue survives broker restarts\n    exclusive: true,    // Only accessible by the current connection\n    autoDelete: true    // Deleted when no consumers are connected\n  });\n \n  const msg = &#039;Hello World!&#039;;\n  channel.sendToQueue(queue, Buffer.from(msg), { persistent: true });\n  console.log(`[x] Sent &#039;${msg}&#039;`);\n \n  setTimeout(() =&gt; {\n    connection.close();\n  }, 500);\n}\n \nproduceMessage();\n \nConsumer Code:\n \nconst amqp = require(&#039;amqplib&#039;);\n \nasync function consumeMessage() {\n  const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n  const channel = await connection.createChannel();\n \n  const queue = &#039;my_special_queue&#039;;\n \n  // Declare the same queue, consumers should match the declaration\n  await channel.assertQueue(queue, {\n    durable: true,\n    exclusive: true,\n    autoDelete: true\n  });\n \n  console.log(`Waiting for messages in ${queue}`);\n \n  // Start consuming messages from the queue\n  channel.consume(queue, (msg) =&gt; {\n    console.log(`[x] Received: ${msg.content.toString()}`);\n  }, { noAck: true });\n}\n \nconsumeMessage();\n \nBehavior Breakdown:\n\nDurable:\n\nIf RabbitMQ restarts, the queue will still exist because it is durable.\nMessages will survive if they were marked as persistent (persistent: true in the message options).\n\n\nExclusive:\n\nOnly the connection that declared the queue will have access to it. If a second producer or consumer tries to access the same queue, it will receive an error.\nIf the connection that declared the queue closes, the queue is automatically deleted.\n\n\nAuto-Delete:\n\nIf the consumer disconnects (closes its connection), RabbitMQ will automatically delete the queue because it has no more consumers.\nIf a new consumer wants to consume from this queue later, they would need to re-declare it because it no longer exists.\n\n\n\nExample Output\n\nProducer:\n\n[x] Sent &#039;Hello World!&#039;\n \n\nConsumer (when receiving a message):\n\nWaiting for messages in my_special_queue\n[x] Received: Hello World!\n \nOnce the consumer disconnects, the queue will be deleted automatically if it‚Äôs the last consumer.\nSummary of Attributes:\n\nDurable: Ensures the queue survives RabbitMQ restarts.\nExclusive: Restricts the queue to a single connection and deletes it when the connection closes.\nAuto-Delete: Automatically deletes the queue when no consumers are connected.\n\nThese attributes are helpful in designing different messaging patterns depending on the persistence and exclusivity requirements of your application.\nWhat is a Connection?\nA connection in RabbitMQ is a TCP connection between your application (producer/consumer) and the RabbitMQ broker (the server). It‚Äôs like having a direct link to RabbitMQ.\n\nEach connection requires system resources (like memory and CPU) to manage, so opening many connections can be expensive.\nTypically, you open one connection per application.\n\nWhat is a Channel?\nA channel is a virtual connection that runs on top of a single TCP connection. Channels allow you to do multiple things (like publishing messages and consuming messages) over a single connection without creating additional connections.\n\nChannels are lightweight and can be created very quickly.\nYou can open multiple channels on a single connection, allowing you to publish and consume messages without creating extra TCP connections. This is more efficient than opening multiple TCP connections.\n\nAnalogy\nImagine a connection is like a highway. You build one big highway (TCP connection) between your application and RabbitMQ. Opening multiple highways would be costly (using more system resources).\nA channel is like an individual lane on the highway. Multiple lanes (channels) can be used to handle different traffic (publishing or consuming messages) without building new highways. Each lane (channel) can carry different cars (messages) at the same time.\nExample\nLet‚Äôs create a RabbitMQ setup using a single connection but two channels: one for producing messages and one for consuming messages.\n1. Create a Connection and Channels in Node.js\nHere‚Äôs an example in Node.js using the amqplib package:\nStep 1: Install amqplib if you haven‚Äôt already\nbash\nCopy code\nnpm install amqplib\n \nStep 2: Producer and Consumer Example\nIn this example, we will create:\n\nA single connection.\nTwo channels: one for sending (producer), one for receiving (consumer).\n\nProducer (Send messages using one channel)\njs\nCopy code\nconst amqp = require(&#039;amqplib&#039;);\n\nasync function producer() {\n  try {\n    // Open a connection\n    const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n\n    // Create a channel (lane) for publishing messages\n    const channel = await connection.createChannel();\n\n    const queue = &#039;test_queue&#039;;\n    const msg = &#039;Hello, RabbitMQ!&#039;;\n\n    // Assert a queue (make sure it exists)\n    await channel.assertQueue(queue, { durable: false });\n\n    // Send a message to the queue\n    channel.sendToQueue(queue, Buffer.from(msg));\n    console.log(`[x] Sent: ${msg}`);\n\n    // Close the connection after a short delay to ensure message is sent\n    setTimeout(() =&gt; {\n      channel.close();\n      connection.close();\n    }, 500);\n\n  } catch (error) {\n    console.error(&#039;Producer error:&#039;, error);\n  }\n}\n\nproducer();\n\n\nConsumer (Receive messages using a different channel)\njs\nCopy code\nconst amqp = require(&#039;amqplib&#039;);\n\nasync function consumer() {\n  try {\n    // Open a connection (same as the producer)\n    const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n\n    // Create a separate channel (lane) for consuming messages\n    const channel = await connection.createChannel();\n\n    const queue = &#039;test_queue&#039;;\n\n    // Assert a queue (make sure it exists)\n    await channel.assertQueue(queue, { durable: false });\n\n    // Start consuming messages from the queue\n    console.log(`[x] Waiting for messages in ${queue}`);\n    channel.consume(queue, (msg) =&gt; {\n      console.log(`[x] Received: ${msg.content.toString()}`);\n    }, { noAck: true });\n\n  } catch (error) {\n    console.error(&#039;Consumer error:&#039;, error);\n  }\n}\n\nconsumer();\n\n\nWhat‚Äôs Happening?\n\nConnection:\n\nBoth the producer and consumer use the same TCP connection to RabbitMQ.\n\n\nChannels:\n\nIn the producer, we created a channel to publish a message to RabbitMQ.\nIn the consumer, we created another channel to receive messages from RabbitMQ.\nBoth channels are running over the same connection, but they handle different tasks (publishing vs consuming).\n\n\n\nWhy Use Channels?\nIf you didn‚Äôt use channels and instead created a new connection for every action (publishing and consuming), you would:\n\nHave more overhead due to opening and maintaining multiple TCP connections.\nConsume more system resources (CPU, memory, etc.).\n\nUsing channels allows multiple independent conversations (like publishing and consuming) to happen over a single TCP connection, which is more efficient.\nPractical Example:\n\nScenario: You have a web server that processes data and sends it to RabbitMQ.\nInstead of opening a new TCP connection every time you want to send or receive data, you open a single TCP connection and use multiple channels for different purposes:\n\nOne channel to publish messages (like sending new tasks).\nAnother channel to consume messages (like processing tasks).\n\n\n\nSummary:\n\nConnection: A TCP connection between your application and RabbitMQ. It‚Äôs heavier and resource-intensive.\nChannel: A lightweight, virtual connection that runs on top of a TCP connection. You can create multiple channels on a single connection to handle different tasks like publishing and consuming messages.\n\nIn this example, you used one connection but two channels ‚Äî one for sending messages and one for receiving them, making the system more resource-efficient."},"notes/2024/Mongodb-Connection-and-mongoose-behind-the-scene":{"title":"Mongodb Connection and mongoose behind the scene","links":[],"tags":["database","backend"],"content":"Mongoose is an Object Data Modeling (ODM) library for MongoDB and Node.js, which simplifies how you interact with MongoDB. It provides an abstraction over MongoDB‚Äôs native driver to work with collections and documents in a more structured way.\nHere‚Äôs a detailed breakdown of how MongoDB connection works with Mongoose, the behind-the-scenes processes, and how connectOptions influence these connections.\n1. Connecting to MongoDB with Mongoose\nWhen you connect to MongoDB with Mongoose, you typically use the mongoose.connect() method, which establishes a connection between your Node.js application and the MongoDB server.\nExample:\nconst mongoose = require(&#039;mongoose&#039;);\n \nmongoose.connect(&#039;mongodb://localhost:27017/mydatabase&#039;, {\n    useNewUrlParser: true,\n    useUnifiedTopology: true,\n    useCreateIndex: true,\n    useFindAndModify: false,\n});\nThe string &#039;mongodb://localhost:27017/mydatabase&#039; is the connection string that points to the MongoDB instance running on localhost at port 27017. It specifies the database (mydatabase) that you are connecting to.\nBehind the scenes process:\n\nDNS Resolution &amp; Connection: When the mongoose.connect() is called, Mongoose uses the MongoDB Node.js driver under the hood. The driver resolves the host name (e.g., localhost) to its IP address.\nEstablishing Connection: The driver then opens a TCP connection to the MongoDB instance. It sends authentication credentials (if needed) and opens communication.\nConnection Pooling: Mongoose creates a connection pool. A connection pool is a set of reusable connections between your application and MongoDB, which improves performance by avoiding the overhead of opening and closing connections for each request. The size of the pool can be controlled by the poolSize option in connectOptions.\nHandshaking: The MongoDB server and Mongoose driver perform a handshake to ensure compatibility (e.g., MongoDB version, authentication, and protocol version).\n\nOnce the connection is established, Mongoose can perform read and write operations using schemas and models.\n2. How Mongoose Reads and Writes Data\nMongoose uses Schemas and Models to handle the interaction between your application and MongoDB.\n\nSchema: Defines the structure of a MongoDB document. It includes data types, validation rules, and default values.\nModel: A wrapper for the MongoDB document that provides an interface to interact with the database (read/write).\n\nWriting Data:\nWhen you create a new document in MongoDB via Mongoose, here‚Äôs what happens:\n\nYou create a new instance of a model:\n\nconst User = mongoose.model(&#039;User&#039;, new mongoose.Schema({ name: String }));\nconst newUser = new User({ name: &#039;Alice&#039; });\n\nWhen you call newUser.save(), Mongoose:\n\nValidates the document based on the schema.\nConverts the JavaScript object into BSON (Binary JSON format).\nSends an insertOne operation to MongoDB.\nMongoDB writes the document to its collection, using the WiredTiger storage engine to persist the data.\n\n\n\nReading Data:\nWhen you query MongoDB using Mongoose (e.g., User.find()):\n\nMongoose compiles the query into MongoDB‚Äôs native query language.\nIt sends a find operation over the established connection.\nThe MongoDB server retrieves the document from storage, converts it from BSON to JSON, and returns it.\nMongoose converts the JSON back into a JavaScript object that conforms to the schema.\n\n3. connectOptions in Mongoose\nThe connectOptions parameter in mongoose.connect() allows you to configure the connection between your application and MongoDB. Some important options:\n\nuseNewUrlParser: This option tells Mongoose to use the new MongoDB connection string parser. The old parser was deprecated due to edge-case handling issues.\nuseUnifiedTopology: This option removes support for several connection mechanisms that MongoDB drivers previously supported but are no longer needed. It enables the new Server Discovery and Monitoring engine, improving how the driver handles failover and replica sets.\nuseCreateIndex: Ensures Mongoose uses MongoDB‚Äôs createIndex() instead of ensureIndex(). This is because ensureIndex() was deprecated in MongoDB.\nuseFindAndModify: This prevents Mongoose from using the deprecated findAndModify() method for update operations. Instead, it uses newer MongoDB functions like findOneAndUpdate(), findOneAndRemove(), etc.\npoolSize: This option sets the maximum number of sockets (connections) in the MongoDB connection pool. A larger pool size allows more concurrent operations, but consumes more memory.\n\n\tpoolSize: 10 // The default is 5\n\nsocketTimeoutMS: Specifies the time in milliseconds a socket stays open without activity before closing. This prevents unnecessary connections from staying open too long.\nconnectTimeoutMS: The amount of time Mongoose will wait to establish a connection before timing out. If MongoDB is down or unreachable, this controls how long Mongoose waits before failing.\n\nconnectTimeoutMS: 30000 // 30 seconds\n\nretryWrites: This enables retryable writes, which allows MongoDB to retry certain write operations automatically if they fail due to transient network errors.\n\n\tretryWrites: true\n\nauthSource: Defines which database the credentials are associated with when connecting to a MongoDB instance.\n\n    authSource: &#039;admin&#039;\nExample of connectOptions:\n \nmongoose.connect(&#039;mongodb://username:password@localhost:27017/mydatabase&#039;, {\n    useNewUrlParser: true,         // Use the new MongoDB connection string parser\n    useUnifiedTopology: true,      // Use the new server topology engine\n    useCreateIndex: true,          // Use `createIndex()` instead of deprecated `ensureIndex()`\n    useFindAndModify: false,       // Use `findOneAndUpdate()` rather than deprecated `findAndModify()`\n    poolSize: 10,                  // Max number of socket connections\n    connectTimeoutMS: 30000,       // Wait up to 30 seconds for a connection\n    retryWrites: true,             // Enable retryable writes\n    authSource: &#039;admin&#039;,           // Authenticate with the admin database\n});\n \n4. Process Flow of a Query in Mongoose\n\nConnection Established: Once the connection is opened, Mongoose sends the query (e.g., find, insertOne, update) to the MongoDB server using a pooled connection.\nMongoDB Execution: MongoDB processes the query using its internal mechanisms (indexing, query planner, execution engines).\nResponse Sent: MongoDB sends the query result (in BSON) over the connection.\nMongoose Response Handling: Mongoose converts the result into a JavaScript object and returns it.\n\nSummary\nMongoose simplifies MongoDB interaction by abstracting the connection process and data management. Under the hood, it uses the MongoDB Node.js driver for communication. The connectOptions are used to fine-tune the connection behavior, such as pooling, timeouts, and retry mechanisms, which can impact performance and reliability.\nConnection pooling is a technique used to manage multiple database connections efficiently. Instead of opening and closing a new database connection for each request or query, a connection pool maintains a set of open, reusable connections. When a new query or operation needs a connection, the pool provides an already established connection from its pool, saving the overhead of establishing a new connection. Once the operation is done, the connection is returned to the pool for reuse by another query.\nHow Connection Pooling Works:\n\nCreation: When the application starts, a pool of database connections is created. The size of the pool (number of connections) is usually predefined by parameters like poolSize.\nReuse: When the application makes a request to the database, instead of opening a new connection, it checks the pool. If there is an available connection, it is reused for the new request.\nReturn to Pool: After the database operation is completed, the connection is not closed. Instead, it is returned to the pool and can be reused for the next request.\nConnection Limits: The pool has a limited number of connections (e.g., 10 connections). If all connections in the pool are busy and a new request comes in, the application can either:\n\nWait until a connection becomes available.\nOpen a new connection (if allowed by the pool configuration).\nFail with a timeout or an error if no connections are available within a certain time frame.\n\n\n\nBenefits of Connection Pooling:\n\nPerformance Improvement: Creating and closing database connections is expensive in terms of time and resources. Reusing connections reduces the overhead of establishing new connections, leading to faster response times.\nResource Management: By limiting the number of active connections (through the poolSize), connection pooling helps manage server resources, preventing the database from being overwhelmed by too many connections.\nScalability: Pooling allows for better handling of concurrent database requests by reusing the available connections, enabling the application to scale more efficiently.\n\nMongoose and Connection Pooling:\nMongoose, like many database libraries, uses connection pooling by default through the underlying MongoDB Node.js driver.\nPooling in Mongoose:\n\nMongoose manages the pool of connections automatically when you connect to the MongoDB server.\nThe number of connections in the pool can be configured with the poolSize option.\n\nExample:\nmongoose.connect(&#039;mongodb://localhost:27017/mydatabase&#039;, {\n    poolSize: 10, // Maximum 10 concurrent connections in the pool\n    useNewUrlParser: true,\n    useUnifiedTopology: true\n});\n \nIn this example, Mongoose maintains a pool of 10 connections that are reused throughout the lifetime of the application. When a new query is executed, it uses one of the available connections from the pool, and when the query finishes, that connection is returned to the pool for future use.\nKey Configuration Options for Pooling in Mongoose:\n\npoolSize: Specifies the maximum number of connections in the pool. The default is 5, but you can adjust it based on the expected workload.\nsocketTimeoutMS: Defines how long a connection should remain open while idle before being closed.\nconnectTimeoutMS: Sets how long to wait when establishing a new connection before timing out.\n\nExample of Connection Pooling Workflow:\n\nThe application starts and initializes the connection pool (e.g., 10 connections).\nEach time the application needs to perform a database operation, it requests a connection from the pool.\nIf there is an available connection in the pool, it is assigned to handle the request.\nOnce the operation is done, the connection is returned to the pool.\nIf all connections are in use, new requests either wait for a free connection or fail with a timeout if no connection is available within a defined period.\n\nSummary:\nConnection pooling is a critical technique for optimizing database performance and managing resources effectively. By reusing established connections, it reduces the overhead of connecting and disconnecting from the database and provides faster response times in high-concurrency environments.\nbehaviour of Connections in a Pool\n\nInitial State:\nWhen the connection pool is first created (e.g., when the application starts), not all the connections are immediately opened. Instead, connections are opened on-demand when queries or operations are made, and the pool grows up to its maximum size (poolSize).\nActive and Idle Connections:\n\nSome connections in the pool will be active (open and in use) when queries or operations are being processed.\nOthers may be idle (open but not currently in use) if no queries are running. These idle connections remain open, waiting for the next operation to use them.\n\n\nConnection Timeout:\nIdle connections may not remain open forever. Depending on the configuration, if a connection stays idle (unused) for too long, it might be closed by the pool to free up resources. This can be controlled by settings like socketTimeoutMS or idleTimeout.\nRe-opening Connections:\nIf a connection is closed due to idling or other factors, the pool can automatically re-open a new connection when needed. For example, if all current connections are closed but a new query comes in, the pool will create and open a new connection (up to the maximum poolSize).\n\nKey Options Controlling Connection Pooling Behavior:\n\npoolSize: Defines the maximum number of connections that can exist in the pool at any given time. This controls the upper limit of connections that can remain open.\nsocketTimeoutMS: Determines how long a connection remains open while idle before being considered inactive and potentially closed by the pool. If a connection remains idle for longer than this timeout, it is closed.\nconnectTimeoutMS: Defines the maximum time allowed to establish a new connection before it times out.\nmaxIdleTimeMS: Sets the maximum time that a connection can remain idle before being closed. This is used to ensure that idle connections don‚Äôt stay open forever and consume resources unnecessarily.\n\nExample of Managing Connection Timeout in Mongoose:\nYou can configure the behavior of the connections using these options in Mongoose:\njavascript\nCopy code\nmongoose.connect(&#039;mongodb://localhost:27017/mydatabase&#039;, {\n    poolSize: 10,             // Maximum 10 concurrent connections\n    socketTimeoutMS: 30000,    // Close idle connections after 30 seconds\n    connectTimeoutMS: 10000,   // Wait up to 10 seconds for a connection\n    maxIdleTimeMS: 60000       // Maximum time a connection can be idle before closing\n});\n \nSummary:\nNot all connections in the pool remain open at all times. Connections are opened when needed and can remain open while idle. However, idle connections may eventually be closed if they are not used for a specified time, depending on the pool‚Äôs configuration. When the pool needs more connections (up to the poolSize), it will open them as necessary."},"notes/2024/Mongodb-Query":{"title":"Mongodb Query","links":[],"tags":["mongodb","database","query"],"content":"$$ - operator dynamically selects the fields value from the collections\nExample ‚Üí\ndb.collectionName.find({},{&quot;field_name&quot;:`$${fieldName}`})\nIt used for the custom projection, suppose if we want to use that field name with custom name the we can use $$ operator and set it in custom projection"},"notes/2024/Node-js-Concurrency":{"title":"Node js Concurrency","links":[],"tags":["nodejs","backend"],"content":"Optimizing Node.js Application Concurrency | Heroku Dev Center"},"notes/2024/Proxy-Server-4f9394ecc9a1467aa87cb46182aa25e3":{"title":"Proxy Server","links":[],"tags":["networking","proxy","backend"],"content":"Proxy Server\n\n\nReverse Proxy\nHere‚Äôs an example scenario illustrating the use of a reverse proxy:\nLet‚Äôs consider a company that operates an online retail website with multiple web servers behind the scenes. They use a reverse proxy server to manage incoming traffic to their website.\n\nClient Request: A customer visits the company‚Äôs website (e.g., www.example.com) in their web browser.\nRequest Intercepted by Reverse Proxy: The request first reaches the reverse proxy server, which acts as the entry point for all incoming traffic to the website.\nReverse Proxy Routes the Request: The reverse proxy examines the request and determines which web server behind the scenes should handle it. This decision could be based on various factors such as server load, geographic location, or specific content requested.\nForwarding the Request to Web Server: The reverse proxy forwards the client‚Äôs request to the selected web server.\nWeb Server Processes the Request: The web server receives the request from the reverse proxy and processes it accordingly. It generates the necessary response, such as serving a webpage or processing a transaction.\nResponse Sent Back to Reverse Proxy: Once the web server has processed the request, it sends the response back to the reverse proxy server.\nResponse Delivered to Client: Finally, the reverse proxy forwards the response from the web server to the client‚Äôs web browser, which displays the requested webpage or content.\n\nIn this example, the reverse proxy server acts as an intermediary between the client and the web servers, handling incoming requests and routing them to the appropriate backend servers based on predefined rules or algorithms. Reverse proxies are commonly used for load balancing, enhancing security, simplifying SSL termination, and providing caching services, among other purposes.\n\n\nNgrok\nNgrok  is used to make our local server as a live server by creating tunnel between the local and server and the internet , it is mostly used in web development  to test the web hook integration,demo website etc and it enables to access the local server remotely\nHow Ngrok Works:\n\nClient-Server Architecture: Ngrok operates on a client-server architecture. The Ngrok client runs on your local machine, while the Ngrok server is hosted on the internet.\nSecure Tunneling: When you start Ngrok, it establishes a secure tunnel between your local machine and Ngrok&#039;s servers. This tunnel encrypts all traffic, ensuring secure communication.\nUnique URLs: Ngrok provides a unique URL for each tunnel created. These URLs are publicly accessible on the internet, allowing external users or services to connect to your local server through Ngrok.\nTraffic Forwarding: Any traffic received at the Ngrok server endpoint is forwarded through the secure tunnel to your local machine. Similarly, responses from your local server are sent back through the tunnel to the client.\n\n\n"},"notes/2024/Redis-with-node-js":{"title":"Redis with node js","links":[],"tags":["redis","nodejs"],"content":"How Redis Works Behind the Scenes\nRedis (Remote Dictionary Server) is an in-memory data structure store that operates primarily in memory but can persist data on disk. It is designed to provide high throughput and low-latency responses, making it ideal for use cases where fast access to data is crucial.\nHere‚Äôs how Redis works behind the scenes:\n\nIn-memory Storage: Redis keeps data in memory (RAM) rather than on disk, which allows for fast read and write operations. Although it can persist data to disk, its main advantage is its speed when working with in-memory data.\nData Structures: Redis supports various data structures like strings, lists, sets, sorted sets, hashes, bitmaps, hyperloglogs, and more. These structures make Redis versatile and applicable to many different use cases.\nPersistence: Redis provides two persistence models:\n\nRDB (Redis Database): Creates snapshots of the dataset at specified intervals.\nAOF (Append-Only File): Logs every write operation to a file in real-time, allowing for more durable persistence.\n\n\nReplication: Redis supports master-slave replication, which allows data from one Redis instance (master) to be replicated to multiple slaves. This is useful for creating redundancy and improving read performance by scaling out read operations to slave nodes.\nEviction Policies: Redis supports multiple eviction policies for managing memory usage, such as LRU (Least Recently Used), LFU (Least Frequently Used), and more, which help control how Redis handles data when the memory limit is reached.\nPub/Sub and Streams: Redis supports publish-subscribe messaging and stream-based data processing, which allows for building real-time applications like chat systems and event streaming.\n\nAdvantages of Redis\n\nHigh Speed: As Redis is an in-memory database, read and write operations are incredibly fast compared to traditional disk-based databases.\nRich Data Structures: Redis supports advanced data structures like lists, sets, sorted sets, and hashes, which allow developers to handle a wide range of use cases efficiently.\nPersistence Options: Redis offers the flexibility to choose between keeping data purely in memory (without persistence), using snapshots (RDB), or logging every change (AOF) for durability.\nScalability: With replication and clustering, Redis can scale horizontally to handle large datasets and high throughput by distributing the workload across multiple nodes.\nAtomic Operations: Redis supports atomic operations, ensuring that commands affecting data are executed entirely or not at all. This is essential for maintaining consistency in applications.\nCaching: Redis is widely used as a caching layer in applications to reduce database load and improve response times.\n\nWhy We Need Redis\nRedis is useful in many situations where speed is critical, such as:\n\nCaching: To store frequently accessed data and reduce the load on a relational database or another backend system.\nReal-time Analytics: For applications like gaming leaderboards or real-time analytics, where the data needs to be updated and queried frequently.\nSession Management: To store user sessions, authentication tokens, or temporary data.\nMessage Queues: Redis‚Äô Pub/Sub or lists can be used to implement message queues for distributing tasks among workers in a distributed system.\nRate Limiting: Redis is often used to implement rate-limiting algorithms due to its high-speed increment/decrement operations.\n\nExample with Node.js\nHere‚Äôs a simple example of using Redis in a Node.js application to cache the result of a database query.\nconst express = require(&#039;express&#039;);\nconst redis = require(&#039;redis&#039;);\nconst fetch = require(&#039;node-fetch&#039;);\n \nconst app = express();\nconst PORT = 3000;\n \n// Create a Redis client\nconst redisClient = redis.createClient({\n    host: &#039;localhost&#039;,\n    port: 6379\n});\n \n// Middleware to check cache\nconst checkCache = (req, res, next) =&gt; {\n    const { username } = req.params;\n \n    // Check if data is in Redis cache\n    redisClient.get(username, (err, data) =&gt; {\n        if (err) throw err;\n \n        if (data !== null) {\n            res.send(`Cache hit: ${data}`);\n        } else {\n            next();\n        }\n    });\n};\n \n// Simulate fetching data from an external API\nconst fetchUserData = async (username) =&gt; {\n    const response = await fetch(`api.github.com/users/${username}`);\n    const data = await response.json();\n    return data;\n};\n \n// Route to get user data\napp.get(&#039;/user/:username&#039;, checkCache, async (req, res) =&gt; {\n    const { username } = req.params;\n \n    try {\n        const data = await fetchUserData(username);\n \n        // Save the fetched data in Redis cache for 1 hour (3600 seconds)\n        redisClient.setex(username, 3600, JSON.stringify(data));\n \n        res.send(`Fetched from API: ${JSON.stringify(data)}`);\n    } catch (err) {\n        console.error(err);\n        res.status(500).send(&#039;Server Error&#039;);\n    }\n});\n \napp.listen(PORT, () =&gt; {\n    console.log(`Server running on port ${PORT}`);\n});\n \nHow It Works:\n\nRedis client: A Redis client is created and connected to the local Redis server.\nCache Check Middleware: When a request is made to /user/:username, the middleware checkCache first checks if the requested data (username) exists in Redis. If it does, it serves the cached data.\nFetch Data: If no cached data is found, it fetches the user data from GitHub‚Äôs API, caches it in Redis with an expiration time of 1 hour, and sends the data back to the client.\n\nThis demonstrates how Redis can be used as a caching layer to speed up subsequent requests and reduce the number of API calls."},"notes/2024/Session-Storage":{"title":"Session Storage","links":[],"tags":["nodejs","passport","sessions"],"content":"Session Storage\nIn Passport.js, the concept of a ‚Äústore‚Äù refers to a mechanism for persisting session data across requests. When using Passport‚Äôs session support, you have the option to specify a store where session data will be stored and retrieved from. This is typically useful in scenarios where you want session data to be stored outside of the server‚Äôs memory, such as in a database or a caching system.\nLet‚Äôs explore the concept of the store in the context of Passport‚Äôs session support:\n\nSession Data Storage:\n\nWhen a user is authenticated, Passport serializes the user‚Äôs information into the session. This session data needs to be stored somewhere so that it can be retrieved on subsequent requests to maintain the user‚Äôs authenticated state.\n\n\nStore Configuration:\n\nPassport provides the flexibility to choose where to store session data. You can specify a store when configuring Passport‚Äôs session middleware.\n\n\nStore Types:\n\nPassport supports various types of stores, including in-memory stores, database-backed stores, and external stores like Redis. Each store type has its own advantages and considerations in terms of performance, scalability, and persistence.\n\n\nPersistence and Scalability:\n\nUsing an external store, such as a database or Redis, allows session data to persist across server restarts and scale across multiple server instances. This ensures that session data remains available and consistent even in distributed environments.\n\n\nCustom Stores:\n\nPassport also allows you to implement custom stores if you have specific requirements or want to integrate with a particular data storage solution.\n\n\n\nHere‚Äôs a basic example of how you might configure Passport‚Äôs session middleware with a Redis store:\njavascriptCopy code\nconst session = require(&#039;express-session&#039;);\nconst RedisStore = require(&#039;connect-redis&#039;)(session);\n \n// Configure session middleware with Redis store\napp.use(session({\n  secret: &#039;your_secret_here&#039;,\n  store: new RedisStore({ url: &#039;redis://localhost:6379&#039; }),\n  resave: false,\n  saveUninitialized: true\n}));\n \nIn this example:\n\nThe store option specifies the Redis store for storing session data.\nThe RedisStore constructor creates an instance of the Redis store, which is then passed to the session middleware.\nSession data will be stored and retrieved from the Redis database specified by the URL (redis://localhost:6379).\n\nIn summary, the store in Passport‚Äôs session support allows you to choose where session data is stored, providing flexibility, persistence, and scalability for managing user sessions in web applications."},"notes/2024/Time-and-space-complexity":{"title":"Time and space complexity","links":[],"tags":["dsa","problem_solving"],"content":"Understanding Time and Space Complexity\nIn software engineering, every algorithm possesses a certain level of complexity, which is crucial in determining its efficiency. Complexity is typically measured in two ways:\n\nTime Complexity:¬†The amount of time an algorithm takes to complete as a function of the input size. It provides a way to quantify the efficiency of an algorithm by expressing how the execution time grows as the input size increases.\nSpace Complexity:¬†The amount of memory an algorithm uses as a function of the input size. It accounts for the total amount of space required by the algorithm, including the space used by variables, data structures, and function call stacks.\n\nUnderstanding both time and space complexity is essential for writing optimized code, as it helps in choosing the most efficient algorithm for a given problem. Efficient algorithms not only reduce the time taken to execute but also minimize the resources required, which is especially important in large-scale applications and systems with limited resources.\nTime Complexity\nWhen code is executed, it undergoes several stages, from being written in an Integrated Development Environment (IDE) to running on a server. During this process, the code takes a certain amount of time to execute. However, the actual time taken to run the code cannot be solely relied upon to determine time complexity due to factors like server specifications and environmental conditions.\nAs the size of the input increases, the time taken by the program to execute also increases. This relationship between input size and execution time is fundamental in understanding time complexity.\n\nRate of Change and Time Complexity\nThe rate of change in the execution time as the input size increases can be expressed as:\n(input2¬†- input1) / (time2¬†- time1)\nThis rate of change is what defines the time complexity of an algorithm. Time complexity is often measured using Big O notation, which provides an upper bound on the time taken as a function of the input size.\nExample: Measuring Time Complexity with Big O\nConsider a simple example involving a single for loop:\nfor (int i = 0; i &lt; 5; i++) {\n    // some constant time operation\n}\n\nIn this case, the time complexity is¬†O(5), because the loop runs five times regardless of the input size. However, when the loop runs¬†N¬†times (where¬†N¬†is the input size), the time complexity is¬†O(N).\nThe thought process here involves understanding how the number of operations scales with the input size. If the number of operations is directly proportional to the input size, then the time complexity is linear, denoted as¬†O(N).\nRules for Solving Time Complexity\n\nAvoid Constants:¬†When calculating time complexity, constant factors are ignored. For example, in the loop mentioned above,¬†O(5)¬†is simplified to¬†O(1)¬†since the constant does not change with the input size. The focus is on how the algorithm scales, not on fixed numbers.\nConsider the Worst Case:¬†Time complexity analysis typically considers the worst-case scenario, as this provides a guarantee on the maximum time an algorithm will take. For instance, in searching algorithms, the worst case occurs when the desired element is at the last position or not present at all.\n\nOther Ways to Measure Time Complexity\nBeyond Big O, time complexity can also be measured using¬†Theta (Œò)¬†and¬†Omega (Œ©):\n\nExample: Analyzing Time Complexity with Big O\nConsider the following examples to understand how to measure time complexity using Big O notation:\nExample 1: Simple For Loop\nfor (int i = 0; i &lt; N; i++) {\n    // some constant time operation\n}\n\nThis loop runs¬†N¬†times, and each iteration performs a constant-time operation (e.g., addition, subtraction). The time complexity for this code is¬†O(N)¬†because the execution time scales linearly with the input size¬†N.\nExample 2: Nested For Loop\nfor (int i = 0; i &lt; N; i++) {\n    for (int j = 0; j &lt; N; j++) {\n        // some constant time operation\n    }\n}\n\nHere, the outer loop runs¬†N¬†times, and for each iteration of the outer loop, the inner loop also runs¬†N¬†times. This results in a total of¬†N * N = N2¬†operations. Therefore, the time complexity is¬†O(N2), indicating a quadratic relationship between the input size and execution time.\nExample 3: Loop with Logarithmic Time Complexity\nfor (int i = 1; i &lt; N; i = i * 2) {\n    // some constant time operation\n}\n\nIn this example, the loop variable¬†i¬†is doubled each time, leading to a logarithmic growth in the number of iterations. The loop runs approximately¬†log2(N)¬†times, making the time complexity¬†O(log N). This type of complexity is common in algorithms like binary search.\nExample 4: Constant Time Operation\nint sum = a + b;\n\nThis operation takes a fixed amount of time, regardless of the input size. The time complexity is¬†O(1), indicating constant time.\nExample 5: Loop with Early Exit (Worst-Case Analysis)\nfor (int i = 0; i &lt; N; i++) {\n    if (arr[i] == target) {\n        break;\n    }\n}\n\nIn this loop, the best-case scenario occurs when the target is found on the first iteration, giving a time complexity of¬†O(1). However, in the worst case, the target is either at the end or not present, requiring the loop to run¬†N¬†times, making the time complexity¬†O(N). When analyzing algorithms, the worst-case scenario is typically used to ensure reliable performance.\nSpace Complexity\nSpace complexity measures the amount of memory an algorithm requires as a function of the input size. It is a critical aspect of algorithm efficiency, especially in memory-constrained environments. Space complexity can be broken down into two main components:\n\nAuxiliary Space:¬†This refers to the extra space or temporary space used by the algorithm, apart from the input data. It includes variables, data structures, and function call stacks that the algorithm utilizes during execution.\nInput Space:¬†This is the memory occupied by the input data itself. In many cases, the input space is considered negligible when analyzing space complexity, as it is independent of the algorithm‚Äôs operations.\n\nExamples of Space Complexity\n\nArray of Size¬†N:¬†Consider an array with¬†N¬†elements. The space complexity in this case is¬†O(N), as the memory required scales linearly with the number of elements in the array.\nRecursive Function:¬†For a recursive algorithm, the space complexity is determined by the call stack, which grows with the depth of the recursion. For a recursive function that calls itself¬†N¬†times, the space complexity is¬†O(N).\n\nConclusion\nUnderstanding time and space complexity is fundamental to evaluating and optimizing algorithms. Time complexity provides insight into how the execution time of an algorithm scales with input size, while space complexity measures the memory usage. By analyzing these complexities, one can choose the most efficient algorithm for a given problem, ensuring that both time and resources are used effectively.\nNote¬†: For a deeper understanding, users are encouraged to test their knowledge by analyzing different algorithms and comparing their complexities. Video editorial attached above can provide additional insights and help validate the analyses."},"notes/2024/git":{"title":"git","links":[],"tags":["git","version_control"],"content":"git commands\n\namend : git commit ‚Äîamend   ‚Üí it will make changes with the last commit message without creating another commit\nforce pull :  git pull -f origin \naborting rebase :  git rebase ‚Äîabort\nstash apply command  :  git stash apply ‚Üí to apply the latest stash\nstash pop command :  git stash pop\nstash command : git stash save ‚Äúmessage ‚Äù\nunset upstream :  git branch ‚Äîunset-upstream\nrebase command : git pull ‚Äîrebase\ngit fetch with git merge and git pull\nref : www.linkedin.com/pulse/git-pull-fetch-understanding-differences-your-devops-guide/\ngit log ‚Üí gives the commit history\nHead ‚Üí In Git, HEAD refers to¬†the currently checked-out branch‚Äôs latest commit.\n\ngit error resolve\n\nFrom github.com:klenty/klenty-client\n\n\nbranch                  master     ‚Üí FETCH_HEAD\nfatal: Not possible to fast-forward, aborting\nsolution :\nHere‚Äôs what could be happening:\n\nLocal and Remote Branches Have Diverged: If you‚Äôve made commits to your local branch that aren‚Äôt in the remote master branch, Git can‚Äôt perform a fast-forward merge because it would potentially overwrite your local changes.\nChanges in Remote master Since Last Pull: Someone else might have pushed changes to the master branch since your last pull. This creates a divergence between your local master and the remote master.\nTo resolve this, you have a few options:\n\n1. **Merge Changes**: Instead of using **`git pull`**, you could use **`git fetch`** to fetch the changes from the remote branch without merging them into your local branch immediately. Then, you can inspect the changes using **`git log origin/master`** and decide how to integrate them into your local branch, using **`git merge`** or **`git rebase`**.\n2. **Rebase Your Changes**: If you prefer a cleaner history, you could rebase your local changes on top of the remote **`master`**. This can be done with **`git pull --rebase`** or by manually rebasing with **`git rebase origin/master`**.\n3. **Force Update (Caution)**: If you&#039;re sure you want to discard your local changes and simply take the remote **`master`** as-is, you could force the update with **`git reset --hard origin/master`**. However, this will overwrite any local changes you&#039;ve made.\n\nRebasing\nRebasing in Git is a way to integrate changes from one branch into another by moving or ‚Äúrebasing‚Äù the commits. This is different from a merge, which creates a new commit for the merge itself. Here‚Äôs a step-by-step explanation of what rebasing does:\nScenario\nImagine you have the following situation:\n\nYour local branch (feature) has diverged from the remote branch (master).\nThe master branch has had new commits added to it, and you want to incorporate those changes into your feature branch.\n\nCurrent State\nscssCopy code\nA---B---C  master (remote)\n     \\\n      D---E---F  feature (local)\n \nRebase Process\n\n\nMove to the feature branch:\nbashCopy code\ngit checkout feature\n \n\n\nStart the rebase onto master:\nbashCopy code\ngit rebase master\n \nWhat happens now is:\n\nGit will temporarily remove your feature branch commits (D, E, F).\nIt will then move the feature branch to the tip of the master branch (C).\nFinally, it will reapply your commits (D, E, F) on top of master.\n\n\n\nIntermediate State\ncssCopy code\n         D&#039;---E&#039;---F&#039;  feature (rebased, not yet completed)\n        /\nA---B---C  master\n \n\nCommits D, E, F are replayed as D&#039;, E&#039;, F&#039; on top of C.\n\nResolving Conflicts\nDuring the rebase process, if there are conflicts between your changes and the changes in master, Git will pause and ask you to resolve the conflicts. You resolve the conflicts and continue the rebase:\nbashCopy code\n# After resolving conflicts\ngit add &lt;resolved files&gt;\ngit rebase --continue\n \nIf you want to abort the rebase at any point, you can do:\nbashCopy code\ngit rebase --abort\n \nFinal State\nAfter resolving any conflicts and completing the rebase, you have the following:\ncssCopy code\nA---B---C---D&#039;---E&#039;---F&#039;  master, feature\n \nYour feature branch now contains all the commits from master followed by your changes, and it looks like your changes were made on top of the latest state of master.\nBenefits of Rebase\n\nClean History: Rebasing results in a linear, clean commit history, which is easier to follow and understand.\nAvoids Merge Commits: Unlike merging, rebasing does not create an additional merge commit, keeping the history cleaner.\n\nConsiderations\n\nLocal Only: Rebase your local changes before pushing them to a shared branch. Rebasing changes history, so doing it on shared branches can confuse others working on the same branch.\nConflicts: Rebasing might require more conflict resolution if there are significant changes between the branches.\n\nIn summary, rebasing helps to keep your commit history clean and linear by applying your changes on top of the latest changes from another branch. This can make it easier to understand the project history and integrate changes smoothly.\nIf you have a branch that was originally taken from master, and you want to update your branch with the latest changes from master before pushing your work, you can follow these steps:\nSteps to Update Your Branch with Latest Changes from Master\n\n\nFetch the latest changes from the remote repository:\nbashCopy code\ngit fetch origin\n \n\n\nSwitch to your branch (assuming it‚Äôs called feature-branch):\nbashCopy code\ngit checkout feature-branch\n \n\n\nRebase your branch onto the updated master:\nbashCopy code\ngit rebase origin/master\n \nDuring the rebase process, Git will attempt to apply your changes on top of the latest master branch. If there are conflicts, Git will pause and prompt you to resolve them. Resolve any conflicts and continue the rebase:\nbashCopy code\n# After resolving conflicts\ngit add &lt;resolved files&gt;\ngit rebase --continue\n \nIf you want to abort the rebase process at any point, you can use:\nbashCopy code\ngit rebase --abort\n \n\n\nPush your updated branch to the remote repository:\nbashCopy code\ngit push origin feature-branch\n \nIf you have previously pushed this branch and the rebase altered its history, you will need to force push the changes:\nbashCopy code\ngit push --force-with-lease origin feature-branch\n \nThe --force-with-lease option is safer than --force as it ensures you don‚Äôt overwrite any changes on the remote that you don‚Äôt know about.\n\n\nSummary of Commands\nbashCopy code\ngit fetch origin\ngit checkout feature-branch\ngit rebase origin/master\n# Resolve any conflicts, if necessary, then continue\ngit push origin feature-branch  # or `git push --force-with-lease origin feature-branch` if history was rewritten\n \nImportant Notes\n\nRebasing vs. Merging: Rebasing rewrites commit history, making it look like your changes were made on top of the latest master. Merging retains the original branch structure but adds a merge commit.\nConflict Resolution: Be prepared to resolve conflicts during the rebase process if there are any changes in master that conflict with your work.\nForce Push with Caution: Only use -force-with-lease or -force if you understand the implications, as it can overwrite changes on the remote repository. -force-with-lease is generally safer.\n\nBy following these steps, you ensure that your branch is up-to-date with the latest changes from master before you push your work, maintaining a clean and consistent project history.\nTo revert the last commit, the method depends on whether the commit has been pushed to a remote repository or not. Here are the options:\n\n1. If the Commit Has NOT Been Pushed\nUse the following commands:\nTo undo the last commit but keep the changes in your working directory:\n \ngit reset --soft HEAD~1\n\n-soft: Keeps the changes in the staging area (you can commit them again or modify them).\n\nTo undo the last commit and remove the changes entirely:\nbash\nCopyEdit\ngit reset --hard HEAD~1\n \n\n-hard: Completely discards the changes, and your working directory will match the previous commit.\n\n\n2. If the Commit HAS Been Pushed\nReverting a pushed commit requires extra care to avoid issues for others working on the same branch.\nOption A: Revert the Commit Safely\nThis creates a new commit that undoes the last commit:\nbash\nCopyEdit\ngit revert HEAD\n \n\nA revert commit is created, and it keeps the history intact.\nThis is the preferred option if the branch has been shared.\n\nOption B: Force Reset (Risky, Avoid If Others Are Using the Branch)\nIf you still want to completely remove the commit from history:\nbash\nCopyEdit\ngit reset --hard HEAD~1\ngit push origin --force\n \n\nWarning: This rewrites history and may cause problems for others if they have already pulled the branch.\n\n\nSummary:\n\nUse git reset for local commits that haven‚Äôt been pushed.\nUse git revert for pushed commits to keep history safe.\nAvoid git push --force unless absolutely necessary.\n\nLet me know if you need help with a specific scenario!"},"notes/2024/javascript-memory-allocation":{"title":"javascript memory allocation","links":[],"tags":["javascript","garbage_collection"],"content":"1. Memory Management in JavaScript\nJavaScript handles memory management automatically using Garbage Collection (GC). Here‚Äôs an overview:\na. Memory Allocation\n\nPrimitive values (e.g., numbers, strings, booleans) are stored on the stack.\nObjects and arrays are stored on the heap, as they are dynamic and can grow in size.\n\nb. Garbage Collection\nThe garbage collector reclaims memory that is no longer in use by the application. Modern JavaScript engines use algorithms like mark-and-sweep to identify and clean up unused memory:\n\nThe GC starts from a set of ‚Äúroots‚Äù (e.g., global variables, the call stack).\nIt marks all objects reachable from these roots as ‚Äúin use.‚Äù\nUnmarked objects are deemed unreachable and are cleaned up.\n\nIn JavaScript, the key difference between a WeakMap and a normal Map lies in how they manage the keys and their behavior in memory management. Here‚Äôs a detailed comparison:\n\n1. Keys\nMap:\n\nA Map can have any data type as keys, including primitives (e.g., string, number) and objects.\nKeys are strongly referenced, meaning the key-value pair will remain in memory as long as the key is reachable.\n\njavascript\nCopyEdit\nlet map = new Map();\nlet key = { id: 1 };\n \nmap.set(key, &quot;value&quot;);\nconsole.log(map.get(key)); // Output: &quot;value&quot;\n \n// Keys remain strongly referenced\nkey = null; // Even if the original reference is null, the map retains the key.\nconsole.log(map.size); // Output: 1\n \n\nWeakMap:\n\nA WeakMap only allows objects (not primitives) as keys.\nKeys are weakly referenced, meaning if there are no other references to the key object, it can be garbage collected.\n\nlet weakMap = new WeakMap();\nlet key = { id: 1 };\n \nweakMap.set(key, &quot;value&quot;);\nconsole.log(weakMap.get(key)); // Output: &quot;value&quot;\n \n// If the key is no longer referenced elsewhere, it will be garbage collected\nkey = null;\n// After garbage collection, the key-value pair is removed from the WeakMap\n \n\n2. Memory Management\nMap:\n\nKeys are strongly held, meaning they prevent the object from being garbage collected, even if no other references exist.\n\nWeakMap:\n\nKeys are weakly held, meaning they do not prevent the object from being garbage collected. Once the object is no longer reachable elsewhere in the code, it is eligible for garbage collection, and the associated key-value pair is automatically removed from the WeakMap.\n\n\n3. Iteration\nMap:\n\nA Map is iterable. You can loop through it using methods like map.keys(), map.values(), or map.entries().\n\n \nlet map = new Map();\nmap.set(&quot;a&quot;, 1);\nmap.set(&quot;b&quot;, 2);\n \nfor (let [key, value] of map.entries()) {\n  console.log(key, value);\n}\n// Output:\n// &quot;a&quot; 1\n// &quot;b&quot; 2\n \nWeakMap:\n\nA WeakMap is not iterable, and there are no methods to retrieve all its keys, values, or entries. This design ensures the privacy of the data and aligns with its memory management principles.\n\nlet weakMap = new WeakMap();\nlet key = {};\nweakMap.set(key, &quot;value&quot;);\n \n// You can&#039;t iterate over WeakMap\n// Example: No weakMap.keys(), weakMap.values(), or weakMap.entries()\n \n\n4. Use Cases\nMap:\n\nUse when you need to store key-value pairs where the keys can be primitive or objects.\nSuitable for general-purpose data storage and retrieval.\nWhen you need iteration over the key-value pairs.\n\nWeakMap:\n\nUse when you need to associate data with an object and want the object to be garbage collected when it is no longer in use elsewhere.\nCommonly used in frameworks and libraries for managing private data related to objects, such as DOM elements or objects in a class instance.\n\n\n5. Example Use Case for WeakMap\n// Example: Caching data related to DOM elements\nlet cache = new WeakMap();\n \nfunction cacheData(element, data) {\n  cache.set(element, data);\n}\n \nfunction getCachedData(element) {\n  return cache.get(element);\n}\n \nlet div = document.createElement(&quot;div&quot;);\ncacheData(div, { id: 123 });\n \nconsole.log(getCachedData(div)); // Output: { id: 123 }\n \ndiv = null; // Once the DOM element is removed, it&#039;s eligible for garbage collection.\n \n\nKey Differences Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureMapWeakMapKey TypesAny (primitives and objects)Objects onlyMemoryStrongly referenced keysWeakly referenced keysGarbage CollectionNot automatically clearedAutomatically cleared when key is unreachableIterationIterable (keys, values, entries)Not iterableUse CaseGeneral-purpose storageObject-private or temporary storage\nThis makes WeakMap useful for scenarios where temporary or private data is tied to the lifetime of objects.\nreference:\ndeveloper.mozilla.org/en-US/docs/Web/JavaScript/Memory_management"},"notes/2024/mobile-app-background-and-foreground-application":{"title":"mobile app background and foreground application","links":[],"tags":["mobile","android","ios"],"content":"mobile app background and foreground application\nIn mobile app development, the terms ‚Äúbackground‚Äù and ‚Äúforeground‚Äù refer to the state of an app in relation to its visibility and activity on the device‚Äôs screen.\nForeground\nWhen an app is in the foreground, it is currently active and visible to the user. This means the app‚Äôs interface is displayed on the screen, and the user can interact with it. In this state, the app has access to resources and can perform tasks such as updating the user interface, handling user input, and running processes that require immediate attention.\nCharacteristics of a foreground app:\n\nThe user is actively interacting with the app.\nThe app‚Äôs UI is visible on the screen.\nThe app has higher priority for system resources (CPU, memory, etc.).\nThe app can receive user inputs such as taps, swipes, and text entry.\n\nBackground\nWhen an app is in the background, it is not currently visible to the user, but it may still be running or performing tasks. Background apps are those that the user has switched away from, but they remain in memory for quick access or to continue performing tasks that don‚Äôt require user interaction.\nCharacteristics of a background app:\n\nThe app‚Äôs UI is not visible to the user.\nThe app has lower priority for system resources.\nThe app may still perform certain tasks, such as playing music, tracking location, or handling incoming messages.\nThe app can be suspended or killed by the system if resources are needed for other tasks.\n\nBackground Activities in Mobile Apps\nThere are specific activities that apps can perform while in the background:\n\nBackground Services:\n\nSome apps run background services to handle tasks such as fetching data from the internet, playing music, or processing information.\n\n\nPush Notifications:\n\nApps can receive and display push notifications while in the background. This helps inform users of important events or updates.\n\n\nBackground Fetch:\n\nSome apps can periodically fetch and update content in the background, ensuring the app is up-to-date when the user opens it.\n\n\nLocation Tracking:\n\nApps that require location updates (e.g., fitness trackers or navigation apps) can continue to track the device‚Äôs location in the background.\n\n\n\nManaging Background and Foreground States in Development\nIn iOS:\n\nApps use AppDelegate methods to handle transitions between foreground and background states. Key methods include:\n\napplicationDidBecomeActive: Called when the app becomes active (foreground).\napplicationDidEnterBackground: Called when the app enters the background.\napplicationWillEnterForeground: Called when the app is about to enter the foreground.\napplicationWillResignActive: Called when the app is about to go from active to inactive state.\n\n\n\nIn Android:\n\nApps use Activity lifecycle methods to manage foreground and background transitions. Key methods include:\n\nonResume: Called when the activity comes to the foreground.\nonPause: Called when the activity goes to the background.\nonStop: Called when the activity is no longer visible to the user.\nonStart: Called when the activity is becoming visible to the user.\n\n\n\nUnderstanding the difference between background and foreground states is crucial for developing responsive and resource-efficient mobile applications. Developers must ensure that their apps handle these transitions smoothly to provide a seamless user experience."},"notes/2024/rate-limit-in-web-application":{"title":"rate limit in webapplication","links":[],"tags":["rate_limiter","backend","redis"],"content":"What is Rate Limiting?\nRate limiting is a technique used to control the number of requests that a user or system can make to an API or service within a specific time frame. It helps protect servers from being overwhelmed by too many requests, preventing misuse or abuse, ensuring fair resource allocation, and maintaining the quality of service.\nRate limiting is commonly used in APIs, websites, and services to:\n\nPrevent Denial of Service (DoS) attacks.\nEnforce fair usage limits for users.\nProtect system resources from being exhausted.\nEnsure that API usage adheres to quota restrictions.\n\nFor example, an API might allow a client to make up to 100 requests per minute. Once the limit is reached, the server can reject further requests for the rest of the time window with a response indicating that the limit has been exceeded.\nRedis as a Backing Store for Rate Limiting\nRedis is an ideal backing store for rate limiting because of its:\n\nIn-memory data storage: Redis is incredibly fast, making it well-suited for real-time request counting.\nExpiry features: Redis supports time-based expiration on keys, which is essential for implementing rate limits that reset over time (e.g., per minute, per hour).\nAtomic operations: Redis offers commands like INCR, SETEX, and others that ensure atomicity, which helps maintain consistency in tracking request counts.\n\nrate-limit-redis Library\nrate-limit-redis is a Node.js package that works with Express and Redis to enforce rate limits. It is often used in combination with the express-rate-limit middleware to store the rate limit data (e.g., request counts, timestamps) in Redis.\nKey Features of rate-limit-redis:\n\nRedis-backed store: Keeps track of request counts and expiry in Redis, allowing for fast and persistent rate limiting across distributed systems.\nShared rate limiting: Since Redis is centralized and distributed, multiple servers (e.g., microservices) can share the same rate-limiting rules.\nScalability: You can rate limit users across different nodes or processes using the same Redis instance.\n\nExample Usage of rate-limit-redis with Express\nHere‚Äôs an example that shows how to use rate-limit-redis in a Node.js application with Express to limit the number of requests a user can make.\nSteps:\n\nInstall the required packages:\n\nnpm install express redis express-rate-limit rate-limit-redis\n\nSet up Express with Redis-backed rate limiting.\n\n \nconst express = require(&#039;express&#039;);\nconst rateLimit = require(&#039;express-rate-limit&#039;);\nconst RedisStore = require(&#039;rate-limit-redis&#039;);\nconst redis = require(&#039;redis&#039;);\n \n// Create a Redis client\nconst redisClient = redis.createClient({\n    host: &#039;localhost&#039;, // Redis server hostname\n    port: 6379         // Redis server port\n});\n \nredisClient.on(&#039;error&#039;, (err) =&gt; {\n    console.error(&#039;Error connecting to Redis:&#039;, err);\n});\n \n// Initialize Express\nconst app = express();\n \n// Define rate limiter middleware with Redis\nconst limiter = rateLimit({\n    store: new RedisStore({\n        sendCommand: (...args) =&gt; redisClient.sendCommand(args),\n    }),\n    windowMs: 1 * 60 * 1000,  // 1 minute window\n    max: 5,                   // Limit each IP to 5 requests per windowMs\n    message: &#039;Too many requests, please try again later.&#039;,\n});\n \n// Apply the rate limiter to all requests\napp.use(limiter);\n \n// Example route\napp.get(&#039;/&#039;, (req, res) =&gt; {\n    res.send(&#039;Welcome! This is a rate-limited route.&#039;);\n});\n \n// Start the Express server\napp.listen(3000, () =&gt; {\n    console.log(&#039;Server running on http://localhost:3000&#039;);\n});\n \nExplanation:\n\nRedis Client: We create a Redis client using redis.createClient() and configure it to connect to a local Redis instance. The client will be used by the rate limiter to store and retrieve request count data.\nRate Limiter Configuration: We set up the rate limiter using express-rate-limit and rate-limit-redis. The rate limiter is configured to:\n\nStore data in Redis using the RedisStore.\nLimit requests to 5 requests per minute for each IP address.\nSend a custom message (&#039;Too many requests, please try again later.&#039;) when the limit is exceeded.\n\n\nApply Rate Limiter: The rate limiter middleware is applied globally to all routes using app.use(limiter). This means every request will go through this rate limiter.\nTesting: When you run this code, a user can make up to 5 requests within a minute. If they exceed this limit, they will receive the response Too many requests, please try again later.. After a minute, the rate limit will reset, and the user can make more requests.\n\nExample Request Flow:\n\nA user sends the first request, and Redis stores the count as 1.\nIf the user sends a second request within the same minute, Redis increments the count.\nOnce the user hits the limit (5 requests), further requests will return the ‚ÄúToo many requests‚Äù error for the rest of the 1-minute window.\n\nRedis Key Structure:\nRedis stores the request counts in keys, typically using the client‚Äôs IP address or user ID to create a unique key. For example, a key for user requests might look like rate_limit:127.0.0.1, where 127.0.0.1 is the user‚Äôs IP address. Redis will expire the key after the time window (in this case, 1 minute).\nAdvantages of Using rate-limit-redis:\n\nScalability: Rate limits can be shared across multiple server instances using a single Redis store. This ensures consistency when you scale your application horizontally (i.e., across multiple servers).\nPerformance: Redis is incredibly fast and can handle high throughput, making it suitable for storing real-time rate-limiting data.\nPersistence: Redis can persist data to disk, so even if your application restarts, the rate limit data remains consistent (if Redis is set to persist data).\nDistributed Rate Limiting: By using Redis, you can implement rate limiting across a distributed system with multiple nodes sharing the same limit, preventing abuse across servers.\n\nUse Cases for Rate Limiting:\n\nAPI Protection: Protect your API from being flooded with too many requests from a single client (e.g., to avoid DoS attacks or abusive behavior).\nFair Usage Enforcement: Limit the usage of free API tiers or services, ensuring that all users get a fair share of resources.\nCost Control: Avoid high operational costs due to excessive API usage by rate limiting expensive operations.\nPrevent Brute-Force Attacks: Protect endpoints like login pages by limiting the number of login attempts per IP.\n\nConclusion:\nRedis-backed rate limiting is a powerful way to control API usage and protect your server from excessive requests. With libraries like rate-limit-redis in Node.js, you can easily integrate Redis for real-time request counting and enforce rate limits that are scalable and persistent.\nwhat is rate limit is not implemented ?\nIf rate limiting is not implemented in an application or API, several issues can arise, especially when the system is handling a high volume of requests. Here are some potential consequences:\n1. Server Overload and Downtime\n\nWithout rate limiting, users or malicious bots can send an excessive number of requests, causing the server to become overloaded.\nThis overload can lead to crashes, downtime, or sluggish response times, affecting all users‚Äô experiences and potentially causing complete service interruptions.\n\n2. Increased Operational Costs\n\nHandling an excessive volume of requests can drive up bandwidth, computing power, and database costs, especially if the infrastructure scales automatically (e.g., cloud services with auto-scaling).\nThis can lead to significant, unnecessary operational expenses, as resources are used to process requests that might not be essential or legitimate.\n\n3. Security Vulnerabilities\n\nWithout rate limiting, systems are more susceptible to DDoS (Distributed Denial of Service) attacks, where attackers flood the system with requests to make it unavailable to legitimate users.\nAttackers may also exploit a lack of rate limiting to brute-force passwords or test for vulnerabilities in APIs by sending repeated requests without restriction.\n\n4. Poor User Experience\n\nA lack of rate limiting can result in slow response times for all users, as the system is constantly processing excessive requests.\nIf some users abuse the system by making many requests, it can degrade the experience for other users who are unable to access the service promptly.\n\n5. Data Integrity and Consistency Issues\n\nHigh request volumes without restrictions can put excessive load on databases, leading to read/write consistency issues.\nFor example, if there is an endpoint to update account data and no rate limit, multiple updates in quick succession could cause data conflicts or lead to unexpected states in the data.\n\n6. API Misuse and Abuse\n\nWithout rate limits, third-party developers or end-users can inadvertently or maliciously consume excessive resources, leading to abuse of your API.\nThis can lead to ‚Äúnoisy neighbors‚Äù in shared environments, where a few heavy users degrade the experience for others due to unrestricted access.\n\n7. Difficulties in Scaling\n\nRate limiting helps control and predict load, which is essential for scaling infrastructure effectively.\nWithout it, sudden, unpredictable surges in traffic are more difficult to handle, making it harder to scale the system efficiently and maintain performance standards.\n\nExample Scenario\nImagine a public API without rate limiting. A single user could write a script to request data thousands of times per second, overwhelming the system. This can result in legitimate users being unable to access the API or experience severe delays, potentially leading to user dissatisfaction, lost revenue, and damage to the application‚Äôs reputation.\nImplementing Rate Limiting\nRate limiting, when implemented, helps prevent all of these issues by controlling the number of requests each user or IP can make within a certain time period. This ensures fair usage, protects server resources, and improves both security and reliability."},"notes/2025/Design-Patterns":{"title":"Design Patterns","links":[],"tags":["DesignPattern"],"content":"The rest of your content lives here. You can use Markdown here :)\nDefinition:\n\nThere are proven solution for proven recurring desgin problems , they are not code , rather than just good practices for reusable abstraction\n\nTypes of design patterns (23 Patterns)\n\nCreational Pattern (5)\nStructural Pattern (7)\nBehavioural Pattern (11)\n\nCreational Pattern\nThey deal with Object Creation Mechanisim\n\nSingleton Pattern\nFactory Pattern\nAbstract pattern\nBuilder Pattern\nPrototype Pattern\n\nStructural Design Patterns\nStructural design patterns deal with how classes and objects are composed to form larger, more complex structures while keeping them flexible and efficient.\nBehavioural Pattern\nWhere this pattern works with behaviour between the objects\nSingletone Pattern\nIt is used manage the single instance all over the application, example mongoose instance , where the mongo connection is presist for all the mongose import\nDoes it is thread safety?\nIf you create the instance during , then it will  be thread safety, because your creating the instance during the class loading\npackage DesignPattern.CreationalPattern;\n \n  \n \n// Lazy Loading - because the object creation happend after the class loading. It is not thread safe\n \n// class Mongoose {\n \n// private static Mongoose instance;\n \n// int counter = 0;\n \n  \n \n// private Mongoose(){\n \n// }\n \n  \n \n// public static Mongoose getMongooseInstance(){\n \n// if(instance == null)\n \n// instance = new Mongoose();\n \n// return instance;\n \n// }\n \n  \n \n// public int getCount(){\n \n// return this.counter;\n \n// }\n \n  \n \n// public void incrementCounter(){\n \n// this.counter++;\n \n// }\n \n// }\n \n  \n  \n \n//Eager Loading It is thread safe because the Object creation happens during the class loading;\n \nclass Mongoose {\n \nprivate static Mongoose instance = new Mongoose();\n \nint counter = 0;\n \n  \n \nprivate Mongoose(){\n \n}\n \n  \n \npublic static Mongoose getMongooseInstance(){\n \nreturn instance;\n \n}\n \n  \n \npublic int getCount(){\n \nreturn this.counter;\n \n}\n \n  \n \npublic void incrementCounter(){\n \nthis.counter++;\n \n}\n \n}\n \n  \n \npublic class SingleTon {\n \npublic static void main(String[] args) {\n \n\tMongoose mongoose = Mongoose.getMongooseInstance();\n\t\n\tmongoose.incrementCounter();\n\t\n\tmongoose.incrementCounter();\n\t\n\t  \n\t\n\tSystem.out.println(&quot;Counter for Mongoose Instance &quot;+ mongoose.counter);\n\t\n\t  \n\t\n\tMongoose mongoose2 = Mongoose.getMongooseInstance();\n\t\n\t  \n\t\n\tmongoose2.incrementCounter();\n\t\n\tmongoose2.incrementCounter();\n\t\n\t  \n\t\n\tSystem.out.println(&quot;Counter for Mongoose Instance2 &quot;+ mongoose2.counter);\n \n}\n \n}\nSuppose if you create the single instance during the execution then the thread is not safe, you want to handle it mannualy by these methods\n\nusing synchronize keyword where it ensures thread safety\ndouble check locking\n"},"notes/2025/How-to-Build-A-10X-API":{"title":"How to Build A 10X API","links":[],"tags":["backend","system_design"],"content":"The rest of your content lives here. You can use Markdown here :)\nUnderstanding API performance challenges\n\nInefficient database queries and access pattern\ncomplex computation blocking request processing\noversized payload that increases the network transfer time\nnetwork latency on global deployment\n\nScaling an API from 0 ‚Üí Millions of Users\nA staged blueprint for growing an API/product from zero ‚Üí millions of users without meltdown. Focuses on practical steps with TypeScript-friendly examples.\n\nPhase 0 ‚Üí 1k users: Ship fast, keep it simple\nGoals: prove value, collect real traffic, keep cost tiny.\n\nArch: Monolith (Fastify/Express + TypeScript), 1 DB, 1 cache.\nDB: Postgres (or Mongo if document-heavy). Add indexes early.\nCache: Redis for hot reads, rate limits, idempotency.\nInfra: Single region, autoscaling containers, CDN for static.\nOps: Health checks, p95 latency, error rate, basic logs.\nData model: Multi-tenancy (tenant_id).\nDevX: OpenAPI, seed data, feature flags, safe migrations.\nSecurity: OAuth/OIDC/JWT, Zod validation, least-privileged DB roles.\n\n‚úÖ Exit criteria:\n\np95 read &lt;200ms, write &lt;500ms\n&lt;0.1% 5xx\n99.9% uptime over a month\n\n\nPhase 1k ‚Üí 100k users: Remove obvious bottlenecks\nGoals: scale reads cheaply, make writes resilient.\n\n\nRead scaling:\n\nHTTP caching (ETag/Cache-Control)\nRedis entity + request cache (with request coalescing)\nCursor pagination, sparse fields, Brotli/Gzip compression\n\n\n\nWrite path:\n\nAsync offload via queues (BullMQ/Redis)\nWebhooks or SSE instead of polling\n\n\n\nDB:\n\nAdd read replicas\nLong reports ‚Üí replica or materialized views\n\n\n\nResilience:\n\nTimeouts, retries with jitter, circuit breakers\n\n\n\nObservability:\n\nOpenTelemetry traces\nRED metrics per endpoint\nStructured logs with traceId\n\n\n\nCost controls:\n\nEndpoint cost dashboards\nCache hit rate targets\nKill switches for expensive ops\n\n\n\n\nPhase 100k ‚Üí 1M users: Split concerns, isolate hotspots\nGoals: keep latency stable as traffic spikes.\n\n\nService slicing:\n\nExtract only high-traffic ‚Äúhot‚Äù services (e.g., Search)\nAPI Gateway for auth, rate limits, request caps\n\n\n\nData patterns:\n\nCQRS-lite: write DB + read projections\nOutbox pattern for reliable events\n\n\n\nStorage:\n\nBlobs ‚Üí S3/GCS with signed URLs\nPrecompute aggregates for dashboards\n\n\n\nGlobal users:\n\nCDN with edge caching\nMulti-region reads; single write region\n\n\n\nTraffic mgmt:\n\nRate limits, quotas, backpressure (429 + Retry-After)\nIdempotency-Key for POST creates\n\n\n\nTesting &amp; releases:\n\nLoad tests\nChaos experiments\nCanary deploys + auto-rollback\n\n\n\n\nPhase 1M ‚Üí 10M+ users: Go global, scale writes, plan shards\nGoals: eliminate single-region limits, scale state.\n\n\nMulti-region strategy:\n\nActive/active reads, single-writer for writes\nGeo DNS + latency routing\n\n\n\nDB sharding:\n\nStable shard key (tenant_id)\nMinimize cross-shard queries\nQuery router + rebalancing\n\n\n\nEvent backbone:\n\nKafka/PubSub for high throughput\nStream processors for projections\n\n\n\nSearch &amp; analytics:\n\nOpenSearch, ClickHouse, or BigQuery\n\n\n\nSchema evolution:\n\nContract tests\nDual-write + verify during migrations\n\n\n\nOrg scaling:\n\nSLOs per service\nError budgets\nCapacity planning\n\n\n\n\nReference Stack (TS-Centric)\n\nAPI: Fastify + Zod + OpenAPI + Pino\nAuth: OAuth2/OIDC (Auth0/Cognito/Keycloak)\nCache/Queue: Redis ‚Üí Kafka\nDB: Postgres + replicas; Meilisearch/Elastic; ClickHouse for analytics\nInfra: Kubernetes/ECS + HPA; Terraform; Canary/Blue-Green deploys\nEdge: Cloudflare/Akamai (CDN, WAF, bot protection)\nObservability: OpenTelemetry ‚Üí Jaeger/Tempo, Prometheus, Loki\n\n\nGuardrails\n\nRequest body max: 1‚Äì2 MB\nPagination: default=50, max=200\nTimeouts: client 3‚Äì5s, DB 800‚Äì1500ms\nRetries: idempotent only, 0‚Äì2 with jitter\nLatency targets: GET &lt;200ms, POST &lt;500ms\nCache hit rate: ‚â•80%\nError budget: ‚â§0.1% 5xx/month\nJobs: bounded concurrency + DLQs\n\n\nHigh-leverage Patterns\n\nRequest coalescing (collapse cache misses)\nOutbox + Transactional events (no missed webhooks)\nFeature flags &amp; dark launches\nBackfills via workers\nSLO-driven work\n\n\nMinimal Examples (TypeScript)\nIdempotency for POST\nif (req.method === &#039;POST&#039;) {\n  const key = req.headers[&#039;idempotency-key&#039;];\n  if (key &amp;&amp; await redis.exists(key)) {\n    return reply.send(await redis.get(key));\n  }\n  const result = await createThing();\n  await redis.setex(key, 86400, JSON.stringify(result));\n  return reply.code(201).send(result);\n}\n \nconst inFlight = new Map&lt;string, Promise&lt;any&gt;&gt;();\nasync function dedupe&lt;T&gt;(key: string, fn: () =&gt; Promise&lt;T&gt;): Promise&lt;T&gt; {\n  if (inFlight.has(key)) return inFlight.get(key)!;\n  const p = fn().finally(() =&gt; inFlight.delete(key));\n  inFlight.set(key, p);\n  return p;\n}\nThings to consider for scalling the application\n\nServer and Database separation\nLoad balancer\ndatabase replication and sharding\ncache\nCDN\n"},"notes/2025/Java-OOP's-Core-Concepts":{"title":"Java OOP's Core Concepts","links":[],"tags":[],"content":"Static Keyword\nStatic keyword is used for memory management , where the memory is created during the class loading . we can make nested classes, variables and methods as static. Those static variable will aslo called as class variables , class methods.\nHow will initialize the  static variable ?\nIt can be initialized with static block"},"notes/2025/LLD-(Low-Level-Design)":{"title":"LLD (Low Level Design)","links":[],"tags":["LLD","DesignPattern"],"content":"The rest of your content lives here. You can use Markdown here :)\nWhat is OOPS ?\nOOP‚Äôs stands for object oriented programming language , it is programming paradigm , where the code is written based on the objects , it contains methods and properties\nWhat is class and objects ?\nClass is the bluebrint , to create Object where it does not take any space in memory,\nObject is the instance of the class\n1. Encapsulation\nDefinition: Encapsulation is the process of bundling data (variables) and methods (functions) into a single unit (class) and restricting direct access to some components using access modifiers.\nüëâ Example: Declaring class variables as private and exposing them via getters and setters.\n\n2. Abstraction\nDefinition: Abstraction is the process of hiding implementation details and exposing only the essential features to the user.\nüëâ Example: When you call .sort() on a list, you don‚Äôt know the internal algorithm, you just know it sorts.\n\n3. Inheritance\nDefinition: Inheritance is a mechanism where one class (child/derived class) acquires the properties and behaviors of another class (parent/base class), promoting reusability.\nüëâ Example: class Dog extends Animal ‚Üí Dog inherits properties and methods from Animal.\n\n4. Polymorphism\nDefinition: Polymorphism means ‚Äúmany forms.‚Äù It allows the same method or operator to behave differently based on the object it is acting upon.\nüëâ Example:\n\n\nCompile-time polymorphism (Method Overloading): Same method name, different parameters.\n\n\nRuntime polymorphism (Method Overriding): Subclass provides a specific implementation of a method already defined in its parent class.\n\n\n\nüí° Super-short version (if interviewer asks quick):\n\n\nEncapsulation: Binding data + methods, restrict access.\n\n\nAbstraction: Show essential, hide details.\n\n\nInheritance: Acquire properties from parent.\n\n\nPolymorphism: One interface, many implementations.\n\n\nOOP Relationships\nIn Object-Oriented Programming (OOP), classes and objects often interact with each other. These interactions are represented by relationships.\n\n1. Association\nDefinition:\nA ‚Äúuses-a‚Äù relationship between two classes. Both objects can exist independently.\nExample (Java):\n \nclass Student {\n \nString name;\n \n}\n \n  \n \nclass Course {\n \nString title;\n \n}\n \n  \n \n// A student enrolls in a course ‚Üí Association\n \nclass Enrollment {\n \nStudent student;\n \nCourse course;\n \n}\n \n\n2. Aggregation\nDefinition:\nA ‚ÄúHas-a‚Äù relationship, but with weak ownership. Represented as a hollow diamond in UML.\nExample (Java):\n \nclass Department {\n \nString name;\n \n}\n \n  \n \nclass University {\n \nString name;\n \nList&lt;Department&gt; departments; // Aggregation\n \n}\n \n\n3. Composition\nDefinition:\nA ‚ÄúHas-a‚Äù relationship with strong ownership. Represented as a filled diamond in UML.\nExample (Java):\n \nclass Engine {\n \nString type;\n \n}\n \n  \n \nclass Car {\n \nEngine engine; // Composition\n \nCar() {\n \nengine = new Engine(); // Created inside Car\n \n}\n \n}\n \n\n4. Inheritance\nDefinition:\nAn ‚ÄúIs-a‚Äù relationship. One class inherits properties and behavior from another.\nExample (Java):\n \nclass Vehicle {\n \nvoid start() {\n \nSystem.out.println(&quot;Vehicle starts&quot;);\n \n}\n \n}\n \n  \n \nclass Car extends Vehicle { // Inheritance\n \nvoid drive() {\n \nSystem.out.println(&quot;Car is driving&quot;);\n \n}\n \n}\n \n\n5. Dependency\nDefinition:\nA ‚Äúuses‚Äù relationship where one class depends on another to function temporarily.\nExample (Java):\n \nclass Printer {\n \nvoid print(String message) {\n \nSystem.out.println(message);\n \n}\n \n}\n \n  \n \nclass Report {\n \nvoid generateReport(Printer printer) { // Dependency\n \nprinter.print(&quot;Report generated&quot;);\n \n}\n \n}\n \n\n‚úÖ Summary Table\n| Relationship | Meaning | Example | Independence |\n|---------------|----------------|---------------------|--------------|\n| Association | Uses-a | Student ‚Üî Course | Independent |\n| Aggregation | Has-a (weak) | University‚ÄìDept. | Independent |\n| Composition | Has-a (strong) | Car‚ÄìEngine | Dependent |\n| Inheritance | Is-a | Car‚ÄìVehicle | Independent but related |\n| Dependency | Uses temporarily | Report‚ÄìPrinter | Temporary |\n\nSOLID PRINCIPLE\nS - Single Responsiblity  Principle\nwhere the class should have the single responsibility and one reason to change the object\nclass Logger {\n     private String path;\n     File file;\n \n\t Logger(String path){\n\t   this.File = new File()\n\t }\n \n\t Log(message){\n\t }\n}\nhere the Logger will have the single responsiblity\nO - Open/Close Principle\nwhere the class will open for extension and closed for modification\n  interface Shape{\n       public void draw();\n  }\n \n  class Circle implements Shape{\n      public void draw(){\n         System.out.println(&quot;Drawing a circle.....&quot;);\n      }\n  }\nL - Liskova Substitution Prinicple\nwhere the subtype can be substituted to their base type without affecting the correctness of the program\n// class that doesn&#039;t follow this principle\n \nclass Bird {\n    public void fly(){\n       System.out.println(&quot;The Birds are flying! .... &quot;);\n    }\n}\n \n \nclass Sparrow extends Bird {\n    @override\n\tpublic void fly(){\n       System.out.println(&quot;Sparrow is flying! .... &quot;);\n    }\n}\n \nclass Penguine extends Bird {\n\t@override\n\tpublic void fly(){\n       throws Expection(&quot;Penguine does not fly&quot;);\n    }\n}\n \nclass Main {\n  public static void main(String arg[]){\n      Sparrow sp = new Sparrow();\n      sp.fly();\n      \n\t  Penguine pq = new Penguine();\n\t  pq.fly() // This is will throws an exception will throws the exceptions\n  }\n}\nFollowed Version\n \ninterface Flyable {\n  public void fly();\n}\n \nclass Bird {\n   public void eat(){\n       System.out.println(&quot;Birds will eat !!! &quot;);\n   }\n}\n \nclass Sparrow extends Bird implements Flyable {\n\tpublic void fly(){\n       System.out.println(&quot;Sparrow is flying! .... &quot;);\n    }\n}\n \nclass Penguine extends Bird {\n}\n \nclass Main {\n  public static void main(String arg[]){\n      Sparrow sp = new Sparrow();\n      sp.fly();\n      \n\t  Penguine pq = new Penguine();\n\t  pq.fly()\n  }\n}\nI - Interface Segregation Principle\nInterface should not force the subclass to implement the method which is not need\nD - Dependcy Inversion Principle\nDI prinicpal states that the object receive their dependcy from the external source rather than creating internally"},"notes/2025/MongoDB-OPLOG":{"title":"MongoDB OPLOG","links":[],"tags":["mongodb","database"],"content":"The rest of your content lives here. You can use Markdown here :)\nüìå What is Oplog?\n\n\nOplog = Operations Log.\n\n\nSpecial capped collection in MongoDB that stores a rolling record of all write operations (insert, update, delete).\n\n\nLocation: local.oplog.rs (for replica sets).\n\n\nPurpose: Used by replication ‚Üí secondary nodes read from oplog to apply the same changes made on the primary.\n\n\n\nüîπ Why is Oplog Important?\n\n\nReplication ‚Üí Keeps secondaries in sync with primary.\n\n\nFailover ‚Üí Secondary can take over if primary fails, with latest data.\n\n\nEvent sourcing / Change Streams ‚Üí Applications can subscribe to oplog (via Change Streams) for real-time reactions.\n\n\n\nüîπ Real World Example\nFood Delivery App (MongoDB backend):\n\n\nCustomer places an order ‚Üí insert operation recorded in oplog.\n\n\nPrimary writes the order, oplog entry is created:\n{ &quot;ts&quot;: ..., &quot;op&quot;: &quot;i&quot;, &quot;ns&quot;: &quot;orders&quot;, &quot;o&quot;: { &quot;_id&quot;: 1, &quot;item&quot;: &quot;Pizza&quot; } }\n\n\nSecondary nodes read the oplog entry and apply the same insert.\n\n\n‚úÖ Result ‚Üí All replicas have the same ‚Äúorder‚Äù data.\n\n\n\n‚ö° Quick Interview Definition\n\nThe Oplog in MongoDB is a special log collection that records all write operations on the primary. Secondaries replay it to stay consistent, enabling replication and failover.\n"},"notes/2025/Mongodb-data-deep-dive":{"title":"Mongodb data deep dive","links":[],"tags":[],"content":"vishalrana9915.medium.com/deep-dive-into-mongodb-and-its-architecture-d6290b6d2285"},"notes/2025/Node-Concurrency-VS-Parallellism":{"title":"Node Concurrency VS Parallellism","links":[],"tags":[],"content":"medium.com/@kumuthini.program/how-does-nodejs-handle-multiple-requests-97a2b094e762"},"notes/2025/RabbitMQ-Learnings":{"title":"RabbitMQ Learnings","links":[],"tags":["message_broker","message_queues","backend","system_design"],"content":"Workers vs Consumers\nIn RabbitMQ:\n1. Consumer\n\nA consumer is any application/service that subscribes to a queue and processes messages.\nIt connects to a queue, receives messages, and acknowledges them.\nCan be part of:\n\nWeb services\nBackground processors\nAny asynchronous system\n\n\n\n2. Worker\n\nA worker typically refers to a background job processor.\nIt acts as a consumer, but the term emphasizes the work done after receiving the message.\n\nSummary:\n\nA worker is often a type of consumer, but not all consumers are considered workers.\nUse ‚Äúconsumer‚Äù when referring to RabbitMQ API concepts; ‚Äúworker‚Äù is more of an application-level term.\n\n\nRabbitMQ Core Abstractions\n1. Producer\n\nSends messages to exchanges (not directly to queues).\n\n2. Exchange\n\nRoutes messages to queues using rules and routing keys.\nTypes:\n\nDirect: Exact match routing key\nTopic: Pattern-based (wildcards like * or #)\nFanout: Broadcast to all bound queues\nHeaders: Based on headers\n\n\n\n3. Queue\n\nStores messages until consumed.\nFIFO by default.\nAttributes:\n\ndurable: survives broker restart\nexclusive: restricted to one connection\nautoDelete: deleted when last consumer disconnects\n\n\n\n4. Binding\n\nConnects an exchange to a queue using a binding key.\n\n5. Consumer\n\nRetrieves and processes messages from queues.\n\n6. Message\n\nBody: Payload\nProperties: Metadata (e.g., headers, delivery mode)\n\n7. Connection and Channel\n\nConnection: TCP connection between app and RabbitMQ.\nChannel: Lightweight virtual connection on a single TCP connection.\n\n8. Virtual Host (vhost)\n\nNamespace to isolate exchanges, queues, etc.\n\n9. Broker\n\nThe RabbitMQ server itself.\n\n10. Routing Key\n\nUsed by producers to guide message routing via exchanges.\n\n11. Acknowledgment (Ack)\n\nSent by consumers after successfully processing messages.\n\n12. Dead Letter Exchange (DLX)\n\nCaptures undelivered or rejected messages.\n\n13. Prefetch Count\n\nLimits the number of unacknowledged messages sent to consumers.\n\n\nüß™ Direct Exchange Example in Node.js\nSetup\nExchange: direct_logs\nQueues: errorQueue, infoQueue, warningQueue\nRouting:\n\nerror ‚Üí errorQueue\ninfo ‚Üí infoQueue\nwarning ‚Üí warningQueue\n\n1. Producer Code\nconst amqp = require(&#039;amqplib&#039;);\n \nasync function produceMessage() {\n  const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n  const channel = await connection.createChannel();\n \n  const exchange = &#039;direct_logs&#039;;\n  const msg = process.argv.slice(2).join(&#039; &#039;) || &#039;Hello World!&#039;;\n  const severity = process.argv[2] || &#039;info&#039;;\n \n  await channel.assertExchange(exchange, &#039;direct&#039;, { durable: false });\n \n  channel.publish(exchange, severity, Buffer.from(msg));\n  console.log(`[x] Sent ${severity}: &#039;${msg}&#039;`);\n \n  setTimeout(() =&gt; {\n    channel.close();\n    connection.close();\n  }, 500);\n}\n \nproduceMessage();\n2. Consumer Code\nconst amqp = require(&#039;amqplib&#039;);\n \nasync function consumeMessage(routingKey) {\n  const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n  const channel = await connection.createChannel();\n \n  const exchange = &#039;direct_logs&#039;;\n  await channel.assertExchange(exchange, &#039;direct&#039;, { durable: false });\n \n  const q = await channel.assertQueue(&#039;&#039;, { exclusive: true });\n  console.log(`[x] Waiting for messages in ${q.queue}`);\n \n  await channel.bindQueue(q.queue, exchange, routingKey);\n \n  channel.consume(q.queue, (msg) =&gt; {\n    if (msg.content) {\n      console.log(`[x] Received ${routingKey}: ${msg.content.toString()}`);\n    }\n  }, { noAck: true });\n}\n \nconsumeMessage(process.argv[2] || &#039;info&#039;);\n \nQueue Attributes Explained\n1. Durable\nchannel.assertQueue(&#039;queue_name&#039;, { durable: true });`\n\nQueue survives RabbitMQ restarts.\nTo persist messages too:\n\nchannel.sendToQueue(&#039;queue_name&#039;, Buffer.from(&#039;data&#039;), { persistent: true });\n2. Exclusive\nchannel.assertQueue(&#039;queue_name&#039;, { exclusive: true });\n\n\nOnly available to the connection that created it.\n\n\nDeleted when that connection closes.\n\n\n3. Auto-Delete\nchannel.assertQueue(&#039;queue_name&#039;, { autoDelete: true });\n\nAutomatically deleted when the last consumer disconnects.\n\nCombined Example:\nchannel.assertQueue(&#039;queue_name&#039;, {   durable: true,   exclusive: true,   autoDelete: true });\n\nüßµ Connection vs Channel in RabbitMQ\nConnection\n\n\nA TCP connection between your app and RabbitMQ.\n\n\nHeavyweight: uses system resources.\n\n\nChannel\n\n\nA virtual, lightweight connection over a TCP connection.\n\n\nMultiple channels can exist on one connection.\n\n\nAnalogy\n\n\nConnection = Highway\n\n\nChannel = Lanes on that highway\n\n\nMultiple lanes (channels) can carry different traffic (tasks) efficiently.\n\n\nNode.js Example\nProducer (Channel 1)\nj\nconst amqp = require(&#039;amqplib&#039;);  \nasync function producer() {   \n\tconst connection = await amqp.connect(&#039;amqp://localhost&#039;);   \n\tconst channel = await connection.createChannel();    \n\tawait channel.assertQueue(&#039;test_queue&#039;, { durable: false });     \n\tchannel.sendToQueue(&#039;test_queue&#039;, Buffer.from(&#039;Hello, RabbitMQ!&#039;));     console.log(&quot;[x] Sent message&quot;);    \n\tsetTimeout(() =&gt; \n\t   {     \n\t    channel.close();     \n\t    connection.close();   \n\t   }, 500); \n}  \nproducer();\nConsumer (Channel 2)\nconst amqp = require(&#039;amqplib&#039;);\n \nasync function consumer() {\n  const connection = await amqp.connect(&#039;amqp://localhost&#039;);\n  const channel = await connection.createChannel();\n \n  await channel.assertQueue(&#039;test_queue&#039;, { durable: false });\n \n  console.log(&quot;[x] Waiting for messages in test_queue&quot;);\n \n  channel.consume(&#039;test_queue&#039;, (msg) =&gt; {\n    console.log(`[x] Received: ${msg.content.toString()}`);\n  }, { noAck: true });\n}\n \nconsumer();\n \nSummary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConceptDescriptionConnectionTCP link to RabbitMQ (expensive, fewer is better)ChannelLightweight virtual link on top of connectionUse CaseUse channels for pub/sub logic over one connection\n\nTip: Always reuse a single connection and use multiple channels in production for better performance and resource management"},"notes/2025/Rate-Limiter":{"title":"Rate Limiter","links":[],"tags":["rate_limiter","system_design"],"content":"The rest of your content lives here. You can use Markdown here :)\nRate Limiter Design\n1) Goals &amp; Requirements\n\nScope: per-user, per-API-key, per-IP, or global?\n‚Üí Assume per-API-key\nLimits: e.g., 100 requests / minute with burst up to 200\nBehavior on exceed: reject (HTTP 429), queue, or degrade?\n‚Üí Use reject\nDistributed: single-process (dev) or multi-instance (prod)?\n‚Üí Assume multi-instance\nStrictness: hard limit vs soft smoothing (token bucket allows bursts)\n\n\n2) Common Algorithms\nFixed Window Counter\n\nHow: Count requests in fixed time buckets (e.g., per minute).\n‚úÖ Pros: Easy, low cost.\n‚ùå Cons: Bursty at boundaries.\nUse when: simple and low-precision is OK.\n\nSliding Window Log\n\nHow: Store timestamps of requests, count those within last interval.\n‚úÖ Pros: Accurate.\n‚ùå Cons: Storage heavy (one entry per request).\nUse when: strict fairness and accuracy matter.\n\nSliding Window Counter (approx)\n\nHow: Keep counts for current &amp; previous window, weight them.\n‚úÖ Pros: Less memory, approximate.\nUse when: low storage, approximation acceptable.\n\nToken Bucket\n\nHow: Tokens refill at rate r; requests consume tokens.\n‚úÖ Pros: Smooth limiting, supports bursts.\n‚ùå Cons: Needs atomic ops in distributed setups.\n\nLeaky Bucket\n\nHow: Queue-like constant outflow.\n‚ùå Less common for API limiting.\n\n\n‚úÖ Recommendation\n\nDev (single-node) ‚Üí In-memory Token Bucket\nProd (multi-instance) ‚Üí\n\nRedis-backed sliding window (sorted set) or\nRedis token-bucket via Lua (atomic, less memory)\n\n\n\n\n3) Rate Limit Response Headers\nBest practice response headers:\n\nX-RateLimit-Limit: maximum requests in window\nX-RateLimit-Remaining: remaining requests in current window\nX-RateLimit-Reset: epoch seconds until limit resets\n\nOn 429 return:\n\nRetry-After: seconds until allowed.\n\n\n4) In-memory Token Bucket (TypeScript)\n// tokenBucket.ts\nexport class TokenBucket {\n  private tokens: number;\n  private lastRefill: number;\n \n  constructor(\n    private capacity: number,        // max tokens (burst)\n    private refillRatePerSec: number // tokens per second\n  ) {\n    this.tokens = capacity;\n    this.lastRefill = Date.now() / 1000;\n  }\n \n  private refill() {\n    const now = Date.now() / 1000;\n    const elapsed = Math.max(0, now - this.lastRefill);\n    const tokensToAdd = elapsed * this.refillRatePerSec;\n    this.tokens = Math.min(this.capacity, this.tokens + tokensToAdd);\n    this.lastRefill = now;\n  }\n \n  public tryRemove(count = 1): boolean {\n    this.refill();\n    if (this.tokens &gt;= count) {\n      this.tokens -= count;\n      return true;\n    }\n    return false;\n  }\n \n  public getRemaining() {\n    this.refill();\n    return this.tokens;\n  }\n}\nExpress Middleware\nimport { Request, Response, NextFunction } from &quot;express&quot;;\nimport { TokenBucket } from &quot;./tokenBucket&quot;;\n \nconst buckets = new Map&lt;string, TokenBucket&gt;();\n \nexport function rateLimitMiddleware(\n  capacity = 100,\n  refillRatePerSec = 100 / 60\n) {\n  return (req: Request, res: Response, next: NextFunction) =&gt; {\n    const key = req.headers[&quot;x-api-key&quot;] as string || req.ip;\n    let bucket = buckets.get(key);\n \n    if (!bucket) {\n      bucket = new TokenBucket(capacity, refillRatePerSec);\n      buckets.set(key, bucket);\n    }\n \n    if (bucket.tryRemove(1)) {\n      res.setHeader(&quot;X-RateLimit-Limit&quot;, capacity);\n      res.setHeader(&quot;X-RateLimit-Remaining&quot;, Math.floor(bucket.getRemaining()));\n      const secondsToFull = Math.ceil((capacity - bucket.getRemaining()) / refillRatePerSec);\n      res.setHeader(&quot;X-RateLimit-Reset&quot;, Math.floor(Date.now()/1000) + secondsToFull);\n      return next();\n    } else {\n      res.setHeader(&quot;Retry-After&quot;, &quot;1&quot;);\n      res.status(429).send({ error: &quot;Too Many Requests&quot; });\n    }\n  };\n}\nRedis-backed Sliding Window (accurate + distributed)\nSteps per request:\n\n\nRemove old timestamps (ZREMRANGEBYSCORE)\n\n\nAdd current timestamp (ZADD)\n\n\nCount requests (ZCARD)\n\n\nSet TTL for cleanup (PEXPIRE)\n\n\nimport Redis from &quot;ioredis&quot;;\nconst redis = new Redis({ host: &#039;127.0.0.1&#039;, port: 6379 });\n \nasync function allowRequest(key: string, maxRequests: number, windowMs: number) {\n  const now = Date.now();\n  const windowStart = now - windowMs;\n  const zkey = `rl:${key}`;\n \n  const pipeline = redis.pipeline();\n  pipeline.zremrangebyscore(zkey, 0, windowStart);\n  pipeline.zadd(zkey, now, `${now}:${Math.random()}`);\n  pipeline.zcard(zkey);\n  pipeline.pexpire(zkey, Math.ceil(windowMs / 1000) + 1);\n  const results = await pipeline.exec();\n \n  const currentCount = Number(results[2][1]);\n  return { allowed: currentCount &lt;= maxRequests, currentCount, resetAfterMs: windowMs - (now - windowStart) };\n}\n \nexport function redisSlidingWindowMiddleware(maxRequests = 100, windowMs = 60_000) {\n  return async (req, res, next) =&gt; {\n    const key = req.headers[&#039;x-api-key&#039;] as string || req.ip;\n    const { allowed, currentCount, resetAfterMs } = await allowRequest(key, maxRequests, windowMs);\n \n    res.setHeader(&quot;X-RateLimit-Limit&quot;, String(maxRequests));\n    res.setHeader(&quot;X-RateLimit-Remaining&quot;, String(Math.max(0, maxRequests - currentCount)));\n    res.setHeader(&quot;X-RateLimit-Reset&quot;, String(Math.floor((Date.now() + resetAfterMs) / 1000)));\n \n    if (!allowed) {\n      res.setHeader(&quot;Retry-After&quot;, String(Math.ceil(resetAfterMs / 1000)));\n      return res.status(429).send({ error: &quot;Too Many Requests&quot; });\n    }\n    return next();\n  };\n}\n \n6) Redis Token Bucket (Lua, atomic)\nLua script (atomic refill + consume):\n-- KEYS[1] =&gt; key\n-- ARGV[1] =&gt; capacity\n-- ARGV[2] =&gt; refill_rate_per_ms\n-- ARGV[3] =&gt; now_ms\n-- ARGV[4] =&gt; requested_tokens\n \nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill_rate = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\nlocal requested = tonumber(ARGV[4])\n \nlocal data = redis.call(&quot;HMGET&quot;, key, &quot;tokens&quot;, &quot;last&quot;)\nlocal tokens = tonumber(data[1]) or capacity\nlocal last = tonumber(data[2]) or now\n \nlocal elapsed = math.max(0, now - last)\nlocal add = elapsed * refill_rate\ntokens = math.min(capacity, tokens + add)\nlast = now\n \nlocal allowed = 0\nif tokens &gt;= requested then\n  tokens = tokens - requested\n  allowed = 1\nend\n \nredis.call(&quot;HMSET&quot;, key, &quot;tokens&quot;, tokens, &quot;last&quot;, last)\nredis.call(&quot;PEXPIRE&quot;, key, math.ceil((capacity/refill_rate) * 2))\nreturn { allowed, tokens }\n \n7) Authentication &amp; Keying\n\n\nRate limit should be per authenticated entity (API key / user ID).\n\n\nUse IP-based only for unauthenticated routes.\n\n\n\n8) Tuning &amp; Ops Notes\n\n\nBurst vs steady: adjust capacity + refill rate.\n\n\nBackoff: clients should use exponential backoff on 429.\n\n\nMetrics: track rate_limit_hits, rejections, Redis latency/errors.\n\n\nMonitoring: Redis memory, latency.\n\n\nPer-endpoint limits: e.g., /login vs /data.\n\n\nGrace periods: whitelist trusted services.\n\n\nTesting: load test with k6/vegeta.\n\n\n\n9) Which Should You Use?\n\n\nDev/single-node ‚Üí in-memory Token Bucket\n\n\nProd, moderate accuracy ‚Üí Redis Token Bucket (Lua)\n\n\nProd, strict fairness ‚Üí Redis Sliding Window\n\n\n\n10) Quick Checklist\n\n\nDefine scope &amp; limits per key\n\n\nPick algorithm (Redis token-bucket recommended)\n\n\nImplement Lua or pipeline for atomic ops\n\n\nAdd middleware with headers + 429 Retry-After\n\n\nAdd unit + integration tests\n\n\nAdd metrics &amp; alerts\n\n"},"notes/2025/SQL-VS-NOSQL":{"title":"SQL VS NOSQL","links":[],"tags":["sql","nosql","database"],"content":"The rest of your content lives here. You can use Markdown here :)\n‚ö° Why SQL is considered Vertically Scalable\n\n\nRelational model (SQL) depends on ACID transactions, joins, and strict schema.\n\n\nThis means data is often tightly coupled ‚Üí hard to split across many machines.\n\n\nHistorically, RDBMS were single-node systems ‚Üí easiest way to handle more load was scale-up (vertical scaling).\n\n\nExample: Add more CPU/RAM to your MySQL/Postgres server.\n\n\n\n‚ö° Why NoSQL is considered Horizontally Scalable\n\n\nNoSQL databases (MongoDB, Cassandra, DynamoDB) were built with distributed architecture in mind.\n\n\nThey sacrifice some ACID guarantees (often use BASE model: Basically Available, Soft state, Eventually consistent) to make partitioning/sharding easier.\n\n\nData is stored in key-value, document, or column family models which are naturally easier to distribute across nodes.\n\n\nSo you can add more machines (scale-out) and the DB balances data automatically.\n\n\n\n‚ö° But‚Ä¶ Both Can Scale Both Ways\n\n\nSQL can be horizontally scaled too ‚Üí through sharding, partitioning, or distributed SQL engines (Vitess for MySQL, Citus for Postgres, CockroachDB, Spanner).\n\n\nNoSQL can be vertically scaled too ‚Üí put MongoDB or Cassandra on a bigger server (faster CPU, more RAM).\n\n\n\n‚úÖ Correct Interview-Friendly Answer\n\n‚ÄúTraditionally, SQL databases are vertically scalable because their relational model makes horizontal distribution harder. NoSQL databases were designed for horizontal scaling from the start, trading off strict consistency for distribution. But in practice, both SQL and NoSQL can scale vertically and horizontally ‚Äî it‚Äôs just that their default design philosophies are different.‚Äù\n\nSharding vs Partitioning\nüîπ Partitioning\n\n\nSplitting a single database into smaller, more manageable parts.\n\n\nUsually done within one server or cluster.\n\n\nGoal: performance optimization (e.g., smaller indexes, faster queries).\n\n\nPartitioning is often logical (the database engine still ‚Äúknows‚Äù all partitions are part of the same database).\n\n\nTypes of Partitioning\n\n\nHorizontal Partitioning ‚Äì rows are split across partitions (e.g., users with IDs 1‚Äì1000 in Partition A, 1001‚Äì2000 in Partition B).\n\n\nVertical Partitioning ‚Äì columns are split across tables/partitions (e.g., user profile info in one partition, user preferences in another).\n\n\nüëâ Example: Splitting a customer table by region (Asia partition, Europe partition).\n\nüîπ Sharding\n\n\nA form of horizontal partitioning but specifically across multiple servers/nodes.\n\n\nEach shard is a self-contained independent database.\n\n\nGoal: scalability + distribution (handle very large datasets and high traffic).\n\n\nRequires a shard key to decide where data lives.\n\n\nApplication or middleware is responsible for routing queries to the right shard.\n\n\nüëâ Example: MongoDB sharding customers by user_id % 10 across 10 different machines.\n\n‚úÖ Key Differences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeaturePartitioningShardingDefinitionSplitting data into smaller partsSplitting data across multiple serversScopeWithin a single DB instanceAcross multiple DB instancesGoalPerformance, manageabilityScalability, distributionData LocationAll partitions usually on same machineShards live on different machinesManagementDB engine manages partitions internallyRequires shard key + routing logic\n\n‚ö° Interview-Safe One-Liner\n\n‚ÄúPartitioning is splitting data inside one database for performance, while sharding is splitting data across multiple machines for scalability. Sharding is basically distributed horizontal partitioning.‚Äù\n\n\nüëâ Sanjay, do you want me to also include a real-world example (like how Netflix or Instagram uses sharding) to make your answer stand out in interviews?\nYou said:\nwhy do we need to partitioning with single db server\nChatGPT said:\nNice follow-up üî• ‚Äî this shows you‚Äôre thinking beyond just definitions.\n\n‚úÖ Why Partitioning with a Single DB Server?\nEven if you don‚Äôt have multiple servers (so no sharding), partitioning inside one database server still brings big benefits:\n\nüîπ 1. Performance Improvement\n\nQueries only scan the relevant partition instead of the whole big table.\nüëâ Example: If you partition an orders table by year, a query for orders_2025 doesn‚Äôt need to scan all years.\n\n\nüîπ 2. Index Efficiency\n\n\nIndexes are smaller within each partition.\n\n\nFaster lookups because the DB searches in a smaller dataset.\n\n\n\nüîπ 3. Manageability\n\n\nLarge tables are easier to maintain when split into partitions.\n\n\nYou can archive, drop, or back up old partitions (like orders_2020) without touching newer data.\n\n\n\nüîπ 4. Parallelism\n\n\nSome databases (Postgres, Oracle) can process queries across partitions in parallel.\n\n\nThis reduces query time significantly.\n\n\n\nüîπ 5. Availability / Fault Isolation\n\n\nIf one partition is corrupted or locked, others can still be accessed.\n\n\nExample: If orders_2019 partition is being repaired, queries on orders_2025 still run normally.\n\n\n\n‚ö° Real-World Example\n\n\nBanking system: A bank stores transactions for many years.\n\n\nPartition transactions by month or year.\n\n\nQuerying transactions_2025 is much faster than searching the entire history.\n\n\nOld partitions can be archived to cheaper storage.\n\n\n\n\nDisadvantage of sharding\n\noperationally complex , (if one shar goes down , then you want to move the data the other shard)\ncross-shard query (which is very very expensive)\n"},"notes/2025/SSH-BASICS":{"title":"SSH BASICS","links":[],"tags":["ssh","backend","security"],"content":"The rest of your content lives here. You can use Markdown here :)\nSSH  Flow Diagram\nreference\nwww.cloudflare.com/en-gb/learning/access-management/what-is-ssh/)"},"notes/2025/Thread-Concepts":{"title":"Thread Concepts","links":[],"tags":["thread","os","java"],"content":"The rest of your content lives here. You can use Markdown here :)\nüß† Main Memory vs Thread Local Cache in Java\n1. Main Memory\n\n\nRefers to the heap and static memory area shared by all threads.\n\n\nEvery object and class-level variable lives in main memory.\n\n\nAll threads see this as the ‚Äúglobal truth‚Äù.\n\n\nBut‚Ä¶ direct access to main memory is slower, so JVM and CPU optimize with caching.\n\n\n\n2. Thread Local Cache (CPU Registers / Working Memory)\n\n\nEach thread keeps a local copy of variables it is working with.\n\n\nThis copy lives in the CPU‚Äôs registers or caches (L1, L2), sometimes called working memory in the JMM.\n\n\nThis makes reads/writes faster since the thread does not always hit main memory.\n\n\n\n3. The Problem\nBecause of caching, one thread‚Äôs updates may not be visible to other threads immediately.\nüëâ Example (without volatile):\nclass SharedResource {     boolean flag = false;      void writer() {         flag = true; // Thread A updates flag     }      void reader() {         while (!flag) {             // Thread B keeps checking flag         }         System.out.println(&quot;Detected flag change&quot;);     } }\n\n\nThread A updates flag = true ‚Üí writes to its local cache.\n\n\nThread B keeps reading flag from its own cache, which may still say false.\n\n\nSo B may never see the change, even though A already updated it.\n\n\n\n4. How volatile Fixes It\n\n\nDeclaring a variable as volatile forces:\n\n\nWrite ‚Üí directly written to main memory.\n\n\nRead ‚Üí always fetched from main memory, not from cache.\n\n\n\n\nvolatile boolean flag = false;\nNow both Thread A and Thread B see the same latest value from main memory. ‚úÖ\n\n5. Analogy\nThink of it like this:\n\n\nMain memory = a shared notice board in an office.\n\n\nThread local cache = each worker keeps a sticky note copy.\n\n\nWithout volatile/sync, workers may keep reading from their own sticky note, not noticing updates.\n\n\nWith volatile, every read/write goes through the notice board, so everyone sees the latest info.\n\n\n\n‚úÖ In Summary\n\n\nMain memory ‚Üí shared, global, slower, visible to all threads.\n\n\nThread local cache ‚Üí per-thread, faster, but may cause stale/incorrect values.\n\n\nJava keywords (volatile, synchronized, final) + JMM rules ensure proper visibility and ordering between them.\n\n"},"notes/2025/UML---Class-Diagrams":{"title":"UML - Class Diagrams","links":[],"tags":["DesignPattern"],"content":"The rest of your content lives here. You can use Markdown here :)\n\nUML class notations\nperspectives of the class diagram\nrelationship between classess\n\nClass Representations\n\nClass Name - TOP\nAttributes - Middle\nMethods/operations - Bottom\n\nClass Visibility\n\nPublic (+)\nprivate (-)\nprotected (#)\npackage (~)\n\nRelationships of class\n\nAssociation ‚Üí (uses-a)\naggregation - (Has-a) (weak coupling)\ncomposition - (Has-a) (tight coupling)\nreliazation (implementation)\ndependecy -( uses)\n\nAssociation:  A -------- B\nAggregation:  Library ‚óá------ Book\nComposition:  House ‚óÜ------ Room\nRealization:  Dog --------‚ñ∑ Animal (interface)\nDependency:   Car - - - - - - &gt; Engine"},"notes/2025/Vector-database":{"title":"Vector database","links":[],"tags":["database","vector_database","genai"],"content":"The rest of your content lives here. You can use Markdown here :)\nüìå Vector Embeddings &amp; Vector Databases\n1. What is a Vector Embedding?\n\nEmbeddings = a way to represent data (text, image, audio, video, code, etc.) in numbers so that computers can understand and compare them.\nAn embedding is just a list of numbers (a vector).\nThese numbers capture the meaning or features of the data.\n\nüëâ Example with text\n\nWord ‚Äúcat‚Äù ‚Üí [0.23, -0.91, 0.65, ‚Ä¶] (vector in 512 or 1536 dimensions)\nWord ‚Äúdog‚Äù ‚Üí [0.25, -0.88, 0.61, ‚Ä¶]\nWord ‚Äúcar‚Äù ‚Üí [0.92, 0.11, -0.45, ‚Ä¶]\n\nHere, the vectors for cat and dog are closer (because they are both animals) than cat and car.\nüëâ Example with images\n\nA picture of a cat ‚Üí [0.12, 0.99, -0.34, ‚Ä¶]\nAnother picture of a different cat ‚Üí [0.10, 1.01, -0.32, ‚Ä¶]\n\nThese two are close in vector space, even if the images are not pixel-to-pixel identical.\nüìå In short:\nEmbeddings = numerical representation of meaning/essence of data.\n\n2. What is a Vector Database?\n\nA vector database is a database specialized in storing embeddings and searching them efficiently.\nNormal databases (SQL, MongoDB) are good at searching exact values (WHERE name = &#039;John&#039;).\nBut embeddings are high-dimensional vectors (512, 1024, 4096 dimensions).\n\nCore Operation ‚Üí Similarity Search (Nearest Neighbor Search)\n\n‚ÄúFind me the 10 most similar vectors to this query vector.‚Äù\nUses mathematical distance:\n\nCosine similarity\nEuclidean distance\nDot product\n\n\n\nüëâ Example:\n\nQuery: ‚Äúcute small animal with fur‚Äù ‚Üí converted into embedding.\nDB finds closest vectors ‚Üí returns cat, dog, rabbit instead of keyword matches.\n\nPopular Vector Databases\n\nPinecone\nWeaviate\nMilvus\nQdrant\nPostgres + pgvector extension\n\n\n3. Why Not Normal Database?\nüü¢ Traditional DB\n\nGood for structured data (IDs, names, emails, numbers).\nSupports exact match or filtering (age &gt; 25).\n‚ùå Bad at semantic similarity.\n\nüëâ Example:\nSearch ‚Äúautomobile‚Äù ‚Üí SQL DB won‚Äôt return ‚Äúcar‚Äù unless explicitly stored.\nüü¢ Vector DB\n\nGood for unstructured data (text, images, audio).\nSupports semantic similarity search.\nHandles high-dimensional vectors.\nUses HNSW (Hierarchical Navigable Small World graphs) for fast nearest-neighbor search.\n\nüëâ Example:\nSearch ‚Äúautomobile‚Äù ‚Üí Vector DB finds ‚Äúcar‚Äù, ‚Äúvehicle‚Äù, ‚ÄúSUV‚Äù because embeddings are close in meaning.\n\n4. Examples &amp; Use Cases\nüîπ Example 1: Semantic Search (Text)\n\nNormal DB: Search ‚ÄúAI developer‚Äù ‚Üí Returns only exact matches.\nVector DB: Returns ‚ÄúMachine learning engineer‚Äù, ‚ÄúData scientist‚Äù, ‚ÄúNLP expert‚Äù.\n\nüîπ Example 2: Chatbot / RAG (Retrieval-Augmented Generation)\n\nStore company docs as embeddings in Vector DB.\nUser: ‚ÄúWhat‚Äôs our refund policy?‚Äù\nQuery ‚Üí Embed ‚Üí Search DB ‚Üí Retrieve relevant passages ‚Üí Feed to LLM ‚Üí Answer generated.\n\nüîπ Example 3: Image Search\n\nUpload dog picture ‚Üí Vectorize ‚Üí Search ‚Üí Returns similar dogs (even with different filenames).\n\nüîπ Example 4: Recommendation Engine\n\nNetflix: Movie embeddings ‚Üí Recommend similar movies.\nSpotify: Song embeddings ‚Üí Recommend similar sounding songs.\n\n\nüîë Summary\n\nVector Embedding = numerical representation of meaning.\nVector Database = specialized DB for embeddings &amp; similarity search.\nWhy not normal DB? ‚Üí Can‚Äôt handle high-dimensional semantic similarity.\nUse Cases ‚Üí Search, Recommendation, Chatbots (RAG), Image/Audio search.\n"},"notes/AI-Notes/Generative-AI-vs-Predictive-AI":{"title":"Notes From Linkedin AI Course","links":[],"tags":["AI"],"content":"AI isn just a matching pattern , where it seems to be intelligent in the end it just matching the pattern and data\nUnderstanding the distinction between Generative AI and Predictive AI is crucial for anyone working with artificial intelligence systems. While both leverage data and machine learning techniques, their goals, data requirements, and outputs differ significantly.\n\nPredictive AI\nPredictive AI focuses on using historical data, patterns, and statistical models to forecast future outcomes or behaviours.\nKey Characteristics\n\nData-driven forecasting: The more high-quality data available, the more accurate the predictions become.\nDomain-specific: Predictive models are usually trained for one specific task (e.g., fraud detection, sales forecasting).\nOutput: Produces probabilistic predictions about future events, not new or original content.\n\nExamples\n\nüéµ A music app recommending songs you are likely to enjoy.\nüõí E-commerce suggesting products you are likely to purchase.\nüè¶ Banks estimating the probability of a loan default.\nüè• Healthcare predicting the risk of a disease.\n\n\nGenerative AI\nGenerative AI takes large-scale datasets and uses them to create new, original outputs that resemble the training data.\nKey Characteristics\n\nMassive training data: Trained on millions of samples from diverse sources.\nVersatile: Can perform multiple tasks (e.g., text generation, image synthesis, code generation).\nOutput: Produces novel content rather than just forecasting outcomes.\n\nExamples\n\nüñãÔ∏è Large Language Models (LLMs) like ChatGPT generating human-like text.\nüé® AI tools generating images, videos, or music from prompts.\nüíª AI models writing code snippets based on natural language descriptions.\n\n\nKey Differences: Generative AI vs Predictive AI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspectGenerative AIPredictive AITraining DataMassive datasets with millions of content samples (text, images, audio, etc.)Historical and domain-specific datasets (smaller, targeted)Primary GoalCreate new and original outputs that resemble training dataForecast future outcomes based on past trendsOutput TypeNovel content (text, image, music, video, code)Probabilistic predictions (numeric values, classifications)Use CasesContent creation, chatbots, design, creative toolsRisk assessment, demand forecasting, personalizationFlexibilityCan perform multiple creative tasksFocused on a single prediction task per model\n\nSummary\nWhile both generative and predictive AI rely on data, their purpose and output differ:\n\nPredictive AI answers: ‚ÄúWhat is likely to happen next?‚Äù\nGenerative AI answers: ‚ÄúWhat new content can I create that resembles what I‚Äôve learned?‚Äù\n\nTogether, they power many modern AI applications ‚Äî predictive AI helps us make data-driven decisions, while generative AI enables creative automation.\n"},"notes/AI-Notes/Introduction-to-airtifical-intelligence":{"title":"Introduction to airtifical intelligence","links":[],"tags":["genai","AI"],"content":"The rest of your content lives here. You can use Markdown here :)\nArtificial Intelligence (AI) is the field of computer science focused on building systems that can perform tasks that normally require human intelligence ‚Äî such as understanding language, recognizing patterns, making decisions, and learning from experience.\n1. Core Idea\nInstead of explicitly programming every rule, AI systems are designed to learn patterns and adapt based on data and experience.\n\n2. Common Abilities of AI\n\n\nPerception ‚Üí recognizing images, sounds, faces, handwriting.\n\n\nReasoning ‚Üí solving problems, making logical decisions.\n\n\nLearning ‚Üí improving performance through data (machine learning).\n\n\nLanguage understanding ‚Üí speech-to-text, chatbots, translation.\n\n\nAction ‚Üí controlling robots, autonomous driving.\n\n\n\n3. Types of AI (by capability)\n\n\nNarrow AI (Weak AI)\n\n\nSpecialized for one task.\n\n\nExamples: Siri, Google Translate, spam filters.\n\n\n\n\nGeneral AI (Strong AI)\n\n\nCan perform any intellectual task a human can.\n\n\nDoesn‚Äôt exist yet ‚Äî still theoretical.\n\n\n\n\nSuperintelligent AI\n\n\nBeyond human intelligence in all aspects.\n\n\nSpeculative and debated.\n\n\n\n\n\n4. Technologies behind AI\n\n\nMachine Learning (ML) ‚Üí teaches computers to learn from data.\n\n\nDeep Learning ‚Üí uses neural networks with many layers.\n\n\nNatural Language Processing (NLP) ‚Üí enables understanding and generating human language.\n\n\nComputer Vision ‚Üí interprets and analyzes visual data.\n\n\nReinforcement Learning ‚Üí learns by trial and error to maximize rewards.\n\n\n\nüí° Simple analogy:\nIf traditional programming is like giving a robot step-by-step cooking instructions, AI is like teaching the robot by showing it thousands of cooking videos so it learns to cook on its own."},"notes/DSA/Heap-Data-Structure":{"title":"Heap Data Structure","links":[],"tags":["dsa","heap","data_structures"],"content":"What is a Heap?\n\n\nBinary Heap (most common):\n\n\nComplete binary tree ‚Üí all levels are full except possibly the last, which is filled from left to right.\n\n\nHeap property:\n\n\nMin Heap ‚Üí parent ‚â§ children (smallest element at the root; same property holds for all subtrees).\n\n\nMax Heap ‚Üí parent ‚â• children (largest element at the root; same property holds for all subtrees).\n\n\n\n\n\n\nUsually implemented using arrays, not pointers.\n\n\n\nRepresentation of a Heap\nA binary heap can be represented using an array:\n\n\nParent node index: ceil(i/2) - 1\n\n\nLeft child index: (2 * i) + 1\n\n\nRight child index: (2 * i) + 2\n\n\nRange of leaf node indices: n/2 to n - 1\n\n\nRange of internal node indices: 0 to (n/2) - 1\n\n\n\nTree Formula for a Complete Binary Tree\nIn a complete binary tree:\n\n\nAll nodes are completely filled up to the second last level.\n\n\nIn the last level, the leaf nodes are filled from left to right.\n\n\nFormulas:\n\n\nThe total number of nodes at level l is 2^l.\n\n\nThe total number of nodes in the entire tree is 2^(l+1) - 1.\n\n\n\nHeapify\nHeapify is the process of rearranging nodes by checking the left and right children recursively to ensure the heap property is maintained.\nImportant points:\n\n\nIf you want to apply heapify to the element at index i, the left and right subtrees of that element must already satisfy the heap property. Otherwise, you should not perform the heapify operation yet.\n\n\nStart heapifying from the leaf nodes upward. Leaf nodes always satisfy the heap property because they have no children.\n\n\nHeapify Time Complexity\n1. Heapify a Single Node\n\nheapify(node) (also called sift-down) compares a node with its children.\nIf the heap property is violated, it swaps with the correct child and continues down.\nWorst case: the node travels all the way down to a leaf.\n\nüëâ Height of a binary heap = log n\nüëâ Time Complexity (single heapify) = O(log n)\n\n2. Build Heap from an Array\nTo build a heap from an unsorted array:\n\nStart from the last non-leaf node (index = ‚åän/2‚åã).\nCall heapify() on each node going upward to the root.\n\nNaive thinking:\n\nn/2 nodes √ó O(log n) each = O(n log n)\n\nBut the actual complexity is O(n).\n\n3. Why Build Heap is O(n)\nDistribution of Nodes by Height\nIn a complete binary tree:\n\nAbout n/2 nodes are leaves (height = 0).\nAbout n/4 nodes are one level above leaves (height = 1).\nAbout n/8 nodes are at height 2.\n‚Ä¶ and so on.\n\nGeneral formula:\n\\text{Nodes at height } h \\approx \\frac{n}{2^{h+1}}\n\nCost of Heapifying a Node\n\nA leaf ‚Üí cost = 0.\nOne level above leaves ‚Üí cost ‚â§ 1.\nHeight 2 ‚Üí cost ‚â§ 2.\nRoot (height log n) ‚Üí cost ‚â§ log n.\n\nSo in general:\n\\text{Cost per node at height } h \\leq h\n\n4. Total Work (Summation)\nThe total cost of building the heap is:\nT(n) \\leq \\sum_{h=0}^{\\log n} \\frac{n}{2^{h+1}} \\cdot h\nFactor out n:\nT(n) \\leq n \\cdot \\sum_{h=0}^{\\log n} \\frac{h}{2^{h+1}}\n\n5. Constant Series\nIt can be shown that:\n\\sum_{h=0}^{\\infty} \\frac{h}{2^{h+1}} = 1\nSo:\nT(n) \\leq n \\times 1 = O(n)\n\n‚úÖ Final Results\n\nHeapify (one node): O(log n)\nBuild Heap (entire array): O(n)\n\n\nüîë Intuition\n\nMost nodes are near the bottom ‚Üí cheap to heapify.\nOnly a few nodes near the top are expensive.\nCombined effect = linear work.\n"},"notes/DSA/Time-Complexity-Each-Data-Structure":{"title":"Time Complexity Each Data Structure","links":[],"tags":["dsa","complexity","data_structures"],"content":"NOTE: all the complexity of data structures\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData StructureAccess (Best)Access (Worst)Search (Best)Search (Worst)Insert (Best)Insert (Worst)Delete (Best)Delete (Worst)ArrayO(1)O(1)O(1) (if index known)O(n)O(1) (end)O(n) (middle)O(1) (end)O(n) (middle)Dynamic ArrayO(1)O(1)O(1) (index)O(n)Amortized O(1)O(n)O(1) (end)O(n)Singly Linked ListO(1) (head)O(n)O(1) (head)O(n)O(1)O(n) (with position search)O(1) (head)O(n)Doubly Linked ListO(1) (head/tail)O(n)O(1) (head/tail)O(n)O(1)O(n)O(1) (head/tail)O(n)Stack (array/list)O(1)O(1)O(n)O(n)O(1)O(1)O(1)O(1)Queue (linked list)O(1)O(1)O(n)O(n)O(1)O(1)O(1)O(1)Deque (double-ended queue)O(1)O(1)O(n)O(n)O(1)O(1)O(1)O(1)Hash TableO(1)O(n)O(1)O(n)O(1)O(n)O(1)O(n)Binary Search Tree (BST)O(log n)O(n)O(log n)O(n)O(log n)O(n)O(log n)O(n)AVL TreeO(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)Red-Black TreeO(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)B-TreeO(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)O(log n)Trie (prefix tree)O(m)O(m)O(m)O(m)O(m)O(m)O(m)O(m)Heap (min/max)O(1)O(1)O(n)O(n)O(log n)O(log n)O(log n)O(log n)Graph (adjacency list)‚Äî‚ÄîO(V + E)O(V + E)O(1)O(1)O(1)O(1)Graph (adjacency matrix)‚Äî‚ÄîO(1)O(1)O(1)O(1)O(1)O(1)\n\n\nn = number of elements\n\n\nm = length of the key (for Tries)\n\n\nBST worst case is O(n) when unbalanced (like inserting sorted data without balancing).\n\n\nHash table worst case O(n) happens when all keys collide into one bucket.\n\n\nAVL, Red-Black Trees, and B-Trees keep themselves balanced, so worst case is still O(log n).\n\n"},"notes/Git/Git-Basic-Merging":{"title":"Example Title","links":[],"tags":["git","github"],"content":"The rest of your content lives here. You can use Markdown here :)\nGit Branching ‚Äì Hotfix &amp; Fast-Forward Merge\nüìå Starting Point\nWe begin with a simple repository and a main branch:\nA --- B --- C   (main)\n\n\nCommits: A, B, C\n\n\nHEAD is at commit C\n\n\n\nüîß Step 1: Create a hotfix Branch\ngit checkout -b hotfix\nBoth main and hotfix point to commit C:\nA --- B --- C   (main, hotfix)\n\nüî® Step 2: Make a Commit on hotfix\n# make code changes git commit -am &quot;Fix critical bug&quot;\nNow history looks like:\nA --- B --- C   (main)              \\               D   (hotfix)\n\nüîÑ Step 3: Merge hotfix Back into main\ngit checkout main git merge hotfix\nSince main is directly behind hotfix, Git fast-forwards main to commit D:\nA --- B --- C --- D   (main, hotfix)\nüëâ Fast-forward merge: No new commit is created, Git just moves the branch pointer forward.\n\nüßπ Step 4: Delete the hotfix Branch (Optional)\ngit branch -d hotfix\nNow only main exists:\nA --- B --- C --- D   (main)\n\n‚ùå When Fast-Forward is NOT Possible\nIf new commits are added on main while hotfix is in progress:\nA --- B --- C --- E   (main)              \\               D   (hotfix)\nMerging requires a merge commit:\nA --- B --- C --- E ------ F   (main)              \\              /               D   (hotfix)\n\nF is the merge commit combining work from hotfix (D) and main (E).\n\n\n‚úÖ Summary\n\n\nFast-forward merge: Pointer moves forward, no new commit.\n\n\nMerge commit: Needed if branches diverged.\n\n\nReference Link:\ngit-scm.com/book/ms/v2/Git-Branching-Rebasing"},"notes/OS/Operation-System-Revise-Notes":{"title":"Operation System Revise Notes","links":[],"tags":["operating_system","os"],"content":"The rest of your content lives here. You can use Markdown here :)\nWhat is kernel in operation ?\nIn an operating system, the kernel is¬†the core component that acts as the bridge between the computer&#039;s hardware and software, managing essential system resources like memory and CPU time.¬†It controls all fundamental operations, ensuring that processes have the necessary resources to run and that hardware devices like the keyboard and monitor can communicate with the rest of the system.¬†The kernel is one of the first programs to load when a computer starts and remains in memory, handling crucial tasks such as process scheduling, memory management, and I/O (input/output) operations.\nKey Functions of the Kernel\n\n\nProcess Management:\nThe kernel schedules which processes get to use the CPU and for how long, managing the overall execution of programs.¬†\n\n\nMemory Management:\nIt monitors and manages how much memory is being used by different processes, ensuring that each application has enough.¬†\n\n\nDevice Management:\nThe kernel controls hardware devices, such as storage, keyboards, and network cards, allowing them to be used by applications.¬†\n\n\nSystem Calls:\nIt provides a set of services, called system calls, that applications use to request resources or access hardware functions from the kernel.¬†\n\n\nInterrupt Handling:\nThe kernel handles hardware and software interrupts (signals requiring immediate attention), ensuring that the system remains responsive.\n\n\nWhat is Context Switching ?"},"notes/OS/Signed-and-Unsigned-value":{"title":"Signed and Unsigned value","links":[],"tags":["operating_system","os"],"content":"üìò Two‚Äôs Complement &amp; Signed Integers\nüîπ 1. Unsigned vs Signed Integers\n\n\nUnsigned (n bits): Range = 0 to 2^n - 1\n\n\nSigned (n bits, two‚Äôs complement): Range = -2^(n-1) to 2^(n-1) - 1\n\n\n\nüîπ 2. Why n-1 in the exponent?\n\n\nTotal bit patterns with n bits = 2^n\n\n\nTwo‚Äôs complement splits them:\n\n\nMSB = 0 ‚Üí non-negative (0 and positives) ‚Üí 2^(n-1) values\n\n\nMSB = 1 ‚Üí negatives ‚Üí 2^(n-1) values\n\n\n\n\nBecause 0 is included in the non-negative half, positive max = 2^(n-1) - 1\n\n\nüìå Final formula:\nRange = -2^(n-1)  to  2^(n-1) - 1\n\nüîπ 3. Why one fewer positive value?\n\n\nNegatives: from -2^(n-1) to -1 ‚Üí total 2^(n-1) numbers\n\n\nNon-negatives: from 0 to 2^(n-1)-1 ‚Üí total 2^(n-1) numbers\n\n\n0 takes one slot on the positive side ‚Üí so positive side has one fewer value\n\n\nExample (4-bit signed):\nRange = -8 to +7\n\nüîπ 4. Two‚Äôs Complement Process\nTo represent -X in n-bit binary:\n\n\nWrite X in binary\n\n\nInvert all bits (one‚Äôs complement)\n\n\nAdd 1\n\n\n\nExample: -4 in 8-bit\n4  = 00000100 invert ‚Üí 11111011 +1     ‚Üí 11111100\nSo:\n-4 = 11111100 (8-bit two‚Äôs complement)\n\nüîπ 5. Subtraction with Two‚Äôs Complement\nSubtraction A - B is done as:\nA - B = A + (two‚Äôs complement of B)\nExample: 8 - 4\n8  = 00001000 4  = 00000100 -4 = 11111100  8 + (-4): 00001000 +11111100 --------- 00000100  = 4\n‚úÖ Correct result.\n\nüîπ 6. MSB (Most Significant Bit)\n\n\nDefinition: The leftmost bit in a binary number.\n\n\nIn unsigned integers: it just represents the largest place value.\n\n\nIn signed integers (two‚Äôs complement): it indicates the sign.\n\n\n0 ‚Üí number is non-negative\n\n\n1 ‚Üí number is negative\n\n\n\n\nExample: 8-bit binary 10110010\n\n\nMSB = 1 ‚Üí negative\n\n\nTwo‚Äôs complement ‚Üí value = -78\n\n\nExample: 4-bit signed integers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinaryMSBDecimal01110+700000010001-811111-1\n\nüîπ 7. LSB (Least Significant Bit)\n\n\nDefinition: The rightmost bit in a binary number.\n\n\nIt represents the smallest place value (2^0 = 1).\n\n\nUseful in checking parity (even/odd numbers).\n\n\nExample:\nBinary 0111 ‚Üí LSB=1 (odd number, decimal 7) Binary 0110 ‚Üí LSB=0 (even number, decimal 6)\n\nüìù Summary\n\n\nRange formula: -2^(n-1) to 2^(n-1) - 1\n\n\nMSB: sign indicator (0=positive, 1=negative) in signed integers\n\n\nTwo‚Äôs complement: invert + add 1\n\n\nSubtraction: done via addition of two‚Äôs complement\n\n\nPositive side has one fewer value than negative side because 0 is included in non-negatives\n\n"},"tags/AI":{"title":"AI","links":[],"tags":[],"content":""},"tags/book":{"title":"book","links":[],"tags":[],"content":""},"tags/database":{"title":"database","links":[],"tags":[],"content":""},"tags/hacking":{"title":"hacking","links":[],"tags":[],"content":""},"tags/operating_system":{"title":"operating_system","links":[],"tags":[],"content":""},"tags/system_design":{"title":"system_design","links":[],"tags":[],"content":""}}